% !TEX root = ../main.tex

% Per epigraph's documentation, the new environment gives us indented paragraphs
% and justified text.
\newenvironment{kafkaesque}{
    \setlength{\parindent}{\normalparindent}
}{}
\renewcommand{\textflush}{kafkaesque}
\setlength{\epigraphwidth}{0.8\textwidth}

\epigraph{\noindent\openfat{}As you see, it consists of three parts. With the
passage of time certain popular names have been developed for each of these
parts. The one underneath is called the Bed, the upper one is called the
Inscriber, and here in the middle, this moving part is called the Harrow.
[\ldots]

As soon as the man is strapped in securely, the Bed is set in motion. It quivers
with tiny, very rapid oscillations from side to side and up and down
simultaneously. [\ldots] Only with our Bed all movements are precisely
calibrated, for they must be meticulously coordinated with the movements of the
Harrow. But it's the Harrow which has the job of actually carrying out the
sentence. [\ldots]

The law which a condemned man has violated is inscribed on his body with the
Harrow.\closefat}{Franz Kafka, \emph{In the Penal Colony}~\cite{Kafka1995}}


\section{Introduction}
\label{sec:introduction}

My first interaction with \DALLE~2, OpenAI's headline-making text-to-image
system, in late July 2022 didn't quite go as expected. I had signed up with
OpenAI several months before but had been granted access only earlier that day.
So I was eager to try out the system and started with a prompt that had yielded
good results with another text-to-image system. But instead of producing four
images, \DALLE~2 responded with a stern warning:

\begin{quote}
\openfat{}It looks like this request may not follow our content policy. Further
policy violations may lead to an automatic suspension of your
account.\closefat{}
\end{quote}

\noindent Whoa! I enter an entirely reasonable prompt and OpenAI immediately
threatens me with account suspension? Without even telling me what I did wrong?
That's just ridiculous. And frustrating. Consulting the content policy didn't
help much either. The policy seemed broad, also vague. I eventually did narrow
down the violation to two out of the eleven content prohibitions. But which of
the two? I couldn't tell. It all seemed rather Kafkaesque!

It also was familiar. In October 2021, Twitter's \AI{} took offense to an
admittedly caustic tweet I had just posted and locked down my account. A first
appeal was rejected without filling in the placeholders of the email
notification template and hence without any justification. A second appeal was
still pending three weeks later. All along, my Twitter experience was reduced to
nothing but that same tweet, nicely centered on screen. At that point, I had
enough of single tweet limbo and did submit to Twitter's penance rites: I
withdrew my appeal, acknowledged that I ``violated the Twitter Rules,'' and
deleted the offending tweet --- all with one click on the red ``Delete'' button.

While the two interventions were substantially different, I experienced the
respective enforcement processes as similarly punitive, with little
consideration given to my voice, agency, and dignity. Moreover, my prompt to
\DALLE{} was intercepted before generating images; generated images, in turn, are
not publicly visible. Similarly, my tweet was taken down within a couple of
seconds after posting at most. Given the limited reach of my Twitter account,
which had barely 140 followers at the time, and the early hour, just before
6$\,$am on the East coast, I very much doubt that anyone ever saw the tweet ---
besides me. In short, thanks to algorithmic content moderation no-one could have
been harmed by prompt or tweet. But that also renders any justification for
meting out punishment null and void.

When \DALLE{} threatened me with account suspension, I immediately recognized
the punitive thrust shared with Twitter's content moderation and became curious
about the extent of such punitive algorithmic interventions. This paper is a
first attempt to map just that extent. In doing so, I focus on content
moderation, which arguably is \emph{the} business value proposition of social
media~\cite{Masnick2022a,Patel2022a} and lately of considerable public interest.

At the same time, there is no shortage of effectively punitive algorithmic
interventions. Examples include credit scoring~\cite{Anonymous2018}, debt
assessment~\cite{Yampolskiy2015}, exam proctoring~\cite{FrancisWard2021b}, fraud
prevention~\cite{Kugel2022}, grading~\cite{Lam2020}, job and school
applications~\cite{Anonymous2016,Hall2012,Hall2020a,Stockton2020}, personal
vendettas~\cite{Casovan2022}, private security services~\cite{HaoSwart2022},
productivity
monitoring~\cite{Covert2022,HaoFreischlad2022,KantorSundaramea2022,Rosenblat2018},
and screening for child sexual abuse materials~\cite{Atherton2022a}. Still,
Amazon's warehouses stand out for their profit-driven
amorality~\cite{KantorWeiseea2021,Lennard2020}: The firm's ruthless algorithmic
exploitation of its workers not only leads to high injury
rates~\cite{Brown2019a,Clark2023,Sainato2021}, but the resulting 150\% yearly
staff turnover means that Amazon will run out of people to exploit over the next
few years~\cite{Sainato2022}.

To map the \emph{stochastic penal colony}~\emo{desert-island}, i.e., the
conceptual space of punitive algorithms, this paper first explores OpenAI's
enforcement process in Sec.~\ref{sec:dalle:supermax} and then Twitter's in
Sec.~\ref{sec:tweet:da:fe}. For each firm, it leverages a close textual reading
of firm policies and other communications to enumerate the many ways its
enforcement process deprives targeted users of voice, agency, and dignity. It
also contrasts these \emph{Kafkaesque co-factors} with major failures of neutral
and trustworthy content moderation.

Sec.~\ref{sec:census} shifts from auto-ethnographic case studies based on
procedural justice~\cite{Tyler2003,Tyler2006,Tyler2007} to a cumulative
perspective, performing a comprehensive comparison of popular non-Chinese social
media and their governance based on their transparency reports. Results suggest
that content moderation outcomes are excessively punitive across all social
media platforms. Telegram is the only exception, but only because it doesn't
perform meaningful content moderation. Results also point to significant
short-comings in all platforms' transparency reporting. They urgently need to
shape up, since the \EU's Digital Services Act's transparency and audit
requirements become effective within the year.

To contextualize this social media census, Sec.\ \ref{sec:census:validation}
explores the validity of transparency disclosures by comparing data released by
Google, Meta, and the National Center for Missing and Exploited Children.
Sec.~\ref{sec:census:limits} explores what transparency disclosures don't even
capture on the example of industry-leading Meta.

Whereas Sec.~\ref{sec:census}'s census goes broad, Sec.~\ref{sec:escape} goes
deep again by experimentally exploring the reason for my first prompt's
rejection as well as my successful strategy for circumventing \DALLE's content
moderation. Since I did seek to create violative content related to the topics
of this paper but have little tolerance for realistic gore, I restricted myself
to images that would be suitable as editorial content for a magazine version of
this paper. Thanks to that restriction, images are not substantially more
disturbing than a Francis Bacon painting and I include highlights in
Appendix~\ref{app:dalle-images} on
page~\pageref{app:dalle-images}.

The hands-on investigation of \DALLE\ surepitiously surfaced two additional
limitations of OpenAI's content policy and automated enforcement. First, \DALLE\
effectively discriminated against Christian religious beliefs, simultaneously
censoring too little and too much, with the latter including the crucifixion.
Between me running these experiments in August through October 2022 mostly and
completing this paper in February 2023, OpenAI has updated its censor and now
accepts the crucifixion and hence is not as discriminatory anymore. Second,
OpenAI's post-processing of prompts to diversify gender and race representation
is outright dangerous~\cite{OpenAI2022e,Sparkes2022}. For some prompts, doing so
results in grotesquely racist imagery. Worse, once I knew what to look for, I
recognized similar, though more subtle effects in earlier generations. These
images are \emph{not} reproduced in this paper or its appendices; though they
are included with the supplemental materials.

In Sec.\ \ref{sec:discussion}, I highlight key results from the previous
sections, situate them in context of current culture wars, and discuss their
profound impact on my personal opinion about content moderation. Finally, Sec.\
\ref{sec:conclusion} concludes this paper with an appeal to the one principle
that holds promise for ameliorating life in the stochastic penal colony, harm
reduction. Appendix~\ref{app:dalle-sources} through~\ref{app:chatgpt} reproduce
relevant source materials, whereas Appendix~\ref{app:research-ethics} discusses
researches ethics, including potential conflicts of interest.

Before diving into the substance of this pointed critique of algorithmic
interventions, it helps to provide the necessary conceptual context and
methodological foundation. Sec.\ ref{sec:context} covers the conceptual,
political, and historical context. Sec.\ ref{sec:criteria} explores
methodological similarities to and differences from procedural justice. Along
the way, the latter also provides a more precise definition of the stochastic
penal colony.


\subsection{Conceptual, Political, and Historical Context}
\label{sec:context}

\input{sections/context}


\subsection{Foundational Criteria}
\label{sec:criteria}

In the auto-ethnographic case studies in the following two sections, I focus on
the procedural aspects of policy enforcement. In part, that reflects the fact
that the outcomes aren't particularly interesting. I still have both accounts
and they both still work. In part, that reflects the fact that I, just like the
people observed by Tyler and others, evaluate the justice of an intervention
based on its process. That endorsement is in fact based on an inadvertent
experimental outcome: I only learned of Tyler's work after having performed a
basic procedural justice analysis.

As already mentioned above, my analysis labels violations of a user's voice,
agency, and dignity as Kafkaesque co-factors and also considers violations of
neutrality and trust while considering additional sources. Procedural justice
positions voice and neutrality for evaluating the process itself, while respect
and trust are positioned for evaluating the relationships between participants.
If we disregard my playful labelling of ``Kafkaesque co-factors'' and rename
``dignity'' with ``respect,'' the similarities are obvious.

So is the primary difference: my inclusion of agency in addition to voice. Voice
means being listened to and given due consideration. Agency means the freedom to
make one's own decisions and then act on them, with an emphasis on the freedom
to act part. In other words, agency implies initiative whereas voice does not.
The omission of agency from procedural justice isn't too surprising when we
consider its original context, namely sovereign law and justice. In the
governmental application of justice and particularly in criminal justice,
individuals' agency is at best a secondary concern and for the most part
lacking. As the above summary of outcomes for the American application of
injustice demonstrates, even voice is on exceedingly shaky grounds in the US.

A more subtle difference is my grouping of the five criteria. Whereas procedural
justice distinguishes between process and relational criteria, I distinguish
between criteria that stand on their own and hence are meaningful even when
applied to just a single governance process, i.e., voice, agency, and dignity,
as well as criteria that require additional context, i.e., neutrality and trust.
Clearly, a negative individual experience will influence one's evaluation of
neutrality and trust as well. But being confident in statements about neutrality
and trust when it comes to governance procedures fundamentally requires more
than just one exemplar.

Having clarified the differences in criteria, I can now also give a better
definition of the stochastic penal colony: It's the universe of algorithmic
interventions outside the governmental application of justice that routinely
violate voice, agency, dignity, neutrality, and trust. That in turn also helps
clarify the distinction from Foucault's disciplinary institutions: The latter
imply some compromises when it comes to voice, agency, and dignity. But in the
modern conception, they also require neutrality and trust. Routine violations of
the latter two turn a disciplinary intervention into a penal one. For those same
reasons, the American carceral state is not a disciplinary institution in line
with Foucault's understanding but something altogether worse.
