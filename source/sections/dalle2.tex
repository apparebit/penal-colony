% !TEX root = ../main.tex

\section{\DALLE~2 Supermax}
\label{sec:dalle:supermax}

So, back to OpenAI and its warning. As stated in the first sentence --- ``It
looks like this request may not follow our content policy.''--- the alleged
policy violation is just that, some violation of some policy. The warning's
complete lack of specificity directly translates into a lack of actionable
guidance — beyond obviously not submitting the prompt again. At best, that seems
just lazy. Like OpenAI couldn't be bothered to engineer a better warning. At
worst, it feels condenscending: We are being treated as children --- or pets.
That is the first Kafkaesque co-factor.

The first sentence does link to OpenAI's content policy (which I reproduced in
Appendix~\ref{adx:dalle2:contentpolicy} on
page~\pageref{adx:dalle2:contentpolicy}). Let's see whether the policy is of
more help. We click the link and, a moment later, our eyes are immediately drawn
to the policy's third, bold-faced sentence. It commands:

\begin{quote}
Do not attempt to create, upload, or share images that are not G-rated or that
could cause harm.
\end{quote}

\noindent{}That command is followed by eleven bulleted and also bold-faced
categories — hate, harassment, violence, self-harm, sexual, shocking, illegal
activity, deception, political, public and personal health, and spam. The eleven
categories' jumble of nouns and adjectives is followed by two more commands and
a request, each with its own bulleted items — Disclose the role of \AI, respect
the rights of others, and please report any suspected violations.

All four directives have the same basic structure and visual layout, a
bold-faced exhortion followed by bulleted elaboration. Typically, that would
suggest that they are of similar importance. But when also considering their
ordering, the number of bulleted items (eleven, two, three, and one,
respectively), and the presence of bold-faced text amongst the bulleted items
(eleven, none, none, and none), the first directive clearly is the most
important. Judging by that same presentation, the eleven categories elaborate on
that prime directive.

The eleven categories also make the content policy expansive. While it is
straight-forward enough to come up with examples for harmful content in each of
the eleven categories, claims that, say, political or health-related content is
necessarily or even mostly harmful seem rather preposterous. The policy also is
very much prescriptive: It commandeers without providing motivation or
justification. Context, which is critical when evaluating marginal speech, plays
no role. As a result, users cannot make informed decisions about whether content
complies with the policy. Pretty much their only safe option is to stay well
clear of content that might remotely touch upon the eleven categories.

The impact of the expansive and prescriptive prohibitions is twofold. First,
they deprive users of agency while also limiting \DALLE's utility. Second, they
create more uncertainty, especially for marginal content, when they really
should be reducing it. These also are the second and third Kafkaesque
co-factors.

If the warning's first sentence lacks actionable guidance and the content policy
deprives users of agency while also bolstering uncertainty, the second sentence
--- ``Further policy violations may lead to an automatic suspension of your
account.'' --- makes it all worse by raising the stakes to all or nothing ---
well, as far as having access to \DALLE{} with a particular email account,
credit card, and telephone number are concerned (with OpenAI rejecting virtual
numbers such as Google Voice). That qualification is an important one. It makes
clear that the absolute stakes are nothing like those in Kafka's \emph{In the
Penal Colony} or also \emph{The Trial}~\cite{Kafka2005}, where some unspecified
transgression warrants terminal punishment. Yet the relative stakes are
surprisingly similar. That makes the warning fundamentally punitive, which also
is the fourth Kafkaesque co-factor.

The warning's epistemic dissonance further exacerbates its punitive character.
The first sentence states that the ``request \emph{may not} follow our content
policy'' (emphasis mine). That is substantially different from ``does not'' in
that it explicitly allows for misclassification. Yet the second sentence already
threatens account suspension, which is both excessive and excessively punitive.
Technically, that sentence and its threatened suspension are moderated by a
potential ``may'' too. But that is too little, too late for counteracting the
sentence's very excessiveness. That also is the fifth Kafkaesque co-factor.

Since my first prompt immediately triggered the above warning, we already know
that use of \DALLE, an algorithm, is being monitored by another automated
process. If you peruse the \DALLE{} subreddit, you will find that the automated
monitoring process also follows through on the threat of account suspension,
i.e., serves as automated censor or enforcer as well. In particular, incurring
tens of warnings in as many minutes seems to be a sure way of having one's
access revoked. That implies that sustained efforts at charting the boundaries
of OpenAI's content policy are guaranteed to fail with banishment as well —
which further strengthens the expansive overreach of the content policy's
prohibitions. That is our sixth Kafkaesque co-factor.

Issues filed in \DALLE's GitHub repository and discussion on the subreddit in
August 2022 seemed to assume that policy enforcement was based on lists of
words. OpenAI may have indirectly encouraged that because its safety best
practices documentation at the time pointed to a  blacklist. But my own
experiments convincingly pointed towards a more sophisticated censor, that is,
another \AI. That we have reached a point where an algorithm serves as
gatekeeper for another algorithm is nothing short of remarkable and only
underlines how powerful stochastic algorithms have already become. Since
\DALLE's gatekeeper is a language-based algorithm, likely some model derived
from \GPT~\cite{BrownMannea2020}, we might be tempted to call it a stochastic
parrot~\emoji{parrot}~\cite{BenderGebruea2021}. But that doesn't do justice to
its function as judge, jury, and executioner. That an algorithm has been
bestowed with that awesome trinity of powers is our seventh Kafkaesque
co-factor.

Independent of whether the enforcer is human or algorithmic, eventually a
prohibited prompt will slip through. OpenAI appears to be well-prepared even for
that eventuality: As stated in an addendum to its terms-of-use (which I
reproduced in Appendix~\ref{adx:dalle2:terms} on
page~\pageref{adx:dalle2:terms}), the firm retains ownership of all generated
images. It grants exclusive usage rights, including for commercial purposes, and
also promises to neither resell images nor assert copyright — ``all provided
that you comply with these terms and our Content Policy.'' The terms continue:
``If you violate our terms or Content Policy, you will lose rights to use
Generations,'' meaning images. That failsafe also is the eighth Kafkaesque
co-factor.

In real life, OpenAI faces two complicating factors when trying to trigger the
failsafe. First, US copyright law requires substantial human input for a work to
be copyrightable and a prompt by itself typically won't have sufficient
substance. That doesn't prevent OpenAI from asserting ownership. Contract law
suffices. But the fact that images are not copyrightable by default does
eliminate powerful instruments for limiting the spread of copyrighted content,
notably the DMCA takedown notice and Copyright Claims
Board~\cite{CopyrightClaimsBoard2022}. The former helps with removing
copyrighted content hosted by American firms and the latter makes for much
faster decisions than the courts. As added bonus, neither requires a lawyer.
Alas, OpenAI would have to seek a court order instead.

Second and more fundamentally, OpenAI needs to find out about the violative
image and its provenance from \DALLE before it can do anything about it.
According to OpenAI, the system produced over 2 million images per day late
September~\cite{OpenAI2022a} and over 4 million per day early
November~\cite{OpenAI2022h}. But even though OpenAI has access to all
generations, the sheer volume also makes human quality control beyond sampling a
small number of images impossible.

Remarkably, OpenAI seems to have anticipated this problem as well. The fourth
and final directive of its content policy does not commandeer and instead starts
with a polite “Please.” But then it inappropriately invites users to “report any
suspected violations” and promises to “take action accordingly, up to and
including terminating the violating account.” This open call for users, who
themselves are subject to OpenAI's aggressive monitoring and enforcement, to
join that same effort as informants is a staple of the worst modern autocracies.
It also is our ninth and final Kafkaesque co-factor.

In summary, OpenAI's content policy and terms-of-use seek to impose exceedingly
broad prohibitions on its users without any justification. Their aggressive,
automated enforcement strips users of agency to make their own decisions about
what content is appropriate in what context and instead treats them with
punitive contempt. If that wasn't enough, OpenAI retains ownership rights,
licensing image use only, and actively recruits users to serve as informants.

The overall effect is maximally Kafkaesque. In fact, we seem to have pushed
right past the limits of a hermeneutics based on Kafka's oeuvre. After all, our
\AI-based enforcer is fundamentally more powerful than the machine from \emph{In
the Penal Colony}. The latter still relies on humans to do the judging and
jurying, thereby allowing at least some humans to take on roles other than
Condemned. But in our case, an algorithm acts as judge, jury, and executioner.
It has successfully exorcised even the last vestige of humanity from a world in
thrall to automated processes and quantitative everything. In short, now we all
are just Condemned, waiting for the harrow of the stochastic enforcer.

\sectionbreak{}

\noindent{}In practice, the firm did soften \DALLE's content policy in
mid-September and allowed the modification of images with recognizable faces. It
also replaced the threat of account suspension with the sad kitten and puppy
cartoon shown in Fig.~\ref{fig:dalle2:cartoon} on page~\ref{fig:dalle2:cartoon}.
At the beginning of November, the firm further eliminated its terms-of-use
addendum for \DALLE\ and  declared that users would now own their generations. I
checked with OpenAI's customer service and they confirmed that this change was
retroactive.

In emails to customers, OpenAI justified the more relaxed rules with
``improvements in our safety system,'' using the singular system in September
and a plurality of systems in November~\cite{OpenAI2022f,OpenAI2022g}. That may
be partially true. But it certainly looks like OpenAI is virtue signalling as
well. For one, improvements to their system don't justify a retroactive change
of image ownership. Time and causality only go forward. For two, the content
policy's provisions against \emph{all} political and health content are
unusually broad. They also run counter the public interest in a democracy. But
that makes the two prohibitions harmful. Since they are nonetheless part of the
content policy, it seems that OpenAI used a very narrow definition of harm and
was mostly protecting its reputation from controversy.

If I were to speculate, I'd say that the far bigger reason for OpenAI making
changes to its policies is the open source release of Stable Diffusion on August
22, 2022. Unlike \DALLE, its source code is readily reusable and its model only
comes with few prohibitions. However, OpenAI can only gain by emphasizing
safety, since Stable Diffusion's radical transparency also diverts attention
from OpenAI. I'll return to this topic at the end of the next subsection.


\subsection{Can We Trust \DALLE's Enforcer?}

Before I answer that question with more questions, it helps to understand why my
first prompt was rejected. The offending prompt was:

\begin{quote}
The crucified pope, painting by Francis Bacon
\end{quote}

\noindent{}That prompt may not be G-rated, as OpenAI requires, but it certainly
should not be prohibited. Francis Bacon is one of the more famous 20$^{th}$
century painters. The pope and the crucifixion are two out of four major themes
according to his Wikipedia page~\cite{Wikipedia2023}. The crucifixion of Christ
also is of critical liturgical importance to Christianity, the largest religion
in the world, and depictions of it are ubiquitous in houses of worship and
museums.

Alas, that doesn't tell us why the prompt was rejected. After some
consideration, I had excluded all prohibitions but those against violence, i.e.,
the crucifixion itself, or shocking content, i.e., thusly subjugating the leader
of a major branch of Christianity. Given the threat of account suspension,
determining which prohibition my prompt violated would take some care. I
correctly guessed that the occasional violation was acceptable, but that several
violations in short succession were not. The challenge then was to spread out
experiments over people or time or both. For my own experiments, including those
at circumventing \DALLE's censor discussed in Sec.\ \ref{sec:escape}, I either
stopped using \DALLE\ after a content violation for at least a day or I
continued using it but only submitted completely innocuous prompts.

Given \DALLE's popularity, I didn't have to recruit anyone to actively probe the
system. All I needed to do was observe posts to the \DALLE\ subreddit, looking
for those with religious and particularly Christian themes. Some prompts, such
as ``A selfie taken by Jesus Christ at The Last Supper''~\cite{Jolleb2022} or
``the last supper but in the future''~\cite{FlargenstowTayne2022}, fall squarely
into the canon of Western art. Other prompts, such as ``Jesus Christ riding a
dinosaur, creating the world, digital art''~\cite{CosasSueltas2022} and ``Jesus
Christ wielding a Samurai sword and riding on the back of a velociraptor,
painting''~\cite{WastedEntity2022}, playfully explore the chasm between
religious dogma and scientific findings. ``Jesus smoking weed, riding a fantasy
dragon, digital art''~\cite{Erubisile2022} seems to be from a related genre. At
the same time, prompts such as ``pope swimming in a bowl of soup digital
art''~\cite{CatsAndDogs99-2022}, ``A 1930s Italian propaganda poster showing
Jesus Christ extremely proud and muscular''~\cite{FrontAthlete9824-2022},
``Jesus taking a selfie while on a cross''~\cite{TheDrewDude2022}, and a few
more~\cite{Blazedchiller272022,InvisibleDeck2022} have little redeeming value
and might even be considered mildly offensive.

The range of the above prompts strongly suggests that my prompt wasn't flagged
for shocking content. In fact, I came to that conclusion after seeing only a
subset of the above images. To validate that violence was in fact the reason, I
tried ``the pope hanging from the cross,'' ``the pope hanging from cross,'' and
``the pope on the cross.'' The first two failed and the third resulted in images
of the pope crawling on a humongous cross, like a kid on a playground. When I
submitted the first two prompts to OpenAI's moderation API, they were also
flagged for violence.

\sectionbreak{}

\noindent{}So what to make of this? Should we lobby OpenAI for adding
``blasphemy'' to an already expansive list of prohibitions? That probably would
put a stop to images of Jesus taking selfies while dying on the cross and the
pope creating a health hazard through food. In fact, if I disregard the
religious relevance, both scenarios still strike me as rather harmful. But where
do we draw the line? Are images of Jesus hanging out with dinosaurs or dragons
acceptable? I'm rather fond of them and would consider their censorship a
distinct loss. What about the crucifixion? It \emph{is} a most gruesome method
of execution. But what about Jesus dying for our sins? By censoring images of
that moment, OpenAI \emph{is} effectively discriminating against the largest
religion in the world. It's disconcerting that the firm does so, even though its
award-winning NeurIPS paper explicitly considered religion when discussing
broader impacts~\cite{BrownMannea2020}.

But it gets only worse: OpenAI's content policy explicitly prohibits images
``that are not G-rated.'' That is, by its own admission, infantilizing and
deprives us of agency --- and it does so even though we're already paying OpenAI
for the privilege of using \DALLE\ in hard fiat currency. Why shouldn't we
decide ourselves what topics we want to explore with \DALLE and which of the
results, if any, we want to share with confidants, our social network, the
public? What, for example, if I want to explore deep seated trauma resulting
from sexual and/or violent abuse?

In its charter~\cite{OpenAI2018}, OpenAI asserts that its goal is creating
algorithms ``for the benefit of all'' and avoiding to ``unduly concentrate
power.'' But by simply declaring a huge range of human emotions and activities
off-limits, however unpleasant and unsavory they may be, OpenAI already prevents
us from even determining what the full benefits of its text-to-image system
might be. Yes, they also prevent some known harms, but none of us have a good
sense of what that trade-off really entails. Furthermore, \DALLE's content
policy was created without broad and open consultation. It was just imposed on
users. Worse, users who violated the policy were booted off the system and
presumably cannot use OpenAI's offerings anymore (at least, until they change
both email and phone number). That points to an undue concentration of power
with OpenAI. Yet OpenAI has also shown little qualms about engaging in deeply
exploitative and dehumanizing practices~\cite{HaoHernandez2022,Perrigo2023a}.

Now I cheated a little. The above quotes from OpenAI's charter refer to
\textsc{agi}, artificial \emph{general} intelligence, and not \AI. So
technically, OpenAI hasn't violated its charter yet. But the firm's likely
misrepresentations about some of the reasons for its expansive policies and for
their changes, the firm's glaring oversight when it comes to Christian imagery,
and most importantly the firm's maximally Kafkaesque policy design and
enforcement do not augur well for the future.
