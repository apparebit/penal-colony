\section{DALL•E~2 Supermax}
\label{sec:dalle}

Back to OpenAI and its warning: As stated in the first sentence --- ``It looks
like this request may not follow our content policy.''--- the alleged policy
violation is just that, some violation of some policy. The warning's complete
lack of specificity directly translates into a lack of actionable guidance —
beyond obviously not submitting the prompt again. At best, that seems just lazy.
Like OpenAI couldn't be bothered to engineer a better warning. At worst, it
feels condenscending: We are being treated as children --- or pets. That is the
first Kafkaesque co-factor.

The first sentence does link to OpenAI's content policy (which I reproduced in
Appendix~\ref{app:dalle-content-policy} on
page~\pageref{app:dalle-content-policy}). Let's see whether the policy is of
more help. We click the link and, a moment later, our eyes are immediately drawn
to the policy's third, bold-faced sentence. It commands:

\begin{quote}
\openfat{}Do not attempt to create, upload, or share images that are not
G-rated or that could cause harm.\closefat{}
\end{quote}

\noindent{}That command is followed by eleven bulleted and also bold-faced
categories — hate, harassment, violence, self-harm, sexual, shocking, illegal
activity, deception, political, public and personal health, and spam. The eleven
categories' jumble of nouns and adjectives is followed by two more commands and
a request, each with its own bulleted items — Disclose the role of \AI, respect
the rights of others, and please report any suspected violations.

All four directives have the same basic structure and visual layout, a
bold-faced exhortion followed by bulleted elaboration. Typically, that would
suggest that they are of similar importance. But when also considering their
ordering, the number of bulleted items (eleven, two, three, and one,
respectively), and the presence of bold-faced text amongst the bulleted items
(eleven, none, none, and none), the first directive clearly is the most
important. Judging by that same presentation, the eleven categories elaborate on
that prime directive.

The eleven categories also make the content policy expansive. While it is
straight-forward enough to come up with examples for harmful content in each of
the eleven categories, claims that, say, political or health-related content is
necessarily or even mostly harmful seem rather preposterous. The policy also is
very much prescriptive: It commandeers without providing motivation or
justification. Context, which is critical when evaluating marginal speech, plays
no role. As a result, users cannot make informed decisions about whether content
complies with the policy. Pretty much their only safe option is to stay well
clear of content that might remotely touch upon the eleven categories.

The impact of the expansive and prescriptive prohibitions is twofold. First,
they deprive users of agency while also limiting \DALLE's utility. Second, they
create more uncertainty, especially for marginal content, when they really
should be reducing it. These also are the second and third Kafkaesque
co-factors.

If the warning's first sentence lacks actionable guidance and the content policy
deprives users of agency while also bolstering uncertainty, the second sentence
--- ``Further policy violations may lead to an automatic suspension of your
account.'' --- makes it all worse by raising the stakes to all or nothing ---
well, as far as having access to \DALLE{} with a particular email account,
credit card, and telephone number are concerned (with OpenAI rejecting virtual
numbers such as Google Voice). That qualification is an important one. It makes
clear that the absolute stakes are nothing like those in Kafka's \emph{In the
Penal Colony} or also \emph{The Trial}~\cite{Kafka2005}, where some unspecified
transgression warrants terminal punishment. Yet the relative stakes are
surprisingly similar. That makes the warning fundamentally punitive, which also
is the fourth Kafkaesque co-factor.

The warning's epistemic dissonance further exacerbates its punitive character.
The first sentence states that the ``request \emph{may not} follow our content
policy'' (emphasis mine). That is substantially different from ``does not'' in
that it explicitly allows for misclassification. Yet the second sentence already
threatens account suspension, which is both excessive and excessively punitive.
Technically, that sentence and its threatened suspension are moderated by a
potential ``may'' too. But that is too little, too late for counteracting the
sentence's very excessiveness. That also is the fifth Kafkaesque co-factor.

Since my first prompt immediately triggered the above warning, we already know
that use of \DALLE, an algorithm, is being monitored by another automated
process. If you peruse the \DALLE{} subreddit, you will find that the automated
monitoring process also follows through on the threat of account suspension,
i.e., serves as automated censor or enforcer as well. In particular, incurring
tens of warnings in as many minutes seems to be a sure way of having one's
access revoked. That implies that sustained efforts at charting the boundaries
of OpenAI's content policy are guaranteed to fail with banishment as well —
which further strengthens the expansive overreach of the content policy's
prohibitions. That is our sixth Kafkaesque co-factor.

Issues filed in \DALLE's GitHub repository and discussion on the subreddit in
August 2022 seemed to assume that policy enforcement was based on lists of
words. OpenAI may have indirectly encouraged that because its safety best
practices documentation at the time pointed to a  blacklist. But my own
experiments convincingly pointed towards a more sophisticated censor, that is,
another \AI. That we have reached a point where an algorithm serves as
gatekeeper for another algorithm is nothing short of remarkable and only
underlines how powerful stochastic algorithms have already become. Since
\DALLE's gatekeeper is a language-based algorithm, likely some model derived
from \GPT~\cite{BrownMannea2020}, we might be tempted to call it a stochastic
parrot~\emo{parrot}~\cite{BenderGebruea2021}. But that doesn't do justice to
its function as judge, jury, and executioner. That an algorithm has been
bestowed with that awesome trinity of powers is our seventh Kafkaesque
co-factor.

Independent of whether the enforcer is human or algorithmic, eventually a
prohibited prompt will slip through. OpenAI appears to be well-prepared even for
that eventuality: As stated in an addendum to its terms-of-use (which I
reproduced in Appendix~\ref{app:dalle:terms} on
page~\pageref{app:dalle:terms}), the firm retains ownership of all generated
images. It grants exclusive usage rights, including for commercial purposes, and
also promises to neither resell images nor assert copyright — ``all provided
that you comply with these terms and our Content Policy.'' The terms continue:
``If you violate our terms or Content Policy, you will lose rights to use
Generations,'' meaning images. That failsafe also is the eighth Kafkaesque
co-factor.

In real life, OpenAI faces two complicating factors when trying to trigger the
failsafe. First, US copyright law requires substantial human input for a work to
be copyrightable and a prompt by itself typically won't have sufficient
substance. That doesn't prevent OpenAI from asserting ownership. Contract law
suffices. But the fact that images are not copyrightable by default does
eliminate powerful instruments for limiting the spread of copyrighted content,
notably the DMCA takedown notice and Copyright Claims Board~\cite{CCB2022}. The
former helps with removing copyrighted content hosted by American firms and the
latter makes for much faster decisions than the courts. As added bonus, neither
requires a lawyer. Alas, OpenAI would have to seek a court order instead.

Second and more fundamentally, OpenAI needs to find out about the violative
image and its provenance from \DALLE before it can do anything about it.
According to OpenAI, the system produced over 2 million images per day late
September~\cite{OpenAI2022a} and over 4 million per day early
November~\cite{OpenAI2022h}. But even though OpenAI has access to all
generations, the sheer volume also makes human quality control beyond sampling a
small number of images impossible.

Remarkably, OpenAI seems to have anticipated this problem as well. The fourth
and final directive of its content policy does not commandeer and instead starts
with a polite “Please.” But then it inappropriately invites users to “report any
suspected violations” and promises to “take action accordingly, up to and
including terminating the violating account.” This open call for users, who
themselves are subject to OpenAI's aggressive monitoring and enforcement, to
join that same effort as informants is a staple of the worst modern autocracies.
It also is our ninth and final Kafkaesque co-factor.

In summary, OpenAI's content policy and terms-of-use seek to impose exceedingly
broad prohibitions on its users without any justification. Their aggressive,
automated enforcement strips users of agency to make their own decisions about
what content is appropriate in what context and instead treats them with
punitive contempt. If that wasn't enough, OpenAI retains ownership rights,
licensing image use only, and actively recruits users to serve as informants.

The overall effect is maximally Kafkaesque. In fact, we seem to have pushed
right past the limits of a hermeneutics based on Kafka's oeuvre. After all, our
\AI-based enforcer is fundamentally more powerful than the machine from \emph{In
the Penal Colony}. The latter still relies on humans to do the judging and
jurying, thereby allowing at least some humans to take on roles other than
Condemned. But in our case, an algorithm acts as judge, jury, and executioner.
It has successfully exorcised even the last vestige of humanity from a world in
thrall to automated processes and quantitative everything. In short, now we all
are just Condemned, waiting for the harrow of the stochastic enforcer.


\subsection{OpenAI's Neutrality and Trustworthiness}

Before \DALLE~2's beta opened up the system to users like me, it was available
to a much smaller number of users, more like a research experiment. The system
card~\cite{GreenProcopeea2022,ProcopeCheemaea2022} for the original release in
April 2022 makes clear that \DALLE~2 was, in part, trained with ``publicly
available sources''~\cite{MishkinAhmad2022}, which in all likelihood includes
Internet-sourced data similar to the LAION-400M
dataset~\cite{SchuhmannVencuea2021}. While OpenAI has declined to elaborate on
the exact sources for \DALLE's training data, we know that such Internet-sourced
datasets are anything but safe~\cite{BirhanePrabhuea2021}. That makes \DALLE{}
unsafe by design.

As shown in Appendix~\ref{app:dalle-content-policy} on
page~\pageref{app:dalle-content-policy}, \DALLE's content policy comes right out
against anything ``that could cause harm.''. In their \FAQ{} entry for \DALLE's
warnings~\cite{Natalie2022}, OpenAI claims that ``safe usage of the platform is
our highest priority.'' Yet \DALLE's content policy disallows \emph{all}
political and health content. Those prohibitions are not just unusually broad,
they also run directly counter the public interest in a democracy. At the same
time, health, like politics, has become exceedingly partisan during the
pandemic. That suggests a very specific kind of harm OpenAI seeks protection
from --- harm to its own reputation.

When OpenAI gave up control over generations in early November 2022, the email
announcement justified that change with ``improvements in our safety
systems''~\cite{OpenAI2022g}. That may be the case for generations created after
those improvements were made. But when I asked customer support about
generations made before the announcement, they confirmed that the new
terms-of-use apply retroactively. In fact, OpenAI deleted the webpage with the
terms-of-use addendum for \DALLE. But if \DALLE's censor required improvements,
then chances are that older images are unsafe. Otherwise, there would have been
no need for further safety improvements. Yet OpenAI pretends it can have it both
ways.

What else might have put OpenAI under significant pressure to relinquish
ownership in generations? What about Stability~AI releasing not just source code
but also model weights for Stable Diffusion in August
2022~\cite{StabilityAI2022}? Unlike \DALLE, that public release enables anyone
with basic fluency in Python and access to recent graphics cards by Nvidia to
open their own competitor to OpenAI's system, without even paying a license fee
to Stability~AI. While that is pure speculation on my part, Microsoft's \$10
billion investment in OpenAI in January 2023 on top of an earlier \$1 billion
investment illustrates the stakes at play~\cite{Bass2023}.

When taken together, the above observations suggest that OpenAI has access to
stellar public relations and legal talent --- and is rather hypocritical. They
also seem very much in line with Karen Hao's 2020 portrait of the firm, which
explored the tension between OpenAI's origin as a research lab and its current,
more commercial orientation~\cite{Hao2020}. However, more recent reports about
the firm's exploitative use of labor in Latin America and Africa for labelling
content~\cite{HaoHernandez2022,Perrigo2023a} is far more problematic because
those practices do harm people and hence raise grave doubts about the firm's
ability to follow through on its own charter~\cite{OpenAI2018}.
