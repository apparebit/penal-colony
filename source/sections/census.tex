\section{A Social Media Census}
\label{sec:census}

In her 2018 article for the Harvard Law Review~\cite{Klonick2018}, Kate Klonick
traces the early history of content moderation at Facebook, Twitter, and
YouTube. She also discusses their privileged position when it comes to speech,
and proposes a new label, and also legal role, to go along with that,
\emph{governance}. Her claims about social media's grounding in the tradition of
free speech seem wilfully oblivious to capitalist reality, which, for example,
grants Mr Zuckerberg over 50\% of voting shares for Meta and hence autocratic
control~\cite{LauricellaNorton2021}. But her notion of governance is compelling.
Alas, governance requires accountability requires transparency. This section
tries to provide just that by comparing non-Chinese social media based on their
content policies and transparency reports. For good measure, I am also including
the \EU's Digital Services Act or \DSA~\cite{EuropeanParliamentAndCouncil2022}.

To determine which social media to include in the census, I started with
Buffer's list of the 20 most popular platforms~\cite{Lua2022}. I treat Facebook
and Instagram as one platform, since Meta sanctions users based on their posts
to either social network. I drop the five platforms targeting China, since they
are unlikely to adhere to Klonick's governance model. I also drop Facebook
Messenger, Microsoft Teams, Skype, and WhatsApp, since they mostly enable
private communication between users. While that arguably is the predominant use
case for Telegram, too, its stickers, channels, and bots are public and require
moderation, as even Telegram acknowledges in their \FAQ. Hence, Telegram
remains. Finally, I add Tumblr because that social network has become a popular
destination for users leaving Twitter and employees laid off by Twitter in the
wake of Elon Musk's takeover of the firm~\cite{Patel2022}.

To determine the criteria for the census, I iterated a few times over a list
gleaned from social media transparency reports, previous
studies~\cite{BradfordGriselea2019,CrockerGebhartea2019}, best practices
recommendations~\cite{AccessNowACLUFoundationOfNorthernCaliforniaea2021}, and
the \EU's \DSA. For each iteration, I also filled in
Table~\ref{table:governance} below. The process converged only after I started
grouping individual criteria according to major aspects of content moderation,
which helped reason about the coverage and completeness. The resulting hierarchy
follows, with emoji serving as column headers in the aforementioned table.

% \hfill pushes the nested enumerations onto the next line!

\begin{description}
\item[The violative content itself]\hfill
    \begin{enumerate}
        \item \emo{receipt}~Prevalence of violative content measured as fraction
            of all content
        \item \emo{triangular-ruler}~Reach of violative content measured in time
            or better views
    \end{enumerate}
\item[The entity reporting violative content]\hfill
    \begin{enumerate}[resume]
        \item \emo{classical-building}~Government inquiries and removal
            requests; also intellectual property claims
        \item \emo{enraged-face}~Platform users
        \item \emo{robot}~Algorithmic enforcement
        \item \emo{judge}~Human moderation, including its mental health impact
    \end{enumerate}
\item[Sanctions on content and users]\hfill
    \begin{enumerate}[resume]
        \item \emo{label}~Warning labels (increasingly used for misinformation)
        \item \emo{foggy}~Visibility reduction and blacklists (aka shadow
            banning)
        \item \emo{wastebasket}~Content removal
        \item \emo{biohazard}~Sanctions on users
    \end{enumerate}
\item[Due process protections]\hfill
    \begin{enumerate}[resume]
        \item \emo{pager}~Meaningful notification
        \item \emo{balance-scale}~Appeals and reversals
        \item \emo{eye}~External oversight, including conflict resolution and auditing
    \end{enumerate}
\item[Cultural proficiency and fluency]\hfill
    \begin{enumerate}[resume]
        \item \emo{globe-africa-europe}~Different countires and cultures
    \end{enumerate}
\end{description}

Table~\ref{table:governance} shows the results of the social media census. Eight
platforms have published content policies and release transparency reports
covering their content moderation. However, three platforms fall short of even
these basic niceties of governance. Telegram has no policy and makes no
transparency disclosures. Quora has a policy but makes no disclosures. Finally,
Tumblr has a policy but discloses only governmental requests and intellectual
property claims. While the platforms' prohibitions have strikingly similar
names, prior work has demonstrated substantial differences in definitions and
enforcement~\cite{FieslerJiangea2018,PaterKimea2016}. Still, a couple of
prohibitions do stick out. Pinterest's ``Harassment and criticism'' is chilling
in its pathologizing overreach. Then again, Tumblr's ``Misattribution or
Non-Attribution'' is rather endearing---but probably not that practical.

\begin{table}
\caption{A survey of governance practices for social media, with a checkmark
    indicating release of \emph{some} statistics for that criterion.
    \emph{Account Sanctions} are coded W for warning, a number for as many days
    of forced timeout, and X for account suspension. The \emo{robot} robot and
    \emo{judge} judge columns share the \emo{keycap-one} number one when a
    transparency report does not distinguish between in-house automated and
    human review.}
\label{table:governance}
\begin{tabular}{LCcCc@{\;}c@{\quad}c@{\;}c@{\;}c@{\;}c@{\quad}c@{\;}c@{\;}c@{\;}c@{\quad}c@{\;}c@{\;}c@{\quad}cC}

\multicolumn{2}{C}{\textbf{Platform}} & \textbf{Account} & \textbf{Latest}
& \multicolumn{14}{c}{\textbf{Included in Transparency Report\T}} & \\

\multicolumn{2}{C}{\textbf{Name \& Launch}}
& \textbf{Sanctions}
& \textbf{Report}
& \emo{receipt}
& \emo{triangular-ruler}
& \emo{classical-building}
& \emo{enraged-face}
& \emo{robot}
& \emo{judge}
& \emo{label}
& \emo{foggy}
& \emo{wastebasket}
& \emo{biohazard}
& \emo{pager}
& \emo{balance-scale}
& \emo{eye}
& \emo{globe-africa-europe} \T\B
& \multirow{-2}{*}{\emo{floppy-disk}} \\ \hline

\href{https://transparency.fb.com/policies/community-standards/}{FB/IG}
& 2003
& \href{https://transparency.fb.com/enforcement/taking-action/restricting-accounts/}{W,1,3,7,30,X}
& \href{https://transparency.fb.com/data/community-standards-enforcement/}{Q3 2022}
& \MK & & \MK & \MK & \MKONE &
& & \MK & & \MK & \MK & \MK & &
\href{https://transparency.fb.com/sr/community-standards/}{CSV} \T\\

\href{https://www.linkedin.com/legal/professional-community-policies}{LinkedIn}
& 2003
& & \href{https://about.linkedin.com/transparency/community-report}{H1 2022}
& & & \MK & \MK & \MKONE
& & & \MK & & & & & & \\

\href{https://policy.pinterest.com/en/community-guidelines}{Pinterest}
& 2010
& & \href{https://policy.pinterest.com/en/transparency-report}{H1 2022}
& & \MK & \MK & \MK & \MK & \MK & &
& \MK & \MK & & \MK & & \MK & \\

\href{https://help.quora.com/hc/en-us/articles/360000470706-Platform-Policies}{Quora}
& 2010
& & & & & & & & & & & & & & & & & \\

\href{https://www.redditinc.com/policies/content-policy}{Reddit}
& 2005
& \href{https://www.redditinc.com/policies/transparency-report-2021-2/}{W,3,7,X}
& \href{https://www.redditinc.com/policies/transparency-report-2021-2/}{2021}
\hidden{\,/\,\href{https://www.redditinc.com/policies/mid-year-transparency-report-2022}{H1
2022}
\href{https://www.reddit.com/r/redditsecurity/comments/rgikn1/q3_safety_security_report/}{Q3
2021}}
& \MK & & \MK & \MK & \MK & \MK & & & \MK & \MK & \MK & \MK & & \MK & \\

\href{https://values.snap.com/privacy/transparency/community-guidelines}{Snap}
& 2011
& & \href{https://values.snap.com/privacy/transparency}{H1 2022}
& & \MK & \MK & \MK & & & &
& \MK & \MK & & & & \MK &
\href{https://assets.ctfassets.net/kw9k15zxztrs/4aBF69gFiv4l9x7LzAgY8X/578e44f80fc51d0ebc52e04fe3d8c857/Snap_H1_2022_TR.csv}{CSV}\\

Telegram & 2013 & & & & & & & & & & & & & & & & & \\

\href{https://www.tiktok.com/community-guidelines}{TikTok}
& 2017
& & \href{https://www.tiktok.com/transparency/en/community-guidelines-enforcement-2022-3/}{Q3 2022}
& \MK & \MK & \MK & \MK & \MKONE & &
& \MK & \MK & & & & \MK &
\href{https://sf16-va.tiktokcdn.com/obj/eden-va2/nuvlojeh7ryht/Transparency_CGE_2022Q3/English_CGE_2022Q3.xlsx}{XLS} \\

\href{https://www.tumblr.com/policy/en/community}{Tumblr}
& 2007
& & \href{https://transparency.automattic.com/tumblr/}{H1 2022}
& & & \MK & & & & & & & & & & & & \\

\href{https://help.twitter.com/en/rules-and-policies/twitter-rules}{Twitter}
& 2006
& \href{https://help.twitter.com/en/rules-and-policies/enforcement-options}{W/0.5--7/X}
& \href{https://transparency.twitter.com/en/reports/rules-enforcement.html#2021-jul-dec}{H2 2021}
& & \MK & \MK & \MK & \MKONE &
& & \MK & \MK & & & & \MK &
\href{https://transparency.twitter.com/content/dam/transparency-twitter/download/rules_enforcement_test.zip}{CSV} \\

\href{https://www.youtube.com/howyoutubeworks/policies/community-guidelines/}{YouTube}
& 2005
& \href{https://support.google.com/youtube/answer/2802032}{W,7,14,X}
& \href{https://transparencyreport.google.com/youtube-policy/removals}{Q3 2022} \B
& \MK & \MK & \MK & \MK & \MK & \MK & & & \MK & \MK & & \MK & & \MK & \\
\hline

\multicolumn{4}{L}{\textbf{Required by EU Digital Services Act\T}} & &
& \emo{eu} & \emo{eu} & \emo{eu} & & \emo{eu} & \emo{eu} & \emo{eu} & \emo{eu} &
\emo{eu} & \emo{eu} & \emo{eu} & & \emo{eu} \\

\end{tabular}
\end{table}

When it comes to sanctioning accounts, \emph{all} social media (besides
Telegram) fall in one of two groups. The first group seems to model their
sanctions on the Queen of Hearts from \emph{Alice's Adventures in Wonderland},
enthusiastically screaming ``Off with their heads!'' and then even figuratively
following through~\cite{Carroll2008}. The second group manages to improve on
that (low) standard, but only barely, by modelling their sanctions on
California's 1994 ``three strikes'' law~\cite{Vitiello2002}. A decade later, the
law accounted for a quarter of the state's prisoners, but did not curb
crime~\cite{BrownJolivette2005}, increased homicides~\cite{MarvellMoody2001},
and led to even more Black men being locked up for even
longer~\cite{BrownJolivette2005}. Three decades after enactment, the most
salient change is that the law accounts for a third of all the state's
prisoners~\cite{BirdGillea2022}. In short, the uniformity and severity of these
sanctions turns all social media into wardens of the stochastic penal colony.

That conclusion does not change if I avoid the damning analogues and restrict
the framing to content moderation only: When it comes to judging the severity of
violative content (with possible exception of child sexual abuse material and
terrorism), Americans, including professional moderators, and international
audiences alike reason along many fine-grained axes and thereby make subtle
distinctions; international audiences only add more variability, depending on
culture~\cite{JiangScheuermanea2021,ScheuermanJiangea2021}. Furthermore,
\AI-based enforcers increasingly intercept content \emph{before} it is posted.
Notably, that is already the case for the first, second, and fourth most popular
platforms, Facebook, YouTube, and Instagram~\cite{Lua2022}. But if an algorithm
intercepts violative content before posting, there is \emph{no} human harm and
hence no justification for punishment---a seemingly inconvenient fact ignored
by social media.

In addition to confirming my hypothesis about being overly punitive, the
sparsity of checkmarks---especially when it comes to disclosing ``soft''
content sanctions, notifying users, and handling appeals---provides a rude
counterpoint to Klonick's cheerful enthusiasm for social media governance.
Worse, as the \emph{Launch} years make clear, all but TikTok had one to two
decades for making the necessary investments into content moderation processes
and transparency reporting.

That won't do for much longer---unless platforms are willing to forgo the
European market. As the bottom table row illustrates, the \EU's \DSA\ includes
extensive requirements for transparency reporting---as well as for independent
yearly audits of those reports. Furthermore, it does not leave much time for
becoming compliant. By February 2023, all platforms must have reported their
user numbers to the \EU, so that the Commission can determine which of the
\DSA's tiered requirements apply. Once the Commission has designated a platform
as \emph{very large} (the \DSA's largest tier with the most requirements), the
firm has only four months to comply with the law. Smaller platforms have until
February 2024. Penalties for non-compliance are steep: Each fine is capped at
6\% of the firm's global revenue for the previous year.

By design, Table~\ref{table:governance} largely obscures the subpar presentation
and organization of social media platforms' transparency disclosures. It is
utterly ridiculous that five out of nine \emph{technology} firms can't be
bothered to release their data in machine-readable form; it is unacceptable to
the \EU, too. Next, ambiguously named prohibitions make for confusing reading of
policies and transparency reports alike. For example, ``violence'' may refer to
the graphic depiction thereof or to extremist groups making use thereof.
``Hate'' may cover prejudiced statements against members of a protected group or
organized groups espousing supremacist ideology. Next, different platforms
report different metrics for the same aspect of content moderation, if they
report it at all. For instance, Pinterest, Snap, TikTok, Twitter, and YouTube
report statistics on the reach of violative content. Yet Pinterest, Twitter, and
YouTube report binned view counts, Snap median minutes, and Tiktok takedowns
within 24 hours as well as without views. For historical, reasons, most
platforms also break out government requests and intellectual property claims
into their own sections or even reports. In the latter case, it is up to users
to notice that the transparency report's statistics are incomplete and manually
combine statistics from three different reports. Still, Pinterest's and
YouTube's disclosures stand out for being better organized and presented than
those of other platforms, with YouTube's coming closest to my own.

Finally, social media's transparency reports tell vastly different stories about
the role of \AI. As already discussed in Section~\ref{sec:tweet-da-fe}, Twitter
is in public denial about its aggressive use of algorithms. In contrast,
Pinterest is very exacting and distinguishes between fully automated and hybrid
detection for each prohibition. In the latter case, algorithms pick violative
content too, but humans need to approve the violation before the system acts on
it. Next, Facebook, Instagram, Twitter, and YouTube have largely automated
content moderation---though algorithmic review doesn't always happen before
posting. Finally, while TikTok is playing catchup with Meta, Twitter, and
YouTube, it also is making rapid strides in that direction. From Q3~2020 to
Q3~2021, the share of automatically removed video clips grew from 7.8\% to
33.9\% of all removed clips. After plateauing at that level for another two
quarters, it started growing again, reaching 48.0\% for Q3~2022.


\subsection{Validating the Transparency Data}
\label{sec:census-validation}

Since transparency disclosures, by definition, are based on platform-internal
data, the public must trust that platforms correctly collect and report the
data. Within the \EU, that will change as the \DSA's yearly audit requirement
takes effect. But that's only the \EU\ and only the future. I was curious if
there were \emph{current} opportunities for independently validating such
disclosures.

As it turns out, there is at least one: In the United States, child sexual abuse
material (\CSAM) must be reported to a clearinghouse, the National Center for
Missing and Exploited Children (\NCMEC). Starting in March 2020 for the year
2019, the center has been making its own, yearly transparency disclosures. They
include a breakdown of how many reports \NCMEC\ received from the different
internet platforms. Hence, it should be possible to cross-check \NCMEC's data
with that disclosed by social media platforms. In the language of \NCMEC, which
I adopt for this section, each submission to its CyberTipline is called a
\emph{report} and includes one or more \emph{pieces} of \CSAM, i.e., photos or
videos. Since report refers to just these CyberTipline submissions, I use
``transparency disclosure'' for organization's statistical data releases.

To make the validation meaningful while also keeping it manageable, I decided to
compare the full history of \CSAM\ statistics disclosures but for Meta and
Google only. I include Meta because, at over 90\% of all reports made to \NCMEC,
it appears to be a ``hotbed of \CSAM''~\cite{Hitt2021} (and not Pornhub, as
falsely asserted~\cite{Brown2020,Grant2020} with devastating
consequences~\cite{Celarier2021,Dickson2020,Harris2021,Stoya2021,GagliardoSilver2021}
by a certain New York Times columnist, who made common cause with anti-porn
crusaders~\cite{Hitt2020a} associated with a White supremacist
church~\cite{Halley2021,ProducerX2020} after pulling similar stunts
before~\cite{Bass2014,Brown2019,Dickson2014,Martin2012,Masnick2017,McCormack2012,Talusan2017}).
While Google is a distant second, I originally included the firm because it uses
the same two \CSAM\ detection systems as Meta~\cite{Allen2011,Davis2018}:
Microsoft's PhotoDNA for detecting previously known instances of \CSAM\ based on
perceptual hashing and Google's own Content Safety \API{} for detecting
previously unknown instances of \CSAM\ based on a machine learning model.

Table~\ref{table:csam} summarizes \emph{all} \CSAM\ disclosures by the three
entities~\cite{NcmecByPlatform2019,NcmecByPlatform2020,NcmecByPlatform2021}.
\NCMEC\ makes yearly disclosures of report counts, Google makes semiannual
disclosures of both report and piece counts, and Meta makes quarterly
disclosures of only piece counts. That already implies that only \NCMEC's and
Google's data are comparable. As the table shows, they are reasonably close,
with a maximum yearly difference of -0.6\% between \NCMEC's count for Google and
Google's own count.

\begin{table}
\caption{\CSAM{} disclosures by \NCMEC, Google, and Meta from 2018 to 2022.}
\label{table:csam}
\begin{tabular}{cRrrRRrRrRr}

& \multicolumn{5}{C}{\textbf{NCMEC}\T}
&
& \multicolumn{3}{C}{\textbf{Google}}
& \multicolumn{1}{c}{\textbf{Meta}} \\

\multirow{-2}{*}{\textbf{Year}}
& \multicolumn{1}{C}{\textbf{Reports}}
& \multicolumn{2}{c}{\textbf{From Meta}}
& \multicolumn{2}{C}{\textbf{From Google}\B}
& \multicolumn{1}{c}{\multirow{-2}{*}{$\bm{\Delta}$}}
& \multicolumn{1}{C}{\textbf{Reports}}
& \multicolumn{1}{c}{$\bm{\Pi$}}
& \multicolumn{1}{C}{\textbf{Pieces}}
& \multicolumn{1}{c}{\textbf{Pieces}} \\
\hline
& & & & & & & & & & \T\\
& & & & & & & & & & \\
& & & & & & & & & & 9,000,000 \\
\multirow{-4}{*}{2018} & & & & & & & & & & \B 7,200,000 \\
\hline
& & & & & & & & & & \T 5,800,000 \\ \cline{11-11}
& & & & & & & & & & 7,426,200 \\
& & & & & & & & & & 12,155,800 \\
\multirow{-4}{*}{2019}
& \multirow{-4}{*}{16,836,694}
& \multirow{-4}{*}{94.3\%} & \multirow{-4}{*}{15,884,511}
& \multirow{-4}{*}{2.7\%}  & \multirow{-4}{*}{449,283}
& & & & & \B 13,986,400 \\
\hline
& & & & & & & & & & \T 9,500,000 \\
& & & & & &
& \multirow{-2}{*}{182,556} & \multirow{-2}{*}{8.4×} & \multirow{-2}{*}{1,533,536}
& 2,958,200 \\
& & & & & & & & & & 10,770,600 \\
\multirow{-4}{*}{2020}
& \multirow{-4}{*}{21,447,786}
& \multirow{-4}{*}{94.7\%} & \multirow{-4}{*}{20,307,216}
& \multirow{-4}{*}{2.5\%}  & \multirow{-4}{*}{546,704}
& \multirow{-4}{*}{+0.2\%}
& \multirow{-2}{*}{365,319} & \multirow{-2}{*}{8.0×} & \multirow{-2}{*}{2,904,317}
& \B 4,958,900 \\
\hline
& & & & & & & & & & \T 5,812,400 \\ \cline{11-11}
& & & & & &
& \multirow{-2}{*}{412,141} & \multirow{-2}{*}{8.3×} & \multirow{-2}{*}{3,413,673}
& 27,000,000 \\
& & & & & & & & & & 22,800,000 \\
\multirow{-4}{*}{2021}
& \multirow{-4}{*}{29,157,083}
& \multirow{-4}{*}{92.2\%} & \multirow{-4}{*}{26,885,302}
& \multirow{-4}{*}{3.0\%}  & \multirow{-4}{*}{875,783}
& \multirow{-4}{*}{-0.6\%}
& \multirow{-2}{*}{458,178} & \multirow{-2}{*}{7.2×} & \multirow{-2}{*}{3,282,824}
& \B 22,400,000 \\
\hline
& & & & & & & & & & \T 18,000,000 \\
& & & & & &
& \multirow{-2}{*}{1,044,277} & \multirow{-2}{*}{6.4×} & \multirow{-2}{*}{6,698,201}
& 21,600,000 \\
& & & & & & & & & & 31,400,000 \\
\multirow{-4}{*}{2022}
& & & & & & & & & & \\
\end{tabular}
\end{table}


\subsubsection{Meta's Intransparent Transparency}

Unfortunately, Meta's statistics aren't just incomparable to \NCMEC's. They also
suffer from several other shortcomings. Let me walk you through them. Looking at
the numbers in Table~\ref{table:csam}, Meta is obviously rounding its
statistics. Worse, it does so with different precision for Facebook and
Instagram and does so even for the machine-readable data. As far as I can tell,
Meta also is the \emph{only} platform to do so.

Originally, Meta started disclosing ``Child Nudity \& Sexual Exploitation'' for
Facebook only in Q3~2018 and for Instagram as well in Q2~2019, hence the
horizontal line above that latter quarter's entry in Table~\ref{table:csam}. In
Q2~2021, the firm switched to reporting ``Nudity and Physical Abuse'' and
``Sexual Exploitation'' under the ``Child Endangerment'' heading, hence the line
above that quarter's entry in Table~\ref{table:csam}. Based on metrics names,
one might expect that ``Child Nudity \& Sexual Exploitation'' includes ``Child
Endangerment: Sexual Exploitation.'' Yet the counts reported from Q2~2021
forward seem markedly higher than those from before. Notably, the 4.6$\times$
increase from Q1~2021 to Q2~2021 is jarring.

Alas, Meta's transparency pages give \emph{no} explanation. Only after searching
for ``Meta Community Standards Enforcement Report Q2~2021'' with an external
search engine, did I find a blog post~\cite{Facebook2021a} pointing towards a
PDF file that hints at an explanation~\cite{Facebook2021}. Apparently, Meta
started measuring ``Child Physical Abuse'' only that quarter and included it in
the at least appropriately named ``Nudity and Physical Abuse'' metric. It also
started measuring ``Sexualization of children'' and ``Inappropriate interactions
with Children'' and included both in the ``Sexual Exploitation'' metric, too.
That would explain the consistently much higher numbers. However, combining such
disparate categories of content into one metric and then labelling that metric
by the most severe category is highly inappropriate and deeply misleading.

Even before Meta thusly changed metrics, there was some indication that Meta was
inflating statistics. In February 2021, Antigone Davis, Meta's Global Head of
Safety, reported in a blog post~\cite{Davis2021} that 90\% of pieces reported to
\NCMEC\ during October and November 2020 had been reported before or were
visually similar to previously reported content. Notably, six videos accounted
for more than half of the reported content. Further analysis of 150 accounts
reported to \NCMEC\ during July and August 2020 as well as January 2021 showed
that 75\% of them didn't share \CSAM\ with malicious intent but ``for other
reasons, such as outrage or in poor humor (i.e. a child's genitals being bitten
by an animal).'' While Meta still is legally mandated to report such instances
to \NCMEC, not including such information in its transparency disclosures is
negligent at best. By comparison, Pinterest's transparency disclosures do
distinguish between posts and the content itself, including for
\CSAM~\cite{Pinterest2022}.

While extracting the data for Meta and Google from \NCMEC's transparency
disclosures, I discovered a hint at a significant hole in Meta's disclosures.
For 2019 and 2020, \NCMEC\ reported one number for Meta under its old name
Facebook. But in 2021, \NCMEC\ reported three counts, one for Facebook, one for
Instagram, and one for WhatsApp. Uhm, \emph{WhatsApp}?? Meta makes no
transparency disclosures about WhatsApp, neither in its ``transparency center''
nor on WhatsApp's website, beyond some outdated statistics about California's
privacy law.

Finally, while replacing the original spreadsheet providing data for the above
table with Python code, I discovered that data going back almost two years had
been changed between Meta's disclosures for Q2~2022 and Q3~2022. The changes are
fairly substantial: Out of 113 modified data points, 77 were for Q4~2020, 3 for
Q1~2021, 4 for Q2~2021, and the rest for Q2~2022. Out of the 77 for Q4~2020, 58
are absolute counts, which changed between -50.0\% and -0.1\%. Yet neither
transparency report nor the blog post announcing the report's release mention
anything about these changes. That seems heedless at the very best.


\subsubsection{The Global Spread of \CSAM}

\NCMEC\ also releases report counts broken down by individual countries. Since
the largest non-Chinese social media, with exception of Telegram, are all based
in the United States, that breakdown should be fairly representative of \CSAM\
sharing patterns across the world. Hence, I decided to wrap up my investigation
into the quality of transparency disclosures by analyzing \NCMEC's dataset for
2021~\cite{NcmecByCountry2021}.

Right away, I did encounter several minor data quality issues. Notably, the data
included two different entries for French Guiana, one labelled thusly and the
other ``Guiana, French.'' Furthermore, it included entries for (1) the
Netherlands Antilles, (2) Bonaire, Sint Eustatius, and Saba (3) Curaçao, as well
as (4) Sint Maarten, even though the former was split into the latter three in
2010. Finally, it included an entry for Bouvet Island. That is surprising
because this subantarctic Norwegian territory is an uninhabited nature preserve
with no man-made structures beyond a weather station.

To characterize geographic distribution, I computed regional totals as well as
per-capita rates per country. Taken together South and South East Asia are
responsible for 58.1\% of all reports despite only comprising 33.8\% of the
world population. However, when ranking countries by their reports per capita,
the countries with the highest number of reports per capita are located on the
Arab peninsula or in North Africa, abutting the Mediterranean. The exception are
the Cocos or Keeling Islands. With 168 reports per 596 capita, the rate for the
Australian territory is seven times higher than that of the second ranked
country, Libya. While there is no reason to doubt the accuracy of that count,
the tiny population also renders the statistic largely meaningless. Overall, my
results are consistent with those reported by a team from Google, \NCMEC, and
Thorn in 2019~\cite{BurszteinBrightea2019}. The
\href{https://github.com/apparebit/penal-colony}{supplemental materials} for
this paper include both data and the notebooks with the analysis.


\subsection{What the Transparency Data Doesn't Cover}
\label{sec:census-limits}

It is imperative to keep in mind that social media's governance as reflected in
their transparency reports offer a partial picture only. Case in point is Meta
n\'ee Facebook. Its governance processes and transparency efforts may be sloppy
and suffer from serious data quality issues. But Meta has also made significant
investments into its governance by, for example, having external experts review
its transparency report metrics~\cite{BradfordGriselea2019}, having the
attendant data collection audited~\cite{Sarang2022}, and endowing an external
oversight board~\cite{BoteroMarinoGreeneea2020}. Yet the firm also has an
astoundingly deadly foreign record. By distributing and amplifying sectarian
messaging, Meta has directly contributed to genocides not once but twice! As a
result, 25,000 Rohingya were murdered in Myanmar from August 2017 to August 2018
and 700,000 were driven into refugee camps across the border in Bangladesh,
which have become the densest settlement on
Earth~\cite{DeGuzman2022,HumanRightsCouncil2018}. As a result, between 385,000
and 800,000 Tigrayans were murdered in Ethiopia's northern-most
province~\cite{AnnysVandenBemptea2021,ChothiaBekit2022}, 882,000 people turned
into refugees, and 4.51 million people were internally displaced between
November 2020 and November 2022~\cite{UNICEF2023}.

In both countries, Meta put its profits well before any other considerations
including content moderation and safety. The firm keeps entering foreign
markets, not head but sales office first, even if user interfaces, policy
documents, help pages, content moderation tools, and machine learning models
have not yet been localized. In Myanmar, Facebook paid local cell phone
providers to provide their customers with free access through the Facebook app.
As a result, ``internet'' and ``Facebook'' are largely synonymous in the country
to this day~\cite{Strom2016}. At the same time, Facebook had no employees who
speak Burmese in early 2017. It ignored warnings from human rights organizations
and other \NGO{}s. It also got stymied by legacy encodings for Burmese
text~\cite{LaGrowPruzan2019,Wade2022}. When the firm finally got its act
together, it was too late: The genocide was well under
way~\cite{McLaughlin2018,MilkoOrtutay2022,Mozur2018,Ortutay2022}.

Over two years later, that same basic dynamic played out across Africa in
general and Ethiopia specifically. Facebook had opened its first office in
Africa, a \emph{sales} office, in 2015~\cite{Wagner2015}. When it started
pushing more aggressively into African countries in 2018, it publicly insisted
that it had learned its lessons~\cite{Tiku2018}. Yet it opened its first content
moderation office in (all of) Africa only in 2019~\cite{Agutu2019}. The firm
lacked an appreciation of the richness of dialects and languages spoken across
the continent~\cite{FickDave2019,JacksonTownsendea2022,Madung2021}. It also
exploited local workers~\cite{AlSibai2022,Perrigo2022,Perrigo2023}. Its own
Oversight Board sounded the alarm because the firm was ignoring telltale
signs~\cite{Faife2021}. When the firm finally got its act together, it was too
late: The genocide was well under
way~\cite{Allen2022,Gilbert2020,GlobalWitness2022,Ilori2020,Malik2022,ElliottChristopherea2021,ZelalemGuest2021,RobinsEarly2021}.

Since Meta is publicly traded, we might wonder what shareholders think about
this abysmal record. They don't seem to be too pleased either and did try
holding the firm's \CEO\ and Chairman of the Board Mark Zuckerberg accountable.
The annual meetings in 2018~\cite{Butler2018}, 2019~\cite{Sumagaysay2019},
2020~\cite{McRitchie2020}, 2021~\cite{Nix2021}, and 2022~\cite{WatersAgnew2022}
featured motions to strip him of one or both titles and the majority of shares
approved the motions in 2019, 2021, and 2022. But thanks to some funny business
with the allocation of votes per share, Mr Zuckerberg controls the majority of
votes. He also keeps voting for himself. So how much transparency and
accountability can we realistically expect from a genocide-enabling autocracy?
