\section{A Social Media Census}
\label{sec:census}

In her April 2018 article for the Harvard Law Review~\cite{Klonick2018}, Kate
Klonick traces the early history of content moderation at Facebook, Twitter, and
YouTube. She also discusses their privileged position when it comes to speech,
and proposes a new label, and also legal role, to go along with that,
\emph{governance}. Her claims about social media's grounding in the tradition of
free speech seem wilfully oblivious to capitalist reality, which, for example,
grants Mr Zuckerberg control over more than 50\% of Meta's voting
shares---making him impervious to corporate
oversight~\cite{LauricellaNorton2021}. But Klonick's notion of governance also
is compelling. Alas, governance requires accountability requires transparency.
This section tries to provide just that by comparing non-Chinese social media
based on their content policies and transparency reports. For good measure, I am
also including the \V{EU}'s Digital Services Act or
\V{DSA}~\cite{EuropeanParliamentAndCouncil2022}.

To determine which social media to include in the census, I started with
Buffer's list of the 20 most popular platforms~\cite{Lua2022}. I treat Facebook
and Instagram as one platform, since Meta sanctions users based on their posts
to either social network. I drop the five platforms targeting China, since they
are unlikely to adhere to Klonick's governance model. I also drop Facebook
Messenger, Microsoft Teams, Skype, and WhatsApp, since they mostly enable
private communication between users. While that arguably is the predominant use
case for Telegram as well, its stickers, channels, and bots are public and
require moderation, as even Telegram acknowledges in their \V{FAQ}. Hence,
Telegram remains. Finally, I add Tumblr because that social network has become a
popular destination for users leaving Twitter as well as for employees laid off
by Twitter in the wake of Elon Musk's takeover of the firm~\cite{Patel2022}.

To determine the criteria for the census, I iterated a few times over a list
gleaned from social media transparency reports, previous studies and
commentary~\cite{BradfordGriselea2019,CrockerGebhartea2019,Douek2022}, best
practices
recommendations~\cite{AccessNowACLUFoundationOfNorthernCaliforniaea2021}, and
the \V{EU}'s \V{DSA}. For each iteration, I also filled in
Table~\ref{table:governance} below. The list stabilized only after I started
grouping individual criteria according to major aspects of content moderation.
After stabilization, I added a few more criteria that are aspirational in that
no existing platform meets them but would go a long way towards more fully
characterizing their impact. They are criteria~\ref{itm:labels},
\ref{itm:visibility}, \ref{itm:mental-health}, and~\ref{itm:resourcing}. The
resulting hierarchy follows, with emoji serving as column headers in the table
below.

% \hfill pushes the nested enumerations onto the next line!

\begin{description}
\item[Violative content or behavior, broken down by prohibition]\hfill
    \begin{enumerate}
        \item \emo{chart-increasing}~Prevalence of violative content measured as
            fraction of all content \label{itm:prevalence}
        \item \emo{triangular-ruler}~Reach of violative content measured in time
            or (preferably) views \label{itm:reach}
        \item \emo{roll-of-paper}~Spam, which may not warrant due process
            protections \label{itm:spam}
        \item \emo{busts}~Content resulting from ``coordinated inauthentic
            behavior'' \label{itm:coordination}
    \end{enumerate}
\item[Flagger of violative content or behavior, including raised vs acted-upon flags]
    \hfill
    \begin{enumerate}[resume]
        \item \emo{classical-building}~Government inquiries and removal requests
            \label{itm:government}
        \item \emo{detective} Trusted flaggers, including fact checkers and \V{IP}
            right holders \label{itm:flaggers}
        \item \emo{enraged-face}~Platform users \label{itm:users}
        \item \emo{robot}~Algorithms, hybrid vs fully automatic
            \label{itm:algorithms}
        \item \emo{person-facepalming}~Human moderators, in-house vs outsourced
            \label{criterion:moderators}
    \end{enumerate}
\item[Actions on content and users, broken down by prohibition]\hfill
    \begin{enumerate}[resume]
        \item \emo{label}~Warning labels, self-imposed while posting vs
            platform-imposed afterwards \label{itm:labels}
        \item \emo{foggy}~Visibility reduction and blacklists (aka shadow
            banning) \label{itm:visibility}
        \item \emo{wastebasket}~Content removal \label{itm:removal}
        \item \emo{biohazard}~Sanctions on users \label{itm:accounts}
    \end{enumerate}
\item[Safeguards for humans, including due process]\hfill
    \begin{enumerate}[resume]
        \item \emo{exploding-head}~Mental health impact on moderators,
            in-house vs outsourced \label{itm:mental-health}
        \item \emo{pager}~Meaningful notification \label{itm:notification}
        \item \emo{balance-scale}~Appeals and reversals \label{itm:appeals}
        \item \emo{eye}~External oversight of policy formulation, conflict
            resolution, and data disclosures (audits) \label{itm:oversight}
    \end{enumerate}
\item[Global differences and impact]\hfill
    \begin{enumerate}[resume]
        \item \emo{currency-exchange}~Platform localization and regional
            resourcing for content moderation \label{itm:resourcing}
        \item \emo{globe-africa-europe}~Outcomes broken down by countries and
            cultures \label{itm:countries}
    \end{enumerate}
\end{description}


\subsection{Census Results}
\label{sec:census-results}

Table~\ref{table:governance} shows the results of the social media census. Out
of the eleven surveyed platforms, eight have published content policies and
release transparency reports covering their content moderation. The other three
fall short of even these basic niceties of governance. In particular, Telegram
has no policy and makes no transparency disclosures. Quora has a policy but
makes no disclosures. Finally, Tumblr has a policy but discloses only
governmental requests and intellectual property claims. Policies differ
significantly in organization and level of detail but seem to mostly comprise
the same basic prohibitions, typically covering adult sexual activity,
harassment, threats and violence, hate speech, violent extremsism, suicide and
self-harm, child endangerment, impersonation, private information,
disinformation, spam, as well as fraud and other illegal activities. Yet prior
work has also demonstrated substantial differences in definitions and
enforcement~\cite{FieslerJiangea2018,PaterKimea2016}. Beyond this core, a couple
of prohibitions do stick out: Pinterest's ``Harassment and criticism'' is
chilling in its pathologizing overreach. Then again, Tumblr's ``Misattribution
or Non-Attribution'' is rather endearing---but probably not that practical.

\begin{table}
\caption{A survey of governance practices for social media, with a checkmark
    indicating release of \emph{some} statistics for that criterion.
    \emph{Account Sanctions} are coded W for warning, a number for as many days
    of forced timeout, and X for permanent account suspension. The \emo{robot}
    robot and \emo{judge} judge columns share the \emo{keycap-one} number one
    when a transparency report does not distinguish between in-house automated
    and human review. The \emo{collision} collision indicates that the firm
    commits to meaningful notification but blatantly violates that commitment in
    practice. The \emo{plus} plus indicates a separate
    dataset~\cite{RothGadde2021}. All years are within the
    21\textsuperscript{st} century.}
\label{table:governance}
\libertineLF
\begin{tabular}{L@{\;\;}CcCc@{\;}c@{\;}c@{\;}c@{\quad}c@{\;}c@{\;}c@{\;}c@{\;}c@{\quad}c@{\;}c@{\;}c@{\;}c@{\quad}c@{\;}c@{\;}c@{\;}c@{\quad}c@{\;}cC}

\multicolumn{2}{C}{\textbf{Platform}} & \textbf{Account} & \textbf{Latest}
& \multicolumn{19}{c}{\textbf{With \emph{Some} Coverage in Transparency Report\T}} & \\

\multicolumn{2}{C}{\textbf{\& Launch}}
& \textbf{Sanctions}
& \textbf{Report}
& \emo{chart-increasing}
& \emo{triangular-ruler}
& \emo{roll-of-paper}
& \emo{busts}
& \emo{classical-building}
& \emo{detective}
& \emo{enraged-face}
& \emo{robot}
& \emo{person-facepalming}
& \emo{label}
& \emo{foggy}
& \emo{wastebasket}
& \emo{biohazard}
& \emo{exploding-head}
& \emo{pager}
& \emo{balance-scale}
& \emo{eye}
& \emo{currency-exchange}
& \emo{globe-africa-europe} \T\B
& \multirow{-2}{*}{\emo{floppy-disk}} \\ \hline

\href{https://transparency.fb.com/policies/community-standards/}{FB/IG}
& '03
& \href{https://transparency.fb.com/enforcement/taking-action/restricting-accounts/}{\small W,1,3,7,30,X}
& \href{https://transparency.fb.com/data/community-standards-enforcement/}{Q3 '22}
& \MK & & \MK & & \MK & & \MK & \MKONE &
& & \MK & & & \MK & \MK & \MK & & &
\href{https://transparency.fb.com/sr/community-standards/}{\small CSV} \T\\

\href{https://www.linkedin.com/legal/professional-community-policies}{LinkedIn}
& '03
& & \href{https://about.linkedin.com/transparency/community-report}{H1 '22}
& & & \MK & & \MK & & \MK & \MKONE
& & & \MK & & & & & & & & \\

\href{https://policy.pinterest.com/en/community-guidelines}{Pinterest}
& '10
& & \href{https://policy.pinterest.com/en/transparency-report}{H1 '22}
& & \MK & \MK & & \MK & & \MK & \MK & \MK & &
& \MK & \MK & & & \MK & & & \MK & \\

\href{https://help.quora.com/hc/en-us/articles/360000470706-Platform-Policies}{Quora}
& '10
& & & & & & & & & & & & & & & & & & & & & & \\

\href{https://www.redditinc.com/policies/content-policy}{Reddit}
& '05
& \href{https://www.redditinc.com/policies/transparency-report-2021-2/}{\small W,3,7,X}
& \href{https://www.redditinc.com/policies/transparency-report-2021-2/}{'21}
\hidden{\,/\,\href{https://www.redditinc.com/policies/mid-year-transparency-report-2022}{H1
'22}
\href{https://www.reddit.com/r/redditsecurity/comments/rgikn1/q3_safety_security_report/}{Q3
'21}}
& \MK & & \MK & & \MK & & \MK & \MK & \MK & & & \MK & \MK & & \emo{collision} & \MK & & & \MK & \\

\href{https://values.snap.com/privacy/transparency/community-guidelines}{Snap}
& '11
& & \href{https://values.snap.com/privacy/transparency}{H1 '22}
& & \MK & \MK & & \MK & & \MK & & & &
& \MK & \MK & & & & & & \MK &
\href{https://assets.ctfassets.net/kw9k15zxztrs/4aBF69gFiv4l9x7LzAgY8X/578e44f80fc51d0ebc52e04fe3d8c857/Snap_H1_2022_TR.csv}{\small CSV}\\

Telegram & '13 & & & & & & & & & & & & & & & & & & & & & & \\

\href{https://www.tiktok.com/community-guidelines}{TikTok}
& '17
& & \href{https://www.tiktok.com/transparency/en/community-guidelines-enforcement-2022-3/}{Q3 '22}
& \MK & \MK & \MK & & \MK & & \MK & \MKONE & &
& \MK & \MK & & & & & & \MK &
\href{https://sf16-va.tiktokcdn.com/obj/eden-va2/nuvlojeh7ryht/Transparency_CGE_2022Q3/English_CGE_2022Q3.xlsx}{\small XLS} \\

\href{https://www.tumblr.com/policy/en/community}{Tumblr}
& '07
& & \href{https://transparency.automattic.com/tumblr/}{H1 '22}
& & & & & \MK & & & & & & & & & & & & & & & \\

\href{https://help.twitter.com/en/rules-and-policies/twitter-rules}{Twitter}
& '06
& \href{https://help.twitter.com/en/rules-and-policies/enforcement-options}{\small W/0.5--7/X}
& \href{https://transparency.twitter.com/en/reports/rules-enforcement.html#2021-jul-dec}{H2 '21}
& & \MK & \MK & \emo{plus} & \MK & & \MK & \MKONE &
& & \MK & \MK & & & & & & \MK &
\href{https://transparency.twitter.com/content/dam/transparency-twitter/download/rules_enforcement_test.zip}{\small CSV} \\

\href{https://www.youtube.com/howyoutubeworks/policies/community-guidelines/}{YouTube}
& '05
& \href{https://support.google.com/youtube/answer/2802032}{\small W,7,14,X}
& \href{https://transparencyreport.google.com/youtube-policy/removals}{Q3 '22} \B
& \MK & \MK & \MK & & \MK & \MK & \MK & \MK & \MK & & & \MK & \MK & & \MK & & & & \MK & \\
\hline

\multicolumn{4}{C}{\textbf{Required by EU's DSA \ding{42}\T}} &
& & & & \emo{eu} & \emo{eu} & \emo{eu} & \emo{eu} & \emo{eu}
& \emo{eu} & \emo{eu} & \emo{eu} & \emo{eu} &
& \emo{eu} & \emo{eu} & \emo{eu} & & & \emo{eu} \\

\end{tabular}
\end{table}

When it comes to sanctioning accounts, \emph{all} social media besides Telegram
fall into one of two groups. The first group seems to model their sanctions on
the Queen of Hearts from \emph{Alice's Adventures in Wonderland},
enthusiastically screaming ``Off with their heads!'' and then figuratively even
following through~\cite{Carroll2009}. The second group manages to improve on
that (low) standard, but only barely. Members of that group apparently model
their sanctions on California's 1994 ``three strikes'' law~\cite{Vitiello2002}.
A decade later, the law accounted for a quarter of the state's prisoners, did
not curb crime~\cite{BrownJolivette2005}, and \emph{increased}
homicides~\cite{MarvellMoody2001}. It also led to even more Black men being
locked up for even longer~\cite{BrownJolivette2005}. Three decades after
enactment, the most salient change is that the law accounts for a third of all
the state's prisoners~\cite{BirdGillea2022}. Since nothing about content
moderation would suggest fundamentally different outcomes, the uniformity and
severity of this sanction regime is clearly excessive and hence punitive.

We can also calibrate account sanctions by comparing to how social groupings
that are based on shared interests or identity traits handle interpersonal
conflict. While the details of a concrete case make all the difference in actual
outcome, expelling members who become too conflicted with too many other members
\emph{is} a valid coping strategy---as long as it is used deliberately and
sparingly. The critical difference between such real-world groupings and social
media is the reach: For a real-world group, the expelled member looses access to
just that one group and, depending on their proximity to an urban center,
probably can find other, similar social groups to engage. For a social media
platform, the expelled user looses access to all of that platform's users.
Switching to another platform may not help because each social media platform
tends to favor a particular kind of expression and hence attract different
people as users. In short, using a different analogue didn't help: Social media
account sanctions still are excessively punitive.

The conclusion does not change if I avoid the damning analogues and restrict
the framing to content moderation only: When it comes to judging the severity of
violative content (with possible exception of child sexual abuse material and
terrorism), Americans, including professional moderators, and international
audiences alike reason along many fine-grained axes and thereby make subtle
distinctions; international audiences only add more variability, depending on
culture~\cite{JiangScheuermanea2021,ScheuermanJiangea2021}. Furthermore,
\V{AI}-based enforcers increasingly intercept content \emph{before} it is posted.
Notably, that is already the case for the first, second, and fourth most popular
platforms, Facebook, YouTube, and Instagram~\cite{Lua2022}. But if an algorithm
intercepts violative content before posting, there is \emph{no} human harm and
hence no justification for punishment---a seemingly inconvenient fact ignored
by social media.

In summary, account sanctions across social media are both excessive and
punitive when calibrated against people's perceptions of severity, social
groupings in the real world, and criminal penalties in the real world. They
threaten to give rise to significant, involuntary disenfranchisement from
digital platforms, at times with traumatic consequences~\cite{Hill2022}, and
turn social media into highly effective boosters of the stochastic penal colony.
If that wasn't bad enough, users have been weaponizing social media's punitive
content moderation and are deliberately manipulating platforms to get other
users' accounts suspended~\cite{SilvermanFortis2023}.

In addition to confirming my hypothesis about being overly punitive, the
relative sparsity of checkmarks---with no platform disclosing statistics for
more than twelve out of the nineteen categories---provides a rude counterpoint
to Klonick's cheerful enthusiasm for social media governance. Worse, as the
\emph{Launch} years make clear, all but TikTok had one to two decades for making
the necessary investments into content moderation processes and transparency
reporting. Remarkably, most social media platforms won't even commit to
meaningful notification and one platform that does commit has been breaking its
commitment on grand scale for years.  To fill in the column, I originally relied
on the Electronic Frontier Foundation's prior work, validating their results as
much as possible~\cite{CrockerGebhartea2019}. Later on, I discovered that
subreddit moderators can ``shadowban'' or ``bot ban'' comments, which renders
them invisible to everyone but the original author. Updating the domain of any
Reddit \V{URL} to \texttt{reveddit.com} makes such comments visible again and
shows that this feature is widely and arbitrarily used even on constructive
contributions, suggesting that Reddit has a massive moderation abuse
problem~\cite{Hawkins2023}.

Committing to meaningful notification seems like an easy public relations win
for social media and implementing it isn't much harder. So if the vast majority
of social media won't commit to meaningful notification, it's very likely
because they don't care to provide meaningful notification---at least for some
significant fraction of cases. That does seem more honest than Reddit claiming
to heed users' fundamental right to such notifications while also devoting
significant software and hardware resources to maintaining the illusion for
content creators that some of their posts are still part of the platform when
\V{IRL} they have been effectively deleted from the platform long ago. At the
same time, the refusal to provide meaningful notification \emph{is}
quintessentially Kafkaesque. Notably, in both \emph{The Trial} and \emph{In the
Penal Colony}, the Condemned never learns the reasons for being subjugated to an
elaborate process that invariably ends with their execution. It is rather
remarkable that seven out of the eleven surveyed social media platforms not only
refuse meaningful notification but also rely on account termination as their
main sanction. That turns their governance into nothing but a (virtual) penal
colony. With that, Twitter's punishment as performance doesn't seem such an
outlier anymore. Instead, dehumanizing condenscencion for who may or may not be
daily active shitheads seems to be industry norm.

Some of these shenanigans won't do for much longer---unless platforms are
willing to forgo the European market. As the bottom table row illustrates, the
\V{EU}'s \V{DSA} includes fairly extensive requirements for transparency
reporting. In turn, that requires continuous collection of data on content
moderation. For the top-most tier as far as obligations go, the so-called
\emph{very large platforms}, those reports have to be independently audited as
well. The \V{DSA} doesn't leave much time for setting up the necessary
infrastructure. All platforms should have reported their user numbers to the
\V{EU} by February 2023. The Commission then decides which platforms to
designate as very large. Once a firm has been thusly designated, it has a mere
four months to comply with the law. Meanwhile, smaller platforms have until
February 2024 to come into compliance. Penalties for non-compliance are steep:
Each fine is capped at 6\% of the firm's global revenue for the previous year.

By design, Table~\ref{table:governance} largely obscures the subpar presentation
and organization of many social media platforms' transparency disclosures. It is
utterly ridiculous that five out of nine \emph{technology} firms can't be
bothered to release their data in machine-readable form; it is unacceptable to
the \V{EU}, too. Next, ambiguously named prohibitions make for confusing reading of
policies and transparency reports alike. For example, ``violence'' may refer to
the graphic depiction thereof or to extremist groups making use thereof.
``Hate'' may cover prejudiced statements against members of a protected group or
organized groups espousing supremacist ideology. Next, different platforms
report different metrics for the same aspect of content moderation, if they
report it at all. For instance, Pinterest, Snap, TikTok, Twitter, and YouTube
report statistics on the reach of violative content. Yet Pinterest, Twitter, and
YouTube report binned view counts, Snap median minutes, and Tiktok takedowns
within 24 hours as well as without views. For historical, reasons, most
platforms also break out government requests and intellectual property claims
into their own sections or even reports. In the latter case, it is up to
interested parties to notice that the transparency report's statistics are
incomplete and manually combine statistics from three different reports.

Finally, social media's transparency reports tell vastly different stories about
the role of \V{AI}. As already discussed in Section~\ref{sec:tweet-da-fe}, Twitter
blithely pretends it doesn't make aggressive use of algorithms. In contrast,
Pinterest is very exacting and clearly distinguishes between fully automated and
hybrid detection for each prohibition. In the latter case, algorithms pick
violative content as well, but humans need to approve the violation before the
system acts on it. Next, Facebook, Instagram, and YouTube have also largely
automated content moderation---though algorithmic review doesn't always happen
before posting. Finally, while TikTok is playing catchup with Meta, Twitter, and
YouTube, it also is making rapid strides in that direction. From \V{Q3}~2020 to
\V{Q3}~2021, the share of automatically removed video clips grew from 7.8\% to
33.9\% of all removed clips. After plateauing at that level for another two
quarters, it started growing again, reaching 48.0\% for \V{Q3}~2022.


\subsection{Validating the Transparency Data}
\label{sec:census-validation}

Since transparency disclosures, by definition, are based on platform-internal
data, the public must trust that platforms correctly collect and report the
data. Within the \V{EU}, that will change as the \V{DSA}'s yearly audit requirement
takes effect. But that's only the \V{EU} and only the future. I was curious if
there were \emph{current} opportunities for independently validating such
disclosures.

As it turns out, there is at least one: In the United States, child sexual abuse
material (\V{CSAM}) must be reported to a clearinghouse, the National Center for
Missing and Exploited Children (\V{NCMEC}). Starting in March 2020 for the year
2019, the center has been making its own, yearly transparency disclosures. They
include a breakdown of how many reports \V{NCMEC} received from the different
internet platforms. Hence, it should be possible to cross-check \V{NCMEC}'s data
with that disclosed by social media platforms. In the language of \V{NCMEC}, which
I adopt for this section, each submission to its CyberTipline is called a
\emph{report} and includes one or more \emph{pieces} of \V{CSAM}, i.e., photos or
videos. Since report refers to just these CyberTipline submissions, I use
``transparency disclosure'' for organizations' statistical data releases.

To make the validation meaningful while also keeping it manageable, I decided to
compare the full history of \V{CSAM} statistics disclosures for Meta and
Google only. I include Meta because, at over 90\% of all reports made to \V{NCMEC},
it appears to be a ``hotbed of \V{CSAM}''~\cite{Hitt2021} (and not Pornhub, as
falsely asserted~\cite{Brown2020,Grant2020} with devastating
consequences~\cite{Celarier2021,Dickson2020,Harris2021,Stoya2021,GagliardoSilver2021}
by a certain New York Times columnist, who made common cause with anti-porn
crusaders~\cite{Hitt2020a} associated with a White supremacist
church~\cite{Halley2021,ProducerX2020} after pulling similar stunts
before~\cite{Bass2014,Brown2019,Dickson2014,Martin2012,Masnick2017,McCormack2012,Talusan2017}).
While Google is a distant second, I originally included the firm because it uses
the same two \V{CSAM} detection systems as Meta~\cite{Allen2011,Davis2018}:
Microsoft's PhotoDNA for detecting previously known instances of \V{CSAM} based on
perceptual hashing and Google's own Content Safety \V{API} for detecting
previously unknown instances of \V{CSAM} based on a machine learning model.

Table~\ref{table:csam} summarizes \emph{all} \V{CSAM} disclosures by the three
entities~\cite{NcmecByPlatform2019,NcmecByPlatform2020,NcmecByPlatform2021}.
\V{NCMEC} makes yearly disclosures of report counts, Google makes semiannual
disclosures of both report and piece counts, and Meta makes quarterly
disclosures of only piece counts. That already implies that only \V{NCMEC}'s and
Google's data are comparable. As the table shows, they are reasonably close,
with a maximum yearly difference of -0.6\% between \V{NCMEC}'s count for Google and
Google's own count.

%https://facebook.com/business/f/973284106800107 roundup

\begin{table}
\caption{\V{CSAM}{} disclosures by \V{NCMEC}, Google, and Meta for the years
2018 to 2022. In past years, \V{NCMEC} released its transparency reports for the
previous year by March. This year, the organization is taking longer, with the
release of reports for 2022 currently scheduled for May 2, 2023.}
\label{table:csam}
\libertineLF
\begin{tabular}{cRrrRRrRrRr}

& \multicolumn{5}{C}{\textbf{NCMEC}\T}
&
& \multicolumn{3}{C}{\textbf{Google}}
& \multicolumn{1}{c}{\textbf{Meta}} \\

\multirow{-2}{*}{\textbf{Year}}
& \multicolumn{1}{C}{\textbf{Reports}}
& \multicolumn{2}{c}{\textbf{From Meta}}
& \multicolumn{2}{C}{\textbf{From Google}\B}
& \multicolumn{1}{c}{\multirow{-2}{*}{$\bm{\Delta}$}}
& \multicolumn{1}{C}{\textbf{Reports}}
& \multicolumn{1}{c}{$\bm{\Pi$}}
& \multicolumn{1}{C}{\textbf{Pieces}}
& \multicolumn{1}{c}{\textbf{Pieces}} \\
\hline
& & & & & & & & & & \T\\
& & & & & & & & & & \\
& & & & & & & & & & 9,000,000 \\
\multirow{-4}{*}{2018} & & & & & & & & & & \B 7,200,000 \\
\hline
& & & & & & & & & & \T 5,800,000 \\ \cline{11-11}
& & & & & & & & & & 7,426,200 \\
& & & & & & & & & & 12,155,800 \\
\multirow{-4}{*}{2019}
& \multirow{-4}{*}{16,836,694}
& \multirow{-4}{*}{94.3\%} & \multirow{-4}{*}{15,884,511}
& \multirow{-4}{*}{2.7\%}  & \multirow{-4}{*}{449,283}
& & & & & \B 13,986,400 \\
\hline
& & & & & & & & & & \T 9,500,000 \\
& & & & & &
& \multirow{-2}{*}{182,556} & \multirow{-2}{*}{8.4×} & \multirow{-2}{*}{1,533,536}
& 2,958,200 \\
& & & & & & & & & & 10,770,600 \\
\multirow{-4}{*}{2020}
& \multirow{-4}{*}{21,447,786}
& \multirow{-4}{*}{94.7\%} & \multirow{-4}{*}{20,307,216}
& \multirow{-4}{*}{2.5\%}  & \multirow{-4}{*}{546,704}
& \multirow{-4}{*}{+0.2\%}
& \multirow{-2}{*}{365,319} & \multirow{-2}{*}{8.0×} & \multirow{-2}{*}{2,904,317}
& \B 4,958,900 \\
\hline
& & & & & & & & & & \T 5,812,400 \\ \cline{11-11}
& & & & & &
& \multirow{-2}{*}{412,141} & \multirow{-2}{*}{8.3×} & \multirow{-2}{*}{3,413,673}
& 27,000,000 \\
& & & & & & & & & & 22,800,000 \\
\multirow{-4}{*}{2021}
& \multirow{-4}{*}{29,157,083}
& \multirow{-4}{*}{92.2\%} & \multirow{-4}{*}{26,885,302}
& \multirow{-4}{*}{3.0\%}  & \multirow{-4}{*}{875,783}
& \multirow{-4}{*}{-0.6\%}
& \multirow{-2}{*}{458,178} & \multirow{-2}{*}{7.2×} & \multirow{-2}{*}{3,282,824}
& \B 22,400,000 \\
\hline
& & & & & & & & & & \T 18,000,000 \\
& & & & & &
& \multirow{-2}{*}{1,044,277} & \multirow{-2}{*}{6.4×} & \multirow{-2}{*}{6,698,201}
& 21,600,000 \\
& & & & & & & & & & 31,400,000 \\
\multirow{-4}{*}{2022}
& & & & & &
& \multirow{-2}{*}{1,130,042} & \multirow{-2}{*}{5.9×} & \multirow{-2}{*}{6,704,684}
& 34,800,000 \\
\end{tabular}
\end{table}


\subsubsection{Meta's Intransparent Transparency}

Unfortunately, Meta's statistics aren't just incomparable to \V{NCMEC}'s. They also
suffer from several other shortcomings. Let me walk you through them. Looking at
the numbers in Table~\ref{table:csam}, Meta is obviously rounding its
statistics. Worse, it does so with different precision for Facebook and
Instagram and does so even for the machine-readable data. As far as I can tell,
Meta also is the \emph{only} platform to do so.

Originally, Meta started disclosing ``Child Nudity \& Sexual Exploitation'' for
Facebook only in \V{Q3}~2018 and for Instagram as well in \V{Q2}~2019, hence the
horizontal line above that latter quarter's entry in Table~\ref{table:csam}. In
\V{Q2}~2021, the firm switched to reporting ``Nudity and Physical Abuse'' and
``Sexual Exploitation'' under the ``Child Endangerment'' heading, hence the line
above that quarter's entry in Table~\ref{table:csam}. Based on metrics names,
one might expect that ``Child Nudity \& Sexual Exploitation'' includes ``Child
Endangerment: Sexual Exploitation.'' Yet the counts reported from \V{Q2}~2021
forward seem markedly higher than those from before. Notably, the $4.6\times$
increase from \V{Q1}~2021 to \V{Q2}~2021 is jarring.

Alas, Meta's transparency pages give \emph{no} explanation. Only after searching
for ``Meta Community Standards Enforcement Report \V{Q2}~2021'' with an external
search engine, did I find a blog post~\cite{Facebook2021a} pointing towards a
\V{PDF} file that hints at an explanation~\cite{Facebook2021}. Apparently, Meta
started measuring ``Child Physical Abuse'' only that quarter and included it in
the at least appropriately named ``Nudity and Physical Abuse'' metric. It also
started measuring ``Sexualization of children'' and ``Inappropriate interactions
with Children'' and included both in the ``Sexual Exploitation'' metric, too.
That would explain the consistently much higher numbers. However, combining such
disparate categories of content into one metric and then labelling that metric
by the most severe category also is highly inappropriate and deeply misleading.

Even before Meta thusly changed metrics, there was some indication that Meta's
statistics were overstating the problem. In February 2021, Antigone Davis,
Meta's Global Head of Safety, reported in a blog post~\cite{Davis2021} that 90\%
of pieces reported to \V{NCMEC} during October and November 2020 had been reported
before or were visually similar to previously reported content. Notably, six
videos accounted for more than half of the reported content. Further analysis of
150 accounts reported to \V{NCMEC} during July and August 2020 as well as January
2021 showed that 75\% of them didn't share \V{CSAM} with malicious intent but
``for other reasons, such as outrage or in poor humor (i.e. a child's genitals
being bitten by an animal).'' While Meta still is legally mandated to report
such instances to \V{NCMEC}, not including such information in its transparency
disclosures is negligent at best. So is Meta's decision to only disclose piece
counts but not reports.

While extracting the data for Meta and Google from \V{NCMEC}'s transparency
disclosures, I discovered indication for a significant omission from Meta's
disclosures. For 2019 and 2020, \V{NCMEC} reported one number for Meta under its
old name Facebook. But in 2021, \V{NCMEC} reported three counts, one for Facebook,
one for Instagram, and one for WhatsApp. Uhm, \emph{WhatsApp}?? Meta makes no
transparency disclosures about WhatsApp, neither in its ``transparency center''
nor on WhatsApp's website, beyond some outdated statistics about California's
privacy law.

Finally, while replacing the original spreadsheet providing data for the above
table with Python code, I discovered that data going back almost two years had
been changed between Meta's disclosures for \V{Q2}~2022 and \V{Q3}~2022. The
changes are fairly substantial: Out of 113 modified data points, 77 were for
\V{Q4}~2020, 3 for \V{Q1}~2021, 4 for \V{Q2}~2021, and the rest for \V{Q2}~2022.
Out of the 77 for \V{Q4}~2020, 58 are absolute counts, which changed between
-50.0\% and -0.1\%. Yet neither transparency report nor the blog post announcing
the report's release mention anything about these changes. That seems heedless
at the very best.


\subsubsection{NCMEC vs Other Platforms}

Given the rather inconsistent results for Google and Meta, one cannot but wonder
about other platforms' disclosures. Hence, I also compared \V{NCMEC}'s data with
the corresponding disclosures by LinkedIn, Pinterest, Reddit, Snap, TikTok, and
Twitter for 2021. Out of the bunch, only Reddit and Pinterest unambiguously
identify \V{CSAM} reports to \V{NCMEC} in their transparency disclosures. But
whereas Reddit provides the exact same number as \V{NCMEC}, Pinterest claims 18\%
more reports than \V{NCMEC}. For H2, Pinterest also discloses the number of
``pins'' (or image cards) reported to \V{NCMEC}; curiously, the number of reports
is 11\% larger. In a dedicated section, Snap explicitly mentions \V{CSAM} and
\V{NCMEC} reporting but then labels the statistic ``Total Account Deletions.'' For
\V{H1} 2021, it also discloses the fraction of ``accounts enforced globally,'' which
amounts to a 14\% larger quantity. But even with that larger quantity, its
number of account deletions still is 35\% smaller than the number of reports
received by \V{NCMEC}. Given Snap's stated zero-tolerance policy, those numbers
should match. Though it is possible that Snap doesn't consistently follow
through with account closure. By contrast, Twitter discloses (presumably
permanent) ``account suspensions'' due to ``child sexual exploitation,'' but its
number is $11.1\times$ larger than the number of reports \V{NCMEC}
received from the firm. That does raise the question of whether Twitter always
reports \V{CSAM} to \V{NCMEC}, as it is legally required. Finally, LinkedIn and
TikTok disclose statistics for the more general ``child exploitation'' and
``minor safety,'' respectively; they are incomparable to \V{NCMEC}'s disclosures.

That results diverge in different ways for platforms other than Google and
Reddit suggests that the fault probably lies with platforms and not \V{NCMEC}. That
generally does not bode well for the accuracy of platforms' other transparency
disclosures. By comparison, Meta's data quality issues still seem out there. At
the same time, Twitter claiming over an order of magnitude more account
suspensions due to \V{CSAM} than reports about \V{CSAM} sent to \V{NCMEC} is highly
problematic as well.


\subsubsection{The Global Spread of CSAM}

\V{NCMEC} also releases report counts broken down by individual countries. Since
all surveyed social media, with exception of Telegram, are based in the United
States, that breakdown should be fairly representative of \V{CSAM} sharing
patterns across the world. Hence, I decided to wrap up my investigation into the
quality of transparency disclosures by analyzing \V{NCMEC}'s country dataset for
2021~\cite{NcmecByCountry2021}.

Right away, I did encounter several minor data quality issues. Notably, the data
included two different entries for French Guiana, one labelled thusly and the
other ``Guiana, French.'' Furthermore, it included entries for (1) the
Netherlands Antilles, (2) Bonaire, Sint Eustatius, and Saba (3) Curaçao, as well
as (4) Sint Maarten, even though the former was split into the latter three in
2010. Finally, it included an entry for Bouvet Island. That is surprising
because this subantarctic Norwegian territory is an uninhabited nature preserve
with no man-made structures beyond an automated weather station.

To characterize geographic distribution, I computed regional totals as well as
per-capita rates per country. Taken together South and South East Asia are
responsible for 58.1\% of all reports despite only comprising 33.8\% of the
world population. However, when ranking countries by their reports per capita,
the countries with the highest number of reports per capita are located on the
Arab peninsula and in North Africa, abutting the Mediterranean. The exception are
the Cocos or Keeling Islands. With 168 reports per 596 capita, the rate for the
Australian territory is seven times higher than that of the second ranked
country, Libya. While there is no reason to doubt the accuracy of that count,
the tiny population also renders the statistic largely meaningless. Overall, my
results are consistent with those reported by a team from Google, \V{NCMEC}, and
Thorn in 2019~\cite{BurszteinBrightea2019}. The
\anonhref{https://github.com/apparebit/penal-colony}{supplemental materials} for
this paper include both data and the notebooks with the analysis.


\subsection{What the Transparency Data Doesn't Cover}
\label{sec:census-limits}

It is imperative to keep in mind that social media's governance as reflected in
their transparency reports offer a partial picture only. Case in point is Meta
n\'ee Facebook. Its governance processes and transparency efforts may be sloppy
and suffer from serious data quality issues. But Meta has also made significant
investments into its governance by, for example, having external experts review
its transparency report metrics~\cite{BradfordGriselea2019}, having the
attendant data collection audited~\cite{Sarang2022}, and endowing an external
oversight board~\cite{BoteroMarinoGreeneea2020}. Yet the firm also has an
astoundingly deadly foreign record. By distributing and amplifying sectarian
messages, Meta has directly contributed to genocides not once but twice! As a
result, 25,000 Rohingya were murdered in Myanmar from August 2017 to August 2018
and 700,000 were driven into refugee camps across the border in Bangladesh,
which have become the densest settlement on
Earth~\cite{DeGuzman2022,HumanRightsCouncil2018}. As a result, between 385,000
and 800,000 Tigrayans were murdered in Ethiopia's northern-most
province~\cite{AnnysVandenBemptea2021,ChothiaBekit2022}, 882,000 people turned
into refugees, and 4.51 million people were internally displaced between
November 2020 and November 2022~\cite{UNICEF2023}.

In both countries, Meta put its profits well before any other considerations
including content moderation and safety. The firm keeps entering foreign
markets, not head but sales office first, even if user interfaces, policy
documents, help pages, content moderation tools, and machine learning models
have not yet been localized. In Myanmar, Facebook paid local cell phone
providers to provide their customers with free access through the Facebook app.
As a result, ``internet'' and ``Facebook'' are largely synonymous in the country
to this day~\cite{Strom2016}. At the same time, Facebook had few to no employees
who speak Burmese in early 2017. It ignored warnings from human rights
organizations and other \V{NGO}s. It also got stymied by legacy encodings for
Burmese text~\cite{LaGrowPruzan2019,Wade2022}. When the firm finally got its act
together, it was too late: The genocide was well under
way~\cite{McLaughlin2018,MilkoOrtutay2022,Mozur2018,Ortutay2022}.

Over two years later, that same basic dynamic played out across Africa in
general and Ethiopia specifically. Facebook had opened its first office in
Africa, a \emph{sales} office, in 2015~\cite{Wagner2015}. When it started
pushing more aggressively into African countries in 2018, it publicly insisted
that it had learned its lessons~\cite{Tiku2018}. Yet it opened its first content
moderation office in Subsaharan Africa only in 2019~\cite{Agutu2019}. The firm
lacked an appreciation of the richness of dialects and languages spoken across
the continent~\cite{FickDave2019,JacksonTownsendea2022,Madung2021}. It also
exploited local workers~\cite{AlSibai2022,Perrigo2022,Perrigo2023}. Its own
Oversight Board sounded the alarm because the firm was ignoring telltale
signs~\cite{Faife2021}. When the firm finally got its act together, it was too
late: The genocide was well under
way~\cite{Allen2022,Gilbert2020,GlobalWitness2022,Ilori2020,Malik2022,ElliottChristopherea2021,ZelalemGuest2021,RobinsEarly2021}.

Since Meta is publicly traded, we might wonder what shareholders think about
this abysmal record. They don't seem to be too pleased either and did try
holding the firm's \V{CEO} and Chairman of the Board Mark Zuckerberg
accountable. The annual meetings in 2018~\cite{Butler2018},
2019~\cite{Sumagaysay2019}, 2020~\cite{McRitchie2020}, 2021~\cite{Nix2021}, and
2022~\cite{WatersAgnew2022} featured motions to strip him of one or both titles
and the majority of shares approved the motions in 2019, 2021, and 2022. But
thanks to some funny business with the allocation of votes per share, Mr
Zuckerberg controls the majority of votes. He also keeps voting for himself. So
how much transparency and accountability can we realistically expect from a
genocide-enabling autocracy?
