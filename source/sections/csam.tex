% !TEX root = ../main.tex

Since transparency disclosures, by definition, are based on platform-internal
data, the public must trust that platforms correctly collect and report the
data. Within the \EU, that will change as the \DSA's yearly audit requirement
takes effect. But that's only the \EU\ and only the future. I was curious if
there were \emph{current} opportunities for independently validating such
disclosures.

As it turns out, there is at least one: In the United States, child sexual abuse
material (\CSAM) must be reported to a clearinghouse, the National Center for
Missing and Exploited Children (\NCMEC). Starting in March 2020 for the year
2019, the center has been making its own, yearly transparency disclosures. They
include a breakdown of how many reports \NCMEC\ received from the different
internet platforms. Hence, it should be possible to cross-check \NCMEC's data
with that disclosed by social media platforms. In the language of \NCMEC, which
I adopt for this section, each submission to its CyberTipline is called a
\emph{report} and includes one or more \emph{pieces} of \CSAM, i.e., photos or
videos. Since report refers to just these CyberTipline submissions, I use
``transparency disclosure'' for organization's statistical data releases.

To make the validation meaningful while also keeping it manageable, I decided to
compare the full history of \CSAM\ statistics disclosures but for Meta and
Google only. I include Meta because, at over 90\% of all reports made to \NCMEC,
it appears to be a ``hotbed of \CSAM''~\cite{Hitt2021} (and not Pornhub, as
falsely asserted~\cite{Brown2020,Grant2020} with devastating
consequences~\cite{Celarier2021,Dickson2020,Harris2021,Stoya2021,GagliardoSilver2021}
by a certain New York Times columnist, who made common cause with anti-porn
crusaders~\cite{Hitt2020a} associated with a white supremacist
church~\cite{Halley2021,ProducerX2020} after pulling similar stunts
before~\cite{Bass2014,Brown2019,Dickson2014,Martin2012,Masnick2017,McCormack2012,Talusan2017}).
While Google is a distant second, I originally included the firm because it uses
the same two \CSAM\ detection systems as Meta~\cite{Allen2011,Davis2018}:
Microsoft's PhotoDNA for detecting previously known instances of \CSAM\ based on
perceptual hashing and Google's own Content Safety \API{} for detecting
previously unknown instances of \CSAM\ based on a machine learning model.

Table~\ref{table:csam} summarizes \emph{all} \CSAM\ disclosures by the three
entities~\cite{NcmecByPlatform2019,NcmecByPlatform2020,NcmecByPlatform2021}.
\NCMEC\ makes yearly disclosures of report counts, Google makes semiannual
disclosures of both report and piece counts, and Meta makes quarterly
disclosures of only piece counts. That already implies that only \NCMEC's and
Google's data are comparable. As the table shows, they are reasonably close,
with a maximum yearly difference of -0.6\% between \NCMEC's count for Google and
Google's own count.

\begin{table}
\caption{\CSAM{} disclosures by \NCMEC, Google, and Meta from 2018 to 2022.}
\label{table:csam}
\begin{tabular}{cRrrRRrRrRr}

& \multicolumn{5}{>{\columncolor[gray]{.91}}c}{\textbf{NCMEC}\T}
&
& \multicolumn{3}{>{\columncolor[gray]{.91}}c}{\textbf{Google}}
& \multicolumn{1}{c}{\textbf{Meta}} \\

\multirow{-2}{*}{\textbf{Year}}
& \multicolumn{1}{>{\columncolor[gray]{.91}}c}{\textbf{Reports}}
& \multicolumn{2}{c}{\textbf{From Meta}}
& \multicolumn{2}{>{\columncolor[gray]{.91}}c}{\textbf{From Google}\B}
& \multicolumn{1}{c}{\multirow{-2}{*}{$\bm{\Delta}$}}
& \multicolumn{1}{>{\columncolor[gray]{.91}}c}{\textbf{Reports}}
& \multicolumn{1}{c}{$\bm{\Pi$}}
& \multicolumn{1}{>{\columncolor[gray]{.91}}c}{\textbf{Pieces}}
& \multicolumn{1}{c}{\textbf{Pieces}} \\
\hline
& & & & & & & & & & \T\\
& & & & & & & & & & \\
& & & & & & & & & & 9,000,000 \\
\multirow{-4}{*}{2018} & & & & & & & & & & \B 7,200,000 \\
\hline
& & & & & & & & & & \T 5,800,000 \\ \cline{11-11}
& & & & & & & & & & 7,426,200 \\
& & & & & & & & & & 12,155,800 \\
\multirow{-4}{*}{2019}
& \multirow{-4}{*}{16,836,694}
& \multirow{-4}{*}{94.3\%} & \multirow{-4}{*}{15,884,511}
& \multirow{-4}{*}{2.7\%}  & \multirow{-4}{*}{449,283}
& & & & & \B 13,986,400 \\
\hline
& & & & & & & & & & \T 9,500,000 \\
& & & & & &
& \multirow{-2}{*}{182,556} & \multirow{-2}{*}{8.4×} & \multirow{-2}{*}{1,533,536}
& 2,958,200 \\
& & & & & & & & & & 10,770,600 \\
\multirow{-4}{*}{2020}
& \multirow{-4}{*}{21,447,786}
& \multirow{-4}{*}{94.7\%} & \multirow{-4}{*}{20,307,216}
& \multirow{-4}{*}{2.5\%}  & \multirow{-4}{*}{546,704}
& \multirow{-4}{*}{+0.2\%}
& \multirow{-2}{*}{365,319} & \multirow{-2}{*}{8.0×} & \multirow{-2}{*}{2,904,317}
& \B 4,958,900 \\
\hline
& & & & & & & & & & \T 5,812,400 \\ \cline{11-11}
& & & & & &
& \multirow{-2}{*}{412,141} & \multirow{-2}{*}{8.3×} & \multirow{-2}{*}{3,413,673}
& 27,000,000 \\
& & & & & & & & & & 22,800,000 \\
\multirow{-4}{*}{2021}
& \multirow{-4}{*}{29,157,083}
& \multirow{-4}{*}{92.2\%} & \multirow{-4}{*}{26,885,302}
& \multirow{-4}{*}{3.0\%}  & \multirow{-4}{*}{875,783}
& \multirow{-4}{*}{-0.6\%}
& \multirow{-2}{*}{458,178} & \multirow{-2}{*}{7.2×} & \multirow{-2}{*}{3,282,824}
& \B 22,400,000 \\
\hline
& & & & & & & & & & \T 18,000,000 \\
& & & & & &
& \multirow{-2}{*}{1,044,277} & \multirow{-2}{*}{6.4×} & \multirow{-2}{*}{6,698,201}
& 21,600,000 \\
& & & & & & & & & & 31,400,000 \\
\multirow{-4}{*}{2022}
& & & & & & & & & & \\
\end{tabular}
\end{table}


\subsubsection{Meta's Intransparent Transparency}

Unfortunately, Meta's statistics aren't just incomparable to \NCMEC's. They also
suffer from several other shortcomings. Let me walk you through them. Looking at
the numbers in Table~\ref{table:csam}, Meta is obviously rounding its
statistics. Worse, it does so with different precision for Facebook and
Instagram and does so even for the machine-readable data. As far as I can tell,
Meta also is the \emph{only} platform to do so.

Originally, Meta started disclosing ``Child Nudity \& Sexual Exploitation'' for
Facebook only in Q3~2018 and for Instagram as well in Q2~2019, hence the
horizontal line above that latter quarter's entry in Table~\ref{table:csam}. In
Q2~2021, the firm switched to reporting ``Nudity and Physical Abuse'' and
``Sexual Exploitation'' under the ``Child Endangerment'' heading, hence the line
above that quarter's entry in Table~\ref{table:csam}. Based on metrics names,
one might expect that ``Child Nudity \& Sexual Exploitation'' includes ``Child
Endangerment: Sexual Exploitation.'' Yet the counts reported from Q2~2021
forward seem markedly higher than those from before. Notably, the 4.6$\times$
increase from Q1~2021 to Q2~2021 is jarring.

Alas, Meta's transparency pages give \emph{no} explanation. Only after searching
for ``Meta Community Standards Enforcement Report Q2~2021'' with an external
search engine, did I find a blog post~\cite{Facebook2021a} pointing towards a
PDF file that hints at an explanation~\cite{Facebook2021}. Apparently, Meta
started measuring ``Child Physical Abuse'' only that quarter and included it in
the at least appropriately named ``Nudity and Physical Abuse'' metric. It also
started measuring ``Sexualization of children'' and ``Inappropriate interactions
with Children'' and included both in the ``Sexual Exploitation'' metric, too.
That would explain the consistently much higher numbers. However, combining such
disparate categories of content into one metric and then labelling that metric
by the most severe category is highly inappropriate and deeply misleading.

Even before Meta thusly changed metrics, there was some indication that Meta was
inflating statistics. In February 2021, Antigone Davis, Meta's Global Head of
Safety, reported in a blog post~\cite{Davis2021} that 90\% of pieces reported to
\NCMEC\ during October and November 2020 had been reported before or were
visually similar to previously reported content. Notably, six videos accounted
for more than half of the reported content. Further analysis of 150 accounts
reported to \NCMEC\ during July and August 2020 as well as January 2021 showed
that 75\% of them didn't share \CSAM\ with malicious intent but ``for other
reasons, such as outrage or in poor humor (i.e. a child's genitals being bitten
by an animal).'' While Meta still is legally mandated to report such instances
to \NCMEC, not including such information in its transparency disclosures is
negligent at best. By comparison, Pinterest's transparency disclosures do
distinguish between posts and the content itself, including for
\CSAM~\cite{Pinterest2022}.

While extracting the data for Meta and Google from \NCMEC's transparency
disclosures, I discovered a hint for a significant hole in Meta's disclosures.
For 2019 and 2020, \NCMEC\ reported one number for Meta under its old name
Facebook. But in 2021, \NCMEC\ reported three counts, one for Facebook, one for
Instagram, and one for WhatsApp. \emph{WhatsApp}??? Meta makes no transparency
disclosures about WhatsApp, neither in its ``transparency center'' nor on
WhatsApp's website, beyond some outdated statistics about California's privacy
law.

Furthermore, while replacing the original spreadsheet behind the above table
with Python code, I discovered that data going back almost two years had been
changed between Meta's disclosure for Q2~2022 and Q3~2022. The changes are
fairly substantial: Out of 113 modified data points, 77 were for Q4~2020, 3 for
Q1~2021, 4 for Q2~2021, and the rest for Q2~2022. Out of the 77 for Q4~2020, 58
are absolute counts, which changed between -50.0\% and -0.1\%. Yet neither
transparency report nor the blog post announcing the report's release mention
anything about these changes. That seems sloppy at best.


\subsubsection{The Global Spread of \CSAM}

\NCMEC\ also releases report counts broken down by individual countries. Since
the largest non-Chinese social media, with exception of Telegram, are all based
in the United States, that breakdown should be fairly representative of \CSAM\
sharing patterns across the world. Hence, I decided to wrap up my investigation
into the quality of transparency disclosures by analyzing \NCMEC's dataset for
2021~\cite{NcmecByCountry2021}.

Right away, I did encounter several minor data quality issues. Notably, the data
included two different entries for French Guiana. Furthermore, it included
entries for (1) the Netherlands Antilles, (2) Bonaire, Sint Eustatius, and Saba
(3) Curaçao, as well as (4) Sint Maarten, even though the former was split into
the latter three in 2010. Finally, it included an entry for Bouvet Island, which
seems unlikely since the island, located in the South Atlantic close to
Antarctica, is an uninhabited nature preserve with no man-made structures beyond
a weather station.

To characterize geographic distribution, I computed regional totals as well as
per-capita rates per country. Taken together South and South East Asia are
responsible for 58.1\% of all reports despite only comprising 33.8\% of the
world population. However, when considered at the granularity of individual
countries, the countries with the highest number of reports per capita are
concentrated on the Arab peninsula and in North Africa, abutting the
Mediterranean. The exception are the Cocos or Keeling Islands; at 168 reports
per 596 capita, its rate is seven times higher than that of the second ranked
country, Libya. It likely also is a fluke thanks to the tiny population.
Overall, my results are consistent with those reported by a team from Google,
\NCMEC, and Thorn in 2019~\cite{BurszteinBrightea2019}. The supplemental
materials for this paper include both data and the notebooks with the analysis.
