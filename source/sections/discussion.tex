\section{Discussion}
\label{sec:discussion}

Between the depth of the analytical case studies in \S\ref{sec:dalle} and
\S\ref{sec:tweet-da-fe} as well as the breadth of the social media census in
\S\ref{sec:census}, this paper presents a comprehensive first exploration of the
stochastic penal colony. It demonstrates that mostly punitive usage of \AI\ is
far from aberation or exceptional occurrance but rather the established, inhuman
norm when it comes to content moderation. While the biggest platforms, Google,
Meta, and Twitter, are almost completely in thrall of algorithmic moderation,
smaller platforms are slowly catching up as well. They certainly have the
punitive enforcement down pat.

More fundamentally, algorithmic content moderation poses a paradox: Basic
economics serve as driving force for social media to continue expanding its
reach. The (misleading) gloss of technological neutrality and infallibility only
strengthens it. As illustrated in \S\ref{sec:dalle} and \S\ref{sec:tweet-da-fe},
that is bound to result in less transparency and diminished
fairness~\cite{GorwaBinnsea2020}. Yet algorithmic content moderation also
provides a near perfect opportunity for abandoning the stochastic penal colony.
After all, when \AI\ detects and intercepts violative content before it is
posted, there is \emph{no} human harm and hence no need for punishment. By
nonetheless insisting on punishment, social media are at best cosplaying
Prudence Pingleton absolutely, positively, permanently punishing her daughter
Penny~\cite{Waters1988} or at worst making the IngSoc Party's ruthless
prosecution of thoughtcrimes become reality~\cite{Orwell1949}.


\subsection{Moderating the Public Interest}

Remarkably, the punitive overreach described in this paper is not social media's
only overreach. Several policy provisions are so expansive they conflict with
the public interest. Notably, \DALLE's prohibition against politics and
Pinterest's prohibition against criticism place significant a-priori constraints
on public debate and hence are fundamentally anti-democratic. \DALLE's
prohibition against health-related content may interfere with both science and
public health, which are key concerns for any state, whether democratic or not.
Finally, the prohibition against violence may discriminate against religious
beliefs. Other examples include prohibitions against extremism and violence
resulting in the suppression of evidence for large-scale human rights violations
and prohibitions against hate speech and sexual content being levelled against
the very people they are intended to protect, notably \LGBT\
folk~\cite{EidelmanLeeea2021,RahmanAlJaloudAlKhatibea2019,Wille2020}.

While I focused on \DALLE\ and the crucifixion in \S\ref{sec:escape}, all social
media surveyed in \S\ref{sec:census} besides Telegram have equivalent
prohibitions against violence and, with some exceptions for Tumblr and Twitter,
also against sexual content. At the same time, many foundational religious texts
contain passages that are blatantly, even excessively violative. Consider the
story of Abraham's nephew Lot, who hosted \YHWH's messengers at his home in
Sodom, protecting them from the mob. In return, the messengers saved him, his
wife, and youngest two daughters before \YHWH\ destroyed Sodom and Gomorrah. The
story appears in the Hebrew Bible and the Quran and hence has direct scriptural
significance for Judaism, Christianity, and Islam alike.

The relevant chapter in the Hebrew Bible, Genesis~19, includes all male
inhabitants of Sodom (save Lot) clamoring to gang rape \YHWH's messengers,
\YHWH\ destroying the cities of Sodom and Gomorrah, their population, and all
surrounding fields with a rain of fire and brimstone, Lot's wife being turned
into a salt pillar for copping a glance at \YHWH\ during Their orgy of
destruction, and Lot's surviving two virgin daughters successfully conspiring to
get daddy drunk so that he knocks them up. If that wasn't disturbing enough, the
destruction of Sodom and Gomorrah was coldly premeditated. In Genesis~18, \YHWH\
hesitates at first to share Their plans for the two cities with Abraham. When
They do, Abraham expresses grave concern about innocent people dying as a result
and ends up haggling \YHWH\ down to a maximum number of~9 instead of~49.

In short, scriptural precedent easily violates many contemporary content
prohibitions and illustrates how unrealistic and contrived blanket bans against,
amongst other things, sex and violence are. I do feel it incumbent on me to
acknowledge that my summary of Genesis~18 and~19 is severely skewed to make a
point. The same two chapters also describe Abraham and Lot exhibiting generous
hospitality towards strangers. Lot even offer his own daughter in the
messengers' stead to the mob outside his home. \YHWH\ doesn't appear solely as
vengeful God, simply calling out Sarah, Abraham's wife, for lying when she
denies having laughed at \YHWH's announcement that she would have a son, even
though she long ago went through menopause. \YHWH\ also cedes substantial ground
in Their discussion about collateral damage with Abraham. Finally, Genesis~18
switches repeatedly and seamlessly between Abraham and Sarah interacting with
three men and interacting with \YHWH, making this one of the earliest passages
suggesting that \YHWH\ and the Holy Trinity are one and the same.


\subsection{Expert Disinformation}

Ironically, when social media do consider the public interest, they use it to
justify their prohibitions against disinformation. In doing so, social media are
supported by a majority of \US\ respondents in at least one survey. Remarkably,
however, political affiliation has a significant impact on people's attitudes,
with Republicans ``consistently less willing than Democrats or independents to
remove posts or penalize the accounts that posted
them''~\cite{KozyrevaHerzogea2023}. That would suggest that, to a degree,
disinformation has become a political cudgel, or at least a shortcut to avoid
the hard work of actually informing and convincing others.

Furthermore, as the pandemic illustrated over and over again, so-called experts
haven't done so well with adjusting to new facts and disseminating correct
information~\cite{SinghMcNabea2021}. Notably, the \WHO\ took years to overcome
the influence of outdated theories and acknowledge airborne transmission of
Covid-19~\cite{Chamary2021,GreenhalghOzbilginea2022,JimenezMarrea2022,Lewis2022,SirleafClark2021,SirleafClarkea2021}.
Its mask guidance wasn't much better~\cite{ChanLeungea2020} and inconsistent
with that of the \CDC~\cite{Curtis2020}. Then again, the \CDC's mask guidance
was often flawed as well, with the agency changing its position for what appear
to be political or otherwise arbitrary reasons~\cite{Netburn2021}. Furthermore,
the \CDC's recommendations on other aspects, notably testing, were also
subpar~\cite{Flam2022,Ngo2022,Scott2022}. So do we really want outsourced,
barely trained content moderators, who are toiling away under despiccable
working conditions~\cite{Newton2019b,Newton2019}, to judge what is legitimate
information and what is illegitimate disinformation in at most 30 seconds per
post?

Tragically, in all that hoopla about airborne transmission, masks, and
vaccinations, another, grave failure of medical experts turned into draconian,
inhuman policy---hospitals prohibiting visitors even for people on respirators,
who often ended up dying alone---took hold across many hospitals without
meaningful public debate or pushback. While the prohibition against visitors may
indeed eliminate potential disease carriers who otherwise might infect
vulnerable patients in general, that argument does not hold for Covid-19
patients on ventilators. They are already infected and face significant odds of
death---almost 1:2 in one study from the beginning of the
pandemic~\cite{AuldCaridiScheibleea2020}. Yet they breathe through a machine, in
a low pressure room with sophisticated air filters. Meanwhile, caregivers wear
high-quality masks and were the first to be vaccinated. In short, the infection
risk posed by patients on ventilators to visitors and visitors to personnell is
low. And even if it was higher, a person's deathbed requires more respect and
hence calls for different trade-offs. Back in manifest reality, most hospitals
imposed a draconian, inhuman policy that leads to worse outcomes, long-term
impairments, and possibly terrible deaths for patients and also significant
trauma for loved ones and caregivers
alike~\cite{AndersonShawZar2020,AnnYiAzharea2021,Capozzo2020,DCouto2022,StrangBergstromea2020,WakamMontgomeryea2020}.


\subsection{Transparency Theater}
\label{sec:transparency-theater}

As if the systemic and broad overreach of their policies wasn't enough, social
media also fail at transparent and accountable governance. The lack of
transparency disclosures, with platforms addressing at most ten out of eighteen
criteria, is somewhat disappointing. Ambiguous policy terms and inconsistent
metrics noted in \S\ref{sec:trusting-twitter} and \S\ref{sec:census-results} are
certainly confusing and get in the way of meaningful comparisons between
platforms---as the British communications regulator recently noted as
well~\cite{HarlingHenesyea2023}. But arguably the most corrosive finding of my
social media census are the many data quality issues and inconsistencies for
platforms other than Google and Reddit presented in
\S\ref{sec:census-validation}. They directly undermine any confidence in the
accuracy of platforms' transparency reports and instead raise more questions
than they answer. That is strong evidence in support of Evelyn Douek's claim in
her December 2022 Harvard Law Review article that social media platforms do not
provide meaningful transparency but rather engage in transparency
theater~\cite{Douek2022}.

That does \emph{not} mean that current transparency reports are useless. In
\S\ref{sec:census}, I gave the example of TikTok improving its algorithmic
content moderation based on the reported fraction of videos flagged by \AI.
Another interesting vignette gleaned from transparency reports concerns Reddit.
In 2021, more than half of its account sanctions were for ban evasion, i.e.,
users not accepting bans from individual subreddits or the entire site and
creating new accounts to access those same fora again. If we assume that at
least some of these users are not only driven by a desire to troll or otherwise
disrupt old haunts, then Reddit suspending their new accounts as well represents
tremendous wasted potential for the platform. After all, these are users, who
care so deeply about Reddit that they go out of their way to access the site.
But instead of finding ways of turning them into net-positive contributors
again, Reddit's punitive account moderation is pushing them away. The large
percentage of sanctions for ban evasion also is troubling given that using
throw-away accounts for posting about sensitive subjects has been an integral
and extensively studied part of culture on
Reddit~\cite{AmmariSchoenebeckea2019,AndalibiHaimsonea2016,ChoudhuryDe2014,Leavitt2015,PavalanathanDeChoudhury2015,SchradingOvesdotterAlmea2015,TreemLeonardi2013}

The problem is that, beyond such small vignettes, it is pretty difficult to gain
any systemic insight into the impact of social media, especially if we'd prefer
to do so in a scientifically acceptable, quantitative manner. Meta again leads
by significant failures. Consider its academic data sharing effort through
Harvard's Social Science One. It was supposed to facilitate unprecedented
empirical research on election integrity. In manifest reality, the first data
release was two years late, the data was marred by overly aggressive
randomization Meta does not apply to the same data shared with non-academic
partners, and the omission of about half of \US\ users was discovered only 1.5
years later, calling the results of ``dozens of papers'' into
question~\cite{Hegelich2020,HegelichMarcoea2020,Ingram2022,OHaraNelson2019,Timberg2021}.

Collecting one's own data by scraping social media websites isn't an acceptable
option either---at least not to Meta. The firm has repeatedly changed the code
of its website for the sole purpose of obstructing scraping (thereby also
breaking accessibility) and in 2021 even terminated the Facebook accounts of
academic researchers---even though they are at the same university as the firm's
chief \AI\ scientist---because they kept upstaging Meta's noticeably incomplete
database of political
ads~\cite{EdelsonMcCoy2021a,Faife2021a,MerrillTobin2019,Roose2021a}. That
certainly is a bit rich for a company that got started by Mark Zuckerberg
scraping the Harvard student directory for images~\cite{Madrigal2019} and that
has been utilizing a scraping service for many years into early 2023
itself~\cite{Newman2023}. Meta's many late, incomplete, and compromised
disclosures and its active obstruction of outside investigations are certainly
impressive. But the most striking aspect about them is that the same apparent
hostility towards transparency extends to other aspects of Meta's business too.
Notably, the firm has been manipulating ad impressions---only the most basic
advertising metric according to the firm's own \SEC\ filings---and thereby has
been misleading customers, investors, and regulators alike~\cite{Grimm2022a}.


\let\Oldthefootnote\thefootnote
\renewcommand*{\thefootnote}{\ding{164}}
\subsection{``The Great Question before us is: Can we Change? In Time?''%
\texorpdfstring{\>\!\protect\footnotemark}{}}
\footnotetext{%
    From the opening monologue of Tony Kushner's \emph{Angels in
    America, part~2, Perestroika}~\cite{Kushner1993}}
\let\thefootnote\Oldthefootnote

\subsubsection{Regulating Content Moderation}
\label{sec:regulating:moderation}

So what should we do about the many failures of social media content moderation?
Since I was just bemoaning Meta's apparent unwillingness to be even minimally
accountable, I'm going to continue with that thrust. The United States has been
missing in action when it comes to meaningful regulation of technology companies
in general and social media in particular. The country's inaction has allowed
internet platforms to ride roughshod over people who value privacy and civility
and to saddle especially African, Asian, and South American countries with
oppressive negative externalities. The two genocides Meta helped bring about
(see \S\ref{sec:census-limits}) are but two particularly extreme examples
amongst
many~\cite{DwoskinNewmyerea2021,ElliottChristopherea2021a,FrenkelAlba2021,Iyengar2021,MacSilverman2020,Morris2021,Newton2021,Satariano2021,Scott2021,SilvermanMacea2020,Simonite2021,WongEllisPetersen2021,WongErnst2021,WongHarding2021,ZahrzewskiDeVynckea2021}.
Not surprisingly, that is having a negative impact on the United States'
international reputation~\cite{GeltzerGhosh2018}. Relying on the Brussels
effect~\cite{Bradford2020} for solving one's homegrown problems isn't a
particularly effective political strategy. Alas, given the extreme political
polarization in Washington, it seems like that's as good as the \US\ gets.

Nonetheless, it's not that hard to draft legislation that codifies a few ground
rules, while also requiring increased transparency to help drive public policy.
Notably, such legislation should require platforms to be:
\begin{enumerate}
    \item Be localized \emph{before} entering a foreign market;
        \label{itm:law:localized}
    \item Similarly have a trust and safety team comprising native speakers for
        \emph{all} major local languages at the ready; \label{itm:law:team}
    \item Notify users electronically of all content moderation actions with
        exception of ``deceptive high-volume commercial
        content''~\cite{EuropeanParliamentAndCouncil2022}, i.e., spam;
    \item Track who flags what content or users for what reasons and with what
        outcomes;
    \item Report attendant summary statistics at least once a year.
    \item Make granular event logs available to research projects approved by a
        jurisdiction's equivalent to the National Science Foundation in the \US.
        That institution also vets projects for global log access for \US-based
        platforms.
        \label{itm:law:research}
\end{enumerate}
For each country with high platform presence, i.e., where at least 10\% of the
local population are monthly active users, it should also require them to:
\begin{enumerate}[resume]
    \item Provide a meaningful appeals process for content moderation decisions;
        \label{itm:law:appeals}
    \item Include corresponding statistics in at least yearly transparency
        reports;
    \item Demonstrate that data collection and transparency reports have passed
        review by an independent auditor;
    \item Conduct a yearly review of systemic risks, develop and implement a
        plan for risk mitigation, and review implementation after the fact. Of
        particular interest are:
        \begin{itemize}
            \item Illegal content and activities;
            \item Fundamental rights including freedom of expression, right to
                privacy, right to non-discrimination, right of the child, and
                consumer protection;
            \item Democratic process and election integrity;
            \item Public health and emotional well-being.
        \end{itemize}
    \item Publish risk analysis, mitigation plan, and implementation review.
        \label{itm:law:publicrisk}
\end{enumerate}
Many of these obligations are directly based on the Digital Services Act. We
could even copy relevant clauses verbatim. Thanks to a 2011 decision by the
European Commission~\cite{EuropeanCommission2011}, such reuse is explicitly
encouraged. I included obligations~\ref{itm:law:localized}
and~\ref{itm:law:team} while also making obligations~\ref{itm:law:appeals}
through~\ref{itm:law:publicrisk} contingent on a platform's per-country presence
to counteract the current neo-imperialist treatment of countries outside of
North America and Europe. The intent is to make internet platforms more attuned
to the countries and cultures they have a significant presence in.

Obligation~\ref{itm:law:research} is a more realistic variation on the
equivalent provisions of the \DSA~\cite{Vermeulen2022}. The latter relies on
newly appointed per-country regulators, the \emph{digital services
coordinators}, for most oversight functions including the vetting of research
projects and hosting of sensitive platform data~\cite{Jaursch2022,Jaursch2022a}.
Having to ramp up all these functions before February 2024, most \EU\ members
are turning to existing regulatory agencies to also take on \DSA\ monitoring and
enforcement. But those agencies lack the necessary expertise when it comes to
data science and academic research~\cite{Jaursch2023}. Worse, much depends on
the Irish digital services coordinator, since it is responsible for the European
subsidiaries of Meta, Pinterest, TikTok, Twitter, and YouTube and hence also the
only regulator that can directly demand acccess to their
data~\cite{Albert2022,Jaursch2023}. At the same time, Irish oversight of these
same firms under the \GDPR\ has been decidedly ho-hum, with several high-profile
complaints not moving forward over years~\cite{Burgess2022}. In contrast, my
proposal delegates to an organization that has over 70~years of experience with
vetting research projects and has the processes in place to do so in a timely
manner and at scale.

Probably the biggest weakness of the \DSA\ is its dependence on national
regulators. Another shortcoming, at least from the perspective of this
independent investigator, is the \DSA's mandate that researchers are affiliated
with a research institution. At the same time, the \DSA's treatment of privacy
concerns is refreshing: It requires that research proposals identify any privacy
risks and include appropriate mitigations. Yet it does not allow social media to
use privacy as an excuse for not providing data. Next, experience with national
regulators insufficiently enforcing \GDPR\ did result in the European Commission
taking on \DSA\ oversight functions in addition to national regulators. Its
stated priorities are ramping up its own oversight processes, determining
supervisory fees, and preparing independent audits of very large
platforms~\cite{Bertuzzi2022,TenThije2022}. I am glad to note the third priority
because it does hold the promise of higher quality transparency disclosures.
Once the Irish digital services coordinator has figured out how to vet research
projects and convince platforms of handing over the requested data, these
provisions may just lead to more high-quality empirical research on social media
by more teams. It's a nice touch that the \DSA\ explicitly requires open access
publication of results.

Finally, we make sure that even platforms with large reserves of cash---e.g.,
Meta had \$40.74 billion of the stuff on December 31,
2022~\cite{MetaPlatforms2023}---don't treat fines as the cost of doing (shady)
business. Hence we impose fines amounting to 5--10\% of the previous year's
global revenue for each case of non-compliance. I'm pretty sure that will
convince even Mr Zuckerberg that accountability has tremendous value. And if it
doesn't, we keep doubling fines until he does.


\subsubsection{Moderating Regulation}

Except I am writing this paper to bemoan the very excessiveness of content
moderation, in policy and implementation, and to push back against punitive
interventions that dehumanize in the name of platform safety. So instead of
advocating a similarly punitive approach towards internet platforms, I'd much
rather extend the same humane courtesy towards social media, including even
Meta. I readily admit that this doesn't come easy, since I consider much of that
firm's wealth ill-gotten and tained by blood. Ethics sure are troublesome.

Except that Douek's Harvard Law Review article raises a good number of
substantive issues with the obligations I just outlined. The starting point for
Douek's article is equivalent to my previous point, namely that content
moderation bureaucracies are people too. And these people are increasingly
moderating based not on content but behavior, often cooperating with
governments, e.g., when it comes to \CSAM, relying on outside organizations
serving as trusted flaggers, and trying out means for devolving control to
smaller communities. The article further attests systemic short-comings due to
unrealistic expectations about perfectability, too much focus on false positives
to the (almost) exclusion of false negatives, likewise too much focus on
individual cases instead of systemic failures, including when it comes to
transparency, and procedural justice privileging the lucky few---an important
insight about equity in content moderation. Overall, I am deeply appreciative of
Douek's focus on systemic forces and concur with many of her observations. As
discussed in \S\ref{sec:transparency-theater} above, my own findings of
pervasive data quality issues support Douek's contention that transparency
disclosures are not very meaningful.

However, the article also suffers from a few major issues. Notably, the
article's basic conceit, that law makers' and academics' misconceptions about
content moderation amount to a ``standard picture,'' seems contrived and too
much of a straw man---especially coming from a professor at Stanford's law
school. Furthermore, after developing the hierarchy of census criteria presented
in \S\ref{sec:census}, I had no difficulty integrating Douek's observations. In
fact, I had independently articulated some of them already. Likewise, the above
regulatory requirements account for many of these criticisms already.
Furthermore, the article acknowledges the \DSA's risk reviews as suitably
systemic and we could easily incorporate the call for bulk reviews and appeals
of similar content into the above provisions. In other words, the chasm between
what Douek calls the standard picture on one side and manifest reality of
content moderation on the other side may not be nearly as far or deep as Douek's
article makes it out to be. Finally, her discounting of speech as plentiful and
hence ``not so special'' isn't just ``almost sacrilegious.'' It runs real danger
of depriving platform users of their voices, even though many people use these
platforms for that same reason---to have a voice. That effectively makes this
argument deeply cynical. It would be a real shame if the humanity of moderators
required treating the moderated as more or less fungible and hence denying their
humanity. Surely, we can do better than engage in such false zero sum games!


\subsubsection{Contextualizing Transparency}

In fact, there is a simple unifying explanation that accounts for shoddy
accountability and befuddled observers alike. It may not amount to much more
than an informed guess at this point. But it is a compelling guess nonetheless
and hence well worth exploring in future work. As I was reading and re-reading
transparency disclosures and becoming familiar with their structure and lingo,
they never ceased to make for frustrating reading. I eventually realized that
the primary cause was their deeply reductive presentation. Each metric exists in
isolation, with little motivation and hardly any helpful context, and hence
comes close to being an abstract cipher. Treating that semantic wasteland as
opportunity, marketing and public relations folk then added their unique kind of
spin, adding introductory notes that celebrate the platform's progress---towards
what exactly usually remains a mystery---and manipulating units and histogram
buckets for appearances and not insight. Hence we find Twitter throwing 0~views
into the same bin as 1—9 views when reporting reach (see
\S\ref{sec:trusting-twitter}), Meta not disclosing \NCMEC\ reports or unique
pieces when reporting \CSAM (see \S\ref{sec:census-validation}), and most every
platform not relating content sanctions to account sanctions.

That is not to say that platforms set out to spin transparency reports to their
marketing or public relations advantage. That's a secondary effect, with those
professionals doing what they are trained to be doing. At the same time, social
media bear more than a little responsibility for this reductionist view taking
hold. When I started working for Facebook during the summer of 2018 and went
through ``bootcamp,'' their orientation program for newly hired engineers,
several long-term employees made it a point of pride that a small number of
engineers were supporting a huge number of users. In fact, Andrew ``Boz''
Bosworth made just that point in the 2009 blog post announcing the creation of
bootcamp~\cite{Bosworth2009}. While that has changed somewhat in recent years,
this disparity used to extend to the overall number of employees as well. It was
markedly lower than that of technology industry peers. Notably, by the end of
2018, Facebook had about 36,000 employees, while Alphabet had 99,000, Apple
132,000, and Amazon
648,000~\cite{MacrotrendsAlphabet,MacrotrendsAmazon,MacrotrendsApple,MacrotrendsMeta}.
More generally, by trying to monopolize our attention and then bundle that
attention for sale to the highest ad bidder, social media have been reducing
their users to fungible ad impressions. That of course is dehumanizing as well
and suggests that these platforms are \emph{asocial media} first and foremost.

If we believe this explanation for the reductionist presentation of transparency
disclosures, then the obvious solution is to re-establish context and re-surface
dependencies. A reasonable approach for getting us started are user story
mappings~\cite{PattonEconomy2014}, only here we track content and users through
the content moderation process. Discounting spam and behavioral triggers, a
simple first template for creating such a content moderation mapping, with
variables shown italicized between angular brackets, might read:
\begin{quote}
In \var{country}, users post \var{content} at \var{volume}. Content violating
\var{prohibition} has \var{prevalence}. It is flagged by \var{flagger} and
results in \var{sanction} for \var{content-or-user}. Users appeal \var{fraction}
of these sanctions, which are reviewed by \var{moderator} and result in
\var{outcome}.
\end{quote}
Since the template represents a process, that also suggests some kind of flow
diagram for (literally) establishing the big picture and thereby providing a
more meaningful overview over the impact of content moderation on content and
users. Since the diagram needs to accommodate eleven variables, it will
necessarily be more complex than most. Since it also needs to accommodate
categorical as well as numerical variables, neither Sankey nor alluvial diagrams
are a clear fit. A hybrid flow diagram should do nicely.

At the same time, not all context is process oriented. Notably, in
\S\ref{sec:census}, I touched on several metrics for characterizing content,
including user posts, pieces of content attached to posts, unique pieces of
content, and for \CSAM, reports to \NCMEC. Since they all capture a different
aspect of the same content, none of them is a-priori more representative and
they should \emph{all} be disclosed \emph{in relation to each other}. That
suggests a parallel sets diagram as suitable visual representation. Another
cause for significant frustration are unexplained and surprising changes in the
data reporting period over reporting period. Addressing them will require
explanations in prose, ideally right next to the data and never in a separate
document. Their preparation may very well involve additional data collection and
analysis, which suggests that at least one report author is a data scientist or
software engineer with suitable background (e.g., a PhD).


\subsubsection{Subverting the Penal Colony}

While such a more holistic and hence humanistic approach towards transparency
might be expressible as a provision in regulation, the result would certainly
run counter my original motivation and intent. When it comes to counteracting
the punitive overreach of contemporary content moderation, I very much doubt
that such a phrasing would be even possible. I also see little chance for that
happening in a society that arbitrarily brutalizes Black, Brown, and Poor people
as a routine matter and simultaneously pretends that this has anything to do
with justice. In that context, it is deeply disappointing, to put it mildly,
that one of the architects of this abominable system, Joe Biden, has not devoted
his presidency to razing the carceral state and sowing it with
salt~\cite{Reinhart2022,StolbergHerndon2019}.

The moral case for moderating content moderation can be put succinctly. I direct
this primarily at trust and safety professionals:

Digital platforms impose too many prohibitions that seek to impose a utopian
positivity and pleasantness onto human expression and enforce these provisions
with draconian penalties that create a rapidly increasing underclass of the
deplatformed.


the just described more holistic approach towards transparency might be
incorporated into legislation, I doubt that the punitive overreach of
contemporary content moderation could be stopped by regulations. Certainly not
in the \US, which has demonstrated an astounding ability to arbitrarily
brutalize Black, Brown and Poor people through the carceral state and then
pretend that



brutalize Black, Brown, and Poor people through the carceral state. It certainly
is disappointing, to put it mildly, that one of the architects of the carceral
state, Joe Biden, instead of devoting his presidency to righten a devastating
wrong, has done little to nothing to raze the city and sow it with salt.


raze the carceral state and salten the



violate
the human rights of Black, Brown, and Poor people



\subsubsection{Reaffirming Human Supremacy}

When it comes to \AI, previous work on dataset, model, and system cards already
provide a reasonable framework for practicing more holistic transparency and
covering relevant quantitative as well as qualitative characteristics of
algorithmic
interventions~\cite{GebruMorgensternea2021,MitchellWuea2019,ProcopeCheemaea2022}.
By definition, however, such cards cannot disclose what nobody has considered or
encountered yet---including due to personal or institutional blindspots or
groupthink. That's exactly the reason why hands-on probing by outsiders is so
important and, as \S\ref{sec:escape} and previous work have
demonstrated~\cite{BirhanePrabhuea2021,CarliniHayesea2023}, may just surface all
sorts of gremlins, orks, and cenobites that have been lurking in latent space.
As the lead for OpenAI's alignment team put it in a recent
interview~\cite{Heaven2023}: ``I think it's very difficult to really anticipate
what the real safety problems are going to be with these systems once you've
deployed them.'' In other words, hands-on engagement with production systems is
the only way to make them safe.

Furthermore, that's exactly the reason why we should always reject ``disclosing
the inner workings of our safety mechanism renders them ineffective'' as
justification for non-disclosure. Chances are that the people making this
argument---OpenAI's chief scientist and co-founder Ilya Sutskever only was the
most recent~\cite{Vincent2023a}---never even evaluated the safety of their
systems, haven't completed evaluation and mitigation, or have too little or too
much confidence in their work. In other words, if someone make this argument,
safety researchers should start paying particular attention to that person's
organization and \AI\ deployments. Or as Cory Doctorow succinctly put it in the
title of a blog post about the same fallacy: ``Como Is
Infosec''~\cite{Doctorow2022}. While I am not a fan of the syllabic abbreviation
``como,'' Doctorow knows his intended audience. The Trust \& Safety Professional
Association emerged out of the COMO Summit in 2018. (The people running that
association are either completely clueless or have a wicked sense of humor:
Their yearly shindig is called ``TrustCon.'')

Since there is no meaningful regulation requiring transparency about \AI-based
interventions and so many fundamental questions about especially generative \AI\
remain unsettled, legal uncertainty and transparency may interact in suprising
and counter-productive ways. For \DALLE~2, OpenAI published a detailed
scientific paper~\cite{RameshDhariwalea2022} and released a system card before
their April 2022 preview release~\cite{MishkinAhmad2022}. While the paper is
sufficiently detailed for others to recreate OpenAI's implementation, the firm
never released the model and its weights. It also has been consistently and
completely silent on the exact composition of its training data. By contrast,
Stability AI and its collaborators have disclosed the training data, the source
code for running the model, and the actual model parameters for Stable
Diffusion. The latter clearly is the more transparent and communitarian effort.
But it is Stability AI that is at the receiving end of highly visible copyright
lawsuits by a stock photography agency and three
artists~\cite{Butterick2023,Setty2023}, whereas OpenAI appears to have avoided
them (so far). As I already observed at the end of \S\ref{sec:dalle}, OpenAI has
stellar public relations and legal talent!

According to a profile in New York Magazine, Emily M Bender made a rule for
herself: ``I'm not going to converse with people who won't posit my humanity as
an axiom in the conversation''~\cite{Weil2023}. I concur. And add: At the end of
the day, \AI\ certainly is artificial but it has little to do with human
intelligence. It fundamentally is software, software that runs on a computer.
Like all other software humans have created so far, \AI\ is buggy. The kinds of
bugs may have changed. And as demonstrated in \S\ref{sec:escape}, effective
attacks on this type of software may have changed. But the fundamental truth
remains: Humans are so much more than software. We rule supreme. If software
turns into a threat to even a single human, there is a very simple solution that
also scales: Switch off the damn computer and wipe its storage!

When well over a thousand industry leaders and experts call for a moratorium on
the most advanced \AI\ techniques~\cite{MetzSchmidt2023}, far more than a single
human are threatened. Though maybe not quite in the way the letter describes the
threats~\cite{KapoorNarayanan2023}. Nonetheless, switching off the damn
computers and wiping their storage remains a viable option. Since that option
also is rather destructive, I am instead going to sketch the outlines of a new
intellectual property regime for \AI. The primary goals for the new \IP\ regime
are to hold those who are pushing the state-of-the-art in \AI\ accountable for
their deployments and to ensure that everyone benefits from \AI. As before, I am
particularly concerned with transparency not because it suffices, but because it
is critical for facilitating sound policy decisions in the future. The new \AI\
\IP\ or \AIP\ regime is inspired by (1) OpenAI's charter, (2) \US\ copyright
provisions for works created by the government, and (3) the internationally
accepted foundations of patent law.

While OpenAI's charter nominally concerns itself with \AGI or artificial
\emph{general} intelligence---``highly autonomous systems that outperform humans
at most economically valuable work''---the same arguments apply to contemporary
``lesser'' \AI as well. The firm states that \AI\ should be ``used for the
benefit of all.'' Furthermore, ``our primary fiduciary duty is to humanity.'' I
couldn't agree more. But I'm not willing to take OpenAI's (or any other
organization's) word for it. \US\ copyright law points to just the right means
for enforcing that: Almost all works created by the government immediately enter
the public domain and are reusable by everyone. So \AIP\ does the same for
\emph{all} \AI\ systems including machine learning models and enters them into
the public domain by default, no matter under what circumstances they were
developed. In addition to ensuring broad access to the technology, public domain
for \AI\ also nicely accounts for recent models incorporating much material of
dubious provenance, which is plainly exploitative.

But with the public domain making it nearly impossible to keep a proprietary
\AI\ actually proprietary, most private investment into \AI\ would likely wither
away permanently. So \AIP\ takes a cue from patent law, which grants a
temporary, legal monopoly to inventors---as long as they fully disclose the
invention upfront. Hence, \AIP\ grants people and organizations a temporary,
legal monopoly on an \AI\ if they follow certain rules---more on them in a
moment. Given recent rapid progress in \AI, setting the monopoly length to the
same 20 years as for patents seems excessive. Instead, I am proposing a duration
of three to five years. To gain this temporary protected status, an \AI\
operator files an intent notice including a preliminary system card with the
national \AIP\ office before starting to train the model. The expectation is
that intent notices are almost always approved.

After completion of training, the operator files the finalized systems card,
which is published upon approval of the protected status on the agency's
website, as well as the actual source code and model weights, which are
embargoed until the end of the protected period. The primary rejection reason is
that an almost identical \AI\ has already been registered. By collecting the
actual artifacts, the \AIP\ website becomes a national if not global
clearinghouse for \AI\ and ensures that, with a couple of years lag, everyone
gains access to the same advanced technology. While the \AI\ has protected
status, the operator must file yearly updates to system card, code, and
models---or indicate that none are necessary. They must also provide an \API\
that gives \AI\ researchers direct access to the \AI\ for experimentation.
Operators are encouraged to make \API\ access free, but must not charge more
than the marginal cost of the compute capacity they consume through the \API.
Researchers must follow a responsible disclosure process, giving operators an
opportunity to remediate any flaws before they are made public. Unlike
\S\ref{sec:regulating:moderation}, the \AIP\ agency performs basic vetting of
researchers to reliably establish their identity. Meanwhile operators have no
vetting or veto rights. They should focus on having a working research endpoint
and collecting accurate data for their transparency disclosures.

In my mind, the most striking aspect of this proposal is how it builds on
well-established copyright and patent law and combines elements from both into a
new coherent regulation. Sure, OpenAI's charter has a strong socialist vibe,
which is suprising when you watch them being smart, aggressive, and exploitative
capitalists over and over again. Maybe, the charter is a remnant from Hippier
days, when OpenAI was still in fact open and when Google and Microsoft still had
ethical \AI\ teams~\cite{GrantBassea2021,GrantEidelson2022,Newton2023}. But
there is nothing Hippie about copyright and patent law. In fact, the core of my
\AIP\ proposal is exceedingly conventional and well-established.
