\section{Discussion}
\label{sec:discussion}

Between the depth of the analytical case studies in
\S\S\ref{sec:dalle}--\ref{sec:tweet-da-fe} and the breadth of the social media
census in \S\ref{sec:census}, this paper presents a comprehensive first
exploration of the stochastic penal colony. It demonstrates that mostly punitive
usage of \AI\ is far from aberation or exceptional occurrance but rather the
established, inhuman norm when it comes to content moderation. That many of the
firms covered in this paper also are significantly advancing the state of the
art in \AI, raises grave concerns about the future of \AI. Yet proactive
\AI-based content moderation also provides the perfect opportunity for
abandoning the stochastic penal colony. After all, when \AI\ detects and
intercepts violative content before it is posted, there is \emph{no} human harm
and hence no need for punishment. By nonetheless insisting on punishment, social
media are cosplaying Prudence Pingleton absolutely, positively, permanently
punishing her daughter Penny at best~\cite{Waters1988} or the IngSoc Party
ruthlessly prosecuting thoughtcrime at worst~\cite{Orwell1949}.

Remarkably, the punitive overreach described in this paper is not their only
overreach. Throughout this paper, I have identified conflicts between expansive
content policies and aggressive enforcement on one hand and the public interest
on the other hand. Notably, \DALLE's prohibition against politics and
Pinterest's prohibition against criticism place significant a-priori constraints
on public debate and hence are fundamentally anti-democratic. \DALLE's
prohibition against health-related content may interfere with both science and
public health, which are key concerns for any state, whether democratic or not.
Finally, the prohibition against violence may discriminate against religious
beliefs. Other examples include prohibitions against extremism and violence
resulting in the suppression of evidence for large-scale human rights violations
and prohibitions against hate speech and sexual content being levelled against
the very people they are intended to protect, notably \LGBT\
folk~\cite{EidelmanLeeea2021,RahmanAlJaloudAlKhatibea2019,Wille2020}.

While I focused on \DALLE\ and the crucifixion in \S\ref{sec:escape}, all social
media (besides Telegram) surveyed in \S\ref{sec:census} have equivalent
prohibitions against violence and sexual content. Furthermore, many foundational
religious texts contain passages that are blatantly violative. Consider the
story of Abraham's nephew Lot, who hosted \YHWH's messengers at his home in
Sodom, protecting them from the mob. In return, the messengers saved him, his
wife, and youngest two daughters before \YHWH\ destroyed Sodom and Gomorrah. The
story appears in the Hebrew Bible and the Quran and hence has direct scriptural
significance for Judaism, Christianity, and Islam alike.

The relevant chapter in the Hebrew Bible, Genesis~19, includes all male
inhabitants of Sodom (save Lot) clamoring to gang rape \YHWH's messengers,
\YHWH\ destroying the cities of Sodom and Gomorrah, their population, and all
surrounding fields with a rain of fire and brimstone, Lot's wife being turned
into a salt pillar for copping a glance at \YHWH\ during Their orgy of
destruction, and Lot's surviving two virgin daughters successfully conspiring to
get daddy drunk so that he knocks them up. Yet that same book of the Hebrew
Bible, Genesis, also contains utterly sublime passages, notably when \YHWH\
entrusts us with Their creation.

This double overreach makes clear that social media content moderation has gone
too far, regulating too much content and doing so in an excessively punitive
manner. Alas, making less moderation palatable to users will likely be difficult
and hence will require some offsetting functionality that lets them set their
own policies. One fairly simple, yet potentially effective option is to require
content warning labels for content that might be problematic and provide users
with simple tools for determining what labelled content if any they see. They
have been available on Mastodon for years and are deployed for a wide range of
content, including spoiler warnings, strong emotions, sensitive subjects ``like
addition, drugs, war, news, politics,'' and \AI-generated
content~\cite{Maloki2022,Sheehan2022,Voit2022}. Tumblr introduced their own
version in September 2022, though the choice of labels is far more conventional
and limited to mature, drug and alcohol addiction, violence, and sexual
themes~\cite{Tumblr2022}.

\S\ref{sec:census} also makes clear that, in addition to their immoderate
moderation, social media fall well short of any reasonable standards for
transparent and accountable governance, even though, as listed in
Table~\ref{table:governance}, all but one platform launched between \emph{one to
two decades ago}. Notably, zero platforms report how much content they label as
violative. Zero platforms report for how much content they reduce visibility.
Only one platform includes external oversight. Only two platforms commit to
meaningful notification and even those two make it quite hard to find their
commitment, forcing me to lean hard on the Electronic Frontier Foundation's
excellent prior work~\cite{CrockerGebhartea2019}. Finally, only four platforms
report on appeals and reversals. That is a major disconnect between platforms
and users, since meaningful appeals are essential for content moderation with
its many greys and hence opportunities for making ``the wrong call.'' The \EU's
\DSA\ does raise prospects for significant improvements in the near future.
However, experience with the \EU's previous ground-breaking internet regulation,
\GDPR, has been disappointing so far --- largely because national data
regulators, notably Ireland's, have been too slow to act and enforce
compliance~\cite{Burgess2022}.

As if a lack of transparency disclosures wasn't bad enough, the data quality
issues identified in \S\ref{sec:census-validation} subvert confidence in the
accuracy of current and future disclosures. Furthermore, none of the identified
issues are particularly complicated, which strongly suggests that Meta and
\NCMEC\ never bothered with much error checking or actually trying to use the
data. In other words, neither organization considered the transparency data
worthy of their full attention. It's just something that needs to be done.
Unfortunately, a similar dynamic seems to have played out with Meta's data
sharing through Harvard's Social Science One. It was supposed to facilitate
unprecedented empirical research on election integrity. But the effort has been
plagued by a two year delay until its first data release, overly aggressive
randomization that interferes with some of the intended research, and a major
omission that invalidated several already finished
studies~\cite{Hegelich2020,HegelichMarcoea2020,Ingram2022,OHaraNelson2019,Timberg2021}.

Collecting one's own data by scraping social media websites does not seem like a
good option either: It runs into strong opposition from Meta, which even
terminated the Facebook accounts of researchers who kept upstaging Meta's
woefully incomplete database of political
ads~\cite{EdelsonMcCoy2021a,Faife2021a,MerrillTobin2019,Roose2021a}. That is a
bit rich for a company that got started when Mark Zuckerberg scraped the Harvard
student directory for images~\cite{Madrigal2019} and that has been utilizing a
scraping service for many years itself~\cite{Newman2023}. Alas, the critical
lesson here is the need for regular third-party audits to ensure that
transparency or research data are, in fact, accurate and meaningful.
Conveniently, yearly audits \emph{are} included in the \DSA\ --- though that
does not help with research data disclosures.

By comparsion, \AI\ complicates matters: Dataset, model, and system cards cover
basic design and implementation of an algorithmic intervention reasonably
well~\cite{GebruMorgensternea2021,MitchellWuea2019,ProcopeCheemaea2022}. They
also are fundamentally limited to capturing characteristics that, one way or
another, were explicitly examined by a system's builders. At the same time, as
\S\ref{sec:escape} and also~\cite{BirhanePrabhuea2021,CarliniHayesea2023}
demonstrate, hands-on experimentation is uniquely suited to surfacing
unanticipated ``features'' that may be lurking in latent space or, just like
\DALLE\ acting out white supremacist imagery, result from unexpected
interactions between system components.

While OpenAI's expansive content policy and aggressive enforcement did not
exactly help matters, experiments with \DALLE\ nonetheless were fairly
straight-forward. Initially, that was in part due to \DALLE's sheer novelty,
which meant that I had nothing riding on the success of those experiments,
privately or professionally. By comparison, I'd be more reluctant to conduct
similar experiments on, say, Facebook or YouTube, especially in light of Meta's
recent deplatforming of researchers~\cite{EdelsonMcCoy2021a}. But content
moderation for \DALLE\ and social media still is conducive to experimentation
because (1) creating and posting content is cheap, if not free, and simple and
(2) the content moderation function is the only other large component besides
the component managing the content itself. That distinctly isn't the case, for
example, when evaluating commercial \AI\ solutions or when reviewing the
allocation of government benefits.

The remedy ought to be legal regulation: Require that any machine learning model
making decisions over human benefits in the widest sense of the word be exposed
by itself as an \API. The responsible organization may require registration from
researchers before they may use the \API\ and may also charge a nominal fee, not
to exceed the marginal cost of provisioning the additional server capacity. Such
a legal requirement would be stronger than current financial and \DSA\ audit
requirements because it allows anyone with the necessary background knowledge
and interest to also experimentally audit the \AI. But given the stakes and
regulatory capture in existing industries, that is just the point. Probably the
closest legal precedent is patent law, which also forces a public disclosure.
Patent law also grants a temporary monopoly on commercial exploitation of a
patent. But history provides some measure for how excessive current rights
grants have become: When the \US\ first enacted nationwide patent and copyright
protections in April and May 1790, respectively, the rights grants for either
kind of intellectual property lasted only 14
years~\cite{FisherIII1999,KhanSokoloff2001}!
