\section{Discussion}
\label{sec:discussion}

Between the depth of the analytical case studies in
\S\S\ref{sec:dalle}--\ref{sec:tweet-da-fe} and the breadth of the social media
census in \S\ref{sec:census}, this paper presents a comprehensive first
exploration of the stochastic penal colony. It demonstrates that mostly punitive
usage of \AI\ is far from aberation or exceptional occurrance but rather the
established, inhuman norm when it comes to content moderation. That many of the
firms covered in this paper also are significantly advancing the state of the
art in \AI, raises grave concerns about the future of \AI. Yet proactive
\AI-based content moderation also provides the perfect opportunity for
abandoning the stochastic penal colony. After all, when \AI\ detects and
intercepts violative content before it is posted, there is \emph{no} human harm
and hence no need for punishment. By nonetheless insisting on punishment, social
media are cosplaying Prudence Pingleton absolutely, positively, permanently
punishing her daughter Penny at best~\cite{Waters1988} or the IngSoc Party
ruthlessly prosecuting thoughtcrime at worst~\cite{Orwell1949}.

Remarkably, the punitive overreach described in this paper is not their only
overreach. Throughout this paper, I have identified conflicts between expansive
content policies and aggressive enforcement on one hand and the public interest
on the other hand. Notably, \DALLE's prohibition against politics and
Pinterest's prohibition against criticism place significant a-priori constraints
on public debate and hence are fundamentally anti-democratic. \DALLE's
prohibition against health-related content may interfere with both science and
public health, which are key concerns for any state, whether democratic or not.
Finally, the prohibition against violence may discriminate against religious
beliefs. Other examples include prohibitions against extremism and violence
resulting in the suppression of evidence for large-scale human rights violations
and prohibitions against hate speech and sexual content being levelled against
the very people they are intended to protect, notably \LGBT\
folk~\cite{EidelmanLeeea2021,RahmanAlJaloudAlKhatibea2019,Wille2020}.

While I focused on \DALLE\ and the crucifixion in \S\ref{sec:escape}, all social
media (besides Telegram) surveyed in \S\ref{sec:census} have equivalent
prohibitions against violence and sexual content. Furthermore, many foundational
religious texts contain passages that are blatantly violative. Consider the
story of Abraham's nephew Lot, who hosted \YHWH's messengers at his home in
Sodom, protecting them from the mob. In return, the messengers saved him, his
wife, and youngest two daughters before \YHWH\ destroyed Sodom and Gomorrah. The
story appears in the Hebrew Bible and the Quran and hence has direct scriptural
significance for Judaism, Christianity, and Islam alike.

The relevant chapter in the Hebrew Bible, Genesis~19, includes all male
inhabitants of Sodom (save Lot) clamoring to gang rape \YHWH's messengers,
\YHWH\ destroying the cities of Sodom and Gomorrah, their population, and all
surrounding fields with a rain of fire and brimstone, Lot's wife being turned
into a salt pillar for copping a glance at \YHWH\ during Their orgy of
destruction, and Lot's surviving two virgin daughters successfully conspiring to
get daddy drunk so that he knocks them up. Yet that same book of the Hebrew
Bible, Genesis, also contains utterly sublime passages, notably when \YHWH\
entrusts us with Their creation.

This double overreach makes clear that social media content moderation has gone
too far, regulating too much content and doing so in an excessively punitive
manner. Alas, making less moderation palatable to users will likely be difficult
and hence will require some offsetting functionality that lets them set their
own policies. One fairly simple, yet potentially effective option is to require
content warning labels for content that might be problematic and provide users
with simple tools for determining what labelled content if any they see. They
have been available on Mastodon for years and are deployed for a wide range of
content, including spoiler warnings, strong emotions, sensitive subjects ``like
addition, drugs, war, news, politics,'' and \AI-generated
content~\cite{Maloki2022,Sheehan2022}. Tumblr introduced their own version in
September 2022, though the choice of labels is far more conventional and limited
to mature, drug and alcohol addiction, violence, and sexual
themes~\cite{Tumblr2022}.

\S\ref{sec:census} also makes clear that, in addition to their immoderate
moderation, social media fall well short of any reasonable standards for
transparent and accountable governance, even though, as listed in
Table~\ref{table:governance}, all but one platform launched between \emph{one to
two decades ago}. Notably, zero platforms report how much content they label as
violative. Zero platforms report for how much content they reduce visibility.
Only one platform includes external oversight. Only two platforms commit to
meaningful notification and even those two make it quite hard to find their
commitment, forcing me to lean hard on the Electronic Frontier Foundation's
excellent prior work~\cite{CrockerGebhartea2019}. Finally, only four platforms
report on appeals and reversals. That is a major disconnect between platforms
and users, since meaningful appeals are essential for content moderation with
its many greys and hence opportunities for making ``the wrong call.'' The \EU's
\DSA\ does raise prospects for significant improvements in the near future.
However, experience with the \EU's previous ground-breaking internet regulation,
\GDPR, has been disappointing so far --- largely because national data
regulators, notably Ireland's, have been too slow to act and enforce
compliance~\cite{Burgess2022}.

As if a lack of transparency disclosures wasn't bad enough, the data quality
issues identified in \S\ref{sec:census-validation} subvert confidence in the
accuracy of current and future disclosures. Furthermore, none of the identified
issues are particularly complicated, which strongly suggests that Meta and
\NCMEC\ never bothered with much error checking or actually trying to use the
data. In other words, neither organization considered the transparency data
worthy of their full attention. It's just something that needs to be done.
Unfortunately, a similar dynamic seems to have played out with Meta's data
sharing through Harvard's Social Science One. It was supposed to facilitate
unprecedented empirical research on election integrity. But the effort has been
plagued by a two year delay until its first data release, overly aggressive
randomization that interferes with some of the intended research, and a major
omission that invalidated several already finished
studies~\cite{Hegelich2020,HegelichMarcoea2020,Ingram2022,Timberg2021}.

Collecting one's own data by scraping social media websites does not seem like a
good option either: It runs into strong opposition from Meta, which even
terminated the Facebook accounts of researchers who kept upstaging Meta's
woefully incomplete database of political
ads~\cite{EdelsonMcCoy2021a,Faife2021a,MerrillTobin2019,Roose2021a}. That is a
bit rich for a company that got started when Mark Zuckerberg scraped the Harvard
student directory for images~\cite{Madrigal2019} and that has been utilizing a
scraping service for many years itself~\cite{Newman2023}. Alas, the critical
lesson here is the need for regular third-party audits to ensure that
transparency or research data are, in fact, accurate and meaningful.
Conveniently, yearly audits \emph{are} included in the \DSA\ --- though that
does not help with research data disclosures.

Finally, \S\ref{sec:escape} illustrates that, when it comes to \AI, dataset,
model, and system cards cover the
basics~\cite{GebruMorgensternea2021,MitchellWuea2019,ProcopeCheemaea2022} and
event counts can help us understand some aspects of a deployed \AI. But finding
the unexpected ``features'' lurking in latent space does require hands-on
experimentation. Without it, chances of faulty classification seem too large.
But
There is a straight-forward legal remedy: Similarly to right to repair laws that
counter abusive and monopolistic industry practices, we should enshrine the
public’s right to probe algorithms that help approve or deny some form of
benefit in the widest sense of the word, especially when the benefits, unlike
posting to some online forum, impact a person’s livelihood or even freedom. Any
such algorithm should be legally required to have a public API endpoint that can
be queried independently of its integration into an organization’s IT systems.
To protect against abuse, the law might allow requiring user registration and,
possibly, a usage fee. But such a few would have to remain nominal, not to
exceed the marginal cost of providing extra server capacity. Such a law would be
stronger than existing legal mandates for financial audits of publicly traded
companies and also the DSA’s requirements for internet platforms because it
enables anyone to perform such an audit. But given the stakes for AI and the
regulatory capture in many existing industries, that is just the point.
Furthermore, facilitating scrutiny by all interested parties and not just some
anointed experts is not without precedent but rather an integral component of
patent law. While that law also grants a temporary monopoly, history provides
some measure for how excessive current right grants have become. When the US
enacted national patent and copyright protections in April and May 1790,
respectively, both right grants lasted 14 years!


\hidden{
requires hands-on experimenting


It can go well beyond
characteristics considered during development and (ideally) documented in
dataset, model, and system
cards~\cite{GebruMorgensternea2021,MitchellWuea2019,ProcopeCheemaea2022} and
surface unexpected features as well as unintended consequences. Other examples
for text-to-image systems include~\cite{BirhanePrabhuea2021,CarliniHayesea2023}.

Since for-profit corporations are unlikely to provide researchers with ready
access to the machine learning models on their own, this is a unique opportunity
for legal intervention: To require

simply counting events counting events just isn't enough. Nothing can replace hands-on experimentation

Since basic economics all but guarantee that algorithmic content moderation will
become the norm, it will be critical for social media moving forward to limit
policies and enforcement alike.


Basic economics ensure that algorithmic content moderation is here to stay.


As \S\ref{sec:escape} illustrates, hands-on experimentation can help surface
such conflicts.

More importantly, \S\ref{sec:escape} illustrates why hands-on experimenting with
a model is so critical: There simply are too many practical factors that cannot
possibly be represented even in the most detailed model card or system card.
\DALLE\ 2 actually has a system card. It also is fairly detailed. Yet it
conveniently (for OpenAI only) leaves unspecified the exact sourcing of \DALLE's
training data. Given the results discussed in \S\ref{sec:dalle} and
\S\ref{sec:escape}, I expect that OpenAI is avoiding such disclosure to minimize
reputational and legal risks. Ironically, the first lawsuits targeting
text-to-image \AI{}s are directed against Stability~AI for copyright violations.
But if lawmakers want to make a difference, they should impose a requirement for
unfettered model access for research purposes for any AI that makes decisions
about any benefit. Now that would be meaningful transparency!

More importantly, \S\ref{sec:escape} provides a compelling example for why
hands-on experimentation with \AI-based systems, the underlying machine learning
models, and their training data is critical: It can go well beyond
characteristics considered during development and (ideally) documented in
dataset, model, and system
cards~\cite{GebruMorgensternea2021,MitchellWuea2019,ProcopeCheemaea2022} and
surface unexpected features as well as unintended consequences. Other examples
for text-to-image systems include~\cite{BirhanePrabhuea2021,CarliniHayesea2023}.
While OpenAI's content policy and (past) threats of account termination do not
exactly encourage such experimentation, my experience shows that it still is
possible. Though the limited role \DALLE\ played in my life outside this
research also increased my risk tolerance. I would be far more reluctant to
perform similar experiments with social networks, even when using a dedicated
account. Notably, Meta


Meta disabled researchers' accounts \cite{EdelsonMcCoy2021,EdelsonMcCoy2021a}
to silence them \cite{Roose2021a}


blocked scraping \cite{Faife2021a,MerrillTobin2019}
paid scrapers \cite{Newman2023}
Zuckerberg scraped Harvard directory \cite{Madrigal2019}
bounty against scraping \cite{Gurfinkel2021}

Meta fails to institute promised changes
\cite{FaifeKerr2021,FaifeNg2021,Merrill2020,YinNg2021}

Social Science One data is useless, clear evidence of organizational capture
\cite{Hegelich2020,HegelichMarcoea2020}
Meta delivered faulty data  to Social Science One~\cite{Timberg2021},
ie without half of US data
need for legal changes   Brookings article
old problem~\cite{Ingram2022}

Dangerous to be too critical of Facebook ~\cite{SmithMatsakis2023}
Its own tool for tracking ads doesn't work \cite{Rosenberg2019}

but now we can pay the firm for blue checkmark \cite{Warzel2023}


GDPR \cite{Burgess2022}



has played in my life before certainly
also impacted my risk appetite.


\S\ref{sec:escape} proves that it is possible. Doing the same for social media's
content moderation is much harder


This paper has also shown some of the limits of transparency data. Partnerships
between industry and academia may provide institutional researchers with access
to more expansive data. But experiences with Meta sharing data through Social
Science One hosted by Harvard University also show the limitations:


The data not only had limited utility because of aggressive randomization for
privacy, but it did so when other data sources exposed the same metrics but
without the manipulation.





because it was coarsely


Institutional capture is a real concern, especially when high-ranking Meta
employees donated significant money to Harvard and more critical researchers are
terminated.



more detailed data access.


alternative are institutional partnerships that grant academ




and require careful work-arounds, it
still is possible. For social media, that



OpenAI's
expansive content policy probably is trying to prevent just that kind of
probing


as
others have done for text-to-image systems


unexpected behaviors as well as unintended consequences --- as
others have done as well~\cite{BirhanePrabhuea2021,CarliniHayesea2023}



A good half year after \DALLE's and Stable Diffusion's launches, OpenAI's
reticence to be fully transparent looks like smart strategy: Whereas Stability
\AI, the startup funding Stable Diffusion, has been sued by a stock photography
agency and by three artists for violating their intellectual property rights,
\DALLE\ has not (yet) been targeted similarly.

a smart public relations and
legal strategy:  may be paying off: While Stability \AI, the
startup funding Stable Diffusion, has been sued (separately) by a stock
photography agency and three artists~\cite{Butterick2023,Setty2023,Vincent2023},
OpenAI has



I may have contributed to that because after my first prompt's rejection I wrote
an impassioned defense of artistic content at times crossing boundaries. I had
given up hope that anyone would ever read the email but then three months later
I received a reply that albeit templated did acknowledge my email.


But the most important lesson from this section is the need to experiment with
machine learning models. Doing so for social networks is hard because it depends
on a steady supply of burner accounts and that is just the type of account
social media try to disable. Doing so for OpenAI still has risks, but was far
more practical. That's also why my criticisms of OpenAI should not be read as
pointing to failures unique to OpenAI. They simply are the result of close
scrutiny. If anything, OpenAI shares more than larger more established firms,
which gave me more material to work with.

If lawmakers want to make a difference, there should be a legal requirement that
any AI that decides about access to a resource \emph{must} be available for
independent experimenting. That sounds a bit like freedom to tinker but goes far
beyond the ability to learn about how things work.


already is close to
impossible because it will result in account closure.






In the long term, work like this is only possible, if researchers can access
platform data and algorithms. That means more than just aggregate transparency
disclosures. Here, legislation is urgently required: For one, legislation needs
to protect, for example, the right to scrape websites. Facebook, for example,
started out when Mark Zuckerberg scraped Harvard's directory for student images
and to this day pays firms to scrape other websites. Yet it also aggressively
fights any attempt to scrape their website and even has disabled researchers'
accounts for doing so.

Aggregating


}
