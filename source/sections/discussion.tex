\section{Discussion}
\label{sec:discussion}

Between the depth of the analytical case studies in \S\ref{sec:dalle} and
\S\ref{sec:tweet-da-fe} as well as the breadth of the social media census in
\S\ref{sec:census}, this paper presents a comprehensive first exploration of the
stochastic penal colony. It demonstrates that mostly punitive usage of \AI\ is
far from aberation or exceptional occurrance but rather the established, inhuman
norm when it comes to content moderation. While the biggest platforms, Google,
Meta, and Twitter, are almost completely in thrall of algorithmic moderation,
smaller platforms are slowly catching up as well. They certainly have the
punitive enforcement down pat.

More fundamentally, algorithmic content moderation poses a paradox: Basic
economics serve as driving force for social media to continue expanding its
reach. The (misleading) gloss of technological neutrality and infallibility only
strengthens it. As illustrated in \S\ref{sec:dalle} and \S\ref{sec:tweet-da-fe},
that is bound to result in less transparency and diminished
fairness~\cite{GorwaBinnsea2020}. Yet algorithmic content moderation also
provides a near perfect opportunity for abandoning the stochastic penal colony.
After all, when \AI\ detects and intercepts violative content before it is
posted, there is \emph{no} human harm and hence no need for punishment. By
nonetheless insisting on punishment, social media are at best cosplaying
Prudence Pingleton absolutely, positively, permanently punishing her daughter
Penny~\cite{Waters1988} or at worst making the IngSoc Party's ruthless
prosecution of thoughtcrimes become reality~\cite{Orwell1949}.


\subsection{Moderating the Public Interest}

Remarkably, the punitive overreach described in this paper is not social media's
only overreach. Several policy provisions are so expansive they conflict with
the public interest. Notably, \DALLE's prohibition against politics and
Pinterest's prohibition against criticism place significant a-priori constraints
on public debate and hence are fundamentally anti-democratic. \DALLE's
prohibition against health-related content may interfere with both science and
public health, which are key concerns for any state, whether democratic or not.
Finally, the prohibition against violence may discriminate against religious
beliefs. Other examples include prohibitions against extremism and violence
resulting in the suppression of evidence for large-scale human rights violations
and prohibitions against hate speech and sexual content being levelled against
the very people they are intended to protect, notably \V{LGBT}
folk~\cite{EidelmanLeeea2021,RahmanAlJaloudAlKhatibea2019,Wille2020}.

While I focused on \DALLE\ and the crucifixion in \S\ref{sec:escape}, all social
media surveyed in \S\ref{sec:census} besides Telegram have equivalent
prohibitions against violence and, with some exceptions for Tumblr and Twitter,
also against sexual content. At the same time, many foundational religious texts
contain passages that are blatantly, even excessively violative. Consider the
story of Abraham's nephew Lot, who hosted \YHWH's messengers at his home in
Sodom, protecting them from the mob. In return, the messengers saved him, his
wife, and youngest two daughters before \YHWH\ destroyed Sodom and Gomorrah. The
story appears in the Hebrew Bible and the Quran and hence has direct scriptural
significance for Judaism, Christianity, and Islam alike.

The relevant chapter in the Hebrew Bible, Genesis~19, includes all male
inhabitants of Sodom (save Lot) clamoring to gang rape \YHWH's messengers,
\YHWH\ destroying the cities of Sodom and Gomorrah, their population, and all
surrounding fields with a rain of fire and brimstone, Lot's wife being turned
into a salt pillar for copping a glance at \YHWH\ during Their orgy of
destruction, and Lot's surviving two virgin daughters successfully conspiring to
get daddy drunk so that he knocks them up. If that wasn't disturbing enough, the
destruction of Sodom and Gomorrah was coldly premeditated. In Genesis~18, \YHWH\
hesitates at first to share Their plans for the two cities with Abraham. When
They do, Abraham expresses grave concern about innocent people dying as a result
and ends up haggling \YHWH\ down to a maximum number of~9 instead of~49.

In short, scriptural precedent easily violates many contemporary content
prohibitions and illustrates how unrealistic and contrived blanket bans against,
amongst other things, sex and violence are. I do feel it incumbent on me to
acknowledge that my summary of Genesis~18 and~19 is severely skewed to make a
point. The same two chapters also describe Abraham and Lot exhibiting generous
hospitality towards strangers. Lot even offer his own daughter in the
messengers' stead to the mob outside his home. \YHWH\ doesn't appear solely as
vengeful God, simply calling out Sarah, Abraham's wife, for lying when she
denies having laughed at \YHWH's announcement that she would have a son, even
though she long ago went through menopause. \YHWH\ also cedes substantial ground
in Their discussion about collateral damage with Abraham. Finally, Genesis~18
switches repeatedly and seamlessly between Abraham and Sarah interacting with
three men and interacting with \YHWH, making this one of the earliest passages
suggesting that \YHWH\ and the Holy Trinity are one and the same.


\subsection{Expert Disinformation}

Ironically, when social media do consider the public interest, they use it to
justify their prohibitions against disinformation. In doing so, social media are
supported by a majority of \US\ respondents in at least one survey. Remarkably,
however, political affiliation has a significant impact on people's attitudes,
with Republicans ``consistently less willing than Democrats or independents to
remove posts or penalize the accounts that posted
them''~\cite{KozyrevaHerzogea2023}. That would suggest that, to a degree,
disinformation has become a political cudgel, or at least a shortcut to avoid
the hard work of actually informing and convincing others.

Furthermore, as the pandemic illustrated over and over again, so-called experts
haven't done so well with adjusting to new facts and disseminating correct
information~\cite{SinghMcNabea2021}. Notably, the \V{WHO} took years to overcome
the influence of outdated theories and acknowledge airborne transmission of
Covid-19~\cite{Chamary2021,GreenhalghOzbilginea2022,JimenezMarrea2022,Lewis2022,SirleafClark2021,SirleafClarkea2021}.
Its mask guidance wasn't much better~\cite{ChanLeungea2020} and inconsistent
with that of the \V{CDC}~\cite{Curtis2020}. Then again, the \V{CDC}'s mask
guidance was often flawed as well, with the agency changing its position for
what appear to be political or otherwise arbitrary reasons~\cite{Netburn2021}.
Furthermore, the \V{CDC}'s recommendations on other aspects, notably testing,
were also subpar~\cite{Flam2022,Ngo2022,Scott2022}. So do we really want
outsourced, barely trained content moderators, who are toiling away under
despiccable working conditions~\cite{Newton2019b,Newton2019}, to judge what is
legitimate information and what is illegitimate disinformation in at most 30
seconds per post?

Tragically, in all that hoopla about airborne transmission, masks, and
vaccinations, hospital leaders made another grave mistake over and over again.
They decided that Covid-19 patients had to be isolated and could not have any
visitors, with the result that many patients, including ones on respirators,
suffered and also died alone. While cutting down on visitors might make sense
for other patients, it made no sense for Covid-19 patients, especially not if
they were on respirators and hence facing grim odds for survival---a little
better than 2:1 in one study from the beginning of the
pandemic~\cite{AuldCaridiScheibleea2020}. In particular, visitors were no
infection risk to already infected patients. Patients, in turn, were breathing
through machines, in low pressure rooms, with sophisticated air filters, and
posed little infection risk to anyone. Meanwhile, caregivers were already
interacting with infected people all day and night, were wearing high-quality
face masks, and were the first to be vaccinated. In short, there was little to
justify this draconian, inhuman policy---besides the unchecked instinct to
isolate the diseased. The policy's impact has been devastating: Significantly
worse outcomes, including long-term impairments for those who survived and
miserable deaths for those who did not, as well as significant trauma for loved
ones and
caregivers~\cite{AndersonShawZar2020,AnnYiAzharea2021,Capozzo2020,DCouto2022,StrangBergstromea2020,WakamMontgomeryea2020}.


\subsection{Transparency Theater}
\label{sec:transparency-theater}

As if the systemic and broad overreach of their policies wasn't enough, social
media also fail at transparent and accountable governance. The lack of
transparency disclosures, with platforms addressing at most ten out of eighteen
criteria, is somewhat disappointing. Ambiguous policy terms and inconsistent
metrics noted in \S\ref{sec:trusting-twitter} and \S\ref{sec:census-results} are
certainly confusing and get in the way of meaningful comparisons between
platforms---as the British communications regulator recently noted as
well~\cite{HarlingHenesyea2023}. But arguably the most corrosive finding of my
social media census are the many data quality issues and inconsistencies for
platforms other than Google and Reddit presented in
\S\ref{sec:census-validation}. They directly undermine any confidence in the
accuracy of platforms' transparency reports and instead raise more questions
than they answer. That is strong evidence in support of Evelyn Douek's claim in
her December 2022 Harvard Law Review article that social media platforms do not
provide meaningful transparency but rather engage in transparency
theater~\cite{Douek2022}.

That does \emph{not} mean that current transparency reports are useless. In
\S\ref{sec:census}, I gave the example of TikTok improving its algorithmic
content moderation based on the reported fraction of videos flagged by \AI.
Another interesting vignette gleaned from transparency reports concerns Reddit.
In 2021, more than half of its account sanctions were for ban evasion, i.e.,
users not accepting bans from individual subreddits or the entire site and
creating new accounts to access those same fora again. If we assume that at
least some of these users are not only driven by a desire to troll or otherwise
disrupt old haunts, then Reddit suspending their new accounts as well represents
tremendous wasted potential for the platform. After all, these are users, who
care so deeply about Reddit that they go out of their way to access the site.
But instead of finding ways of turning them into net-positive contributors
again, Reddit's punitive account moderation is pushing them away. The large
percentage of sanctions for ban evasion also is troubling given that using
throw-away accounts for posting about sensitive subjects has been an integral
and extensively studied part of culture on
Reddit~\cite{AmmariSchoenebeckea2019,AndalibiHaimsonea2016,ChoudhuryDe2014,Leavitt2015,PavalanathanDeChoudhury2015,SchradingOvesdotterAlmea2015}

The problem is that, beyond such small vignettes, it is pretty difficult to gain
any systemic insight into the impact of social media, especially if we'd prefer
to do so in a scientifically acceptable, quantitative manner. Meta again leads
by significant failures. Consider its academic data sharing effort through
Harvard's Social Science One. It was supposed to facilitate unprecedented
empirical research on election integrity. In manifest reality, the first data
release was two years late, the data was marred by overly aggressive
randomization Meta does not apply to the same data shared with non-academic
partners, and the omission of about half of \US\ users was discovered only 1.5
years later, calling the results of ``dozens of papers'' into
question~\cite{Hegelich2020,HegelichMarcoea2020,Ingram2022,OHaraNelson2019,Timberg2021}.

Collecting one's own data by scraping social media websites isn't an acceptable
option either---at least not to Meta. The firm has repeatedly changed the code
of its website for the sole purpose of obstructing scraping (thereby also
breaking accessibility) and in 2021 even terminated the Facebook accounts of
academic researchers---even though they are at the same university as the firm's
chief \AI\ scientist---because they kept upstaging Meta's noticeably incomplete
database of political
ads~\cite{EdelsonMcCoy2021a,Faife2021a,MerrillTobin2019,Roose2021a}. That
certainly is a bit rich for a company that got started by Mark Zuckerberg
scraping the Harvard student directory for images~\cite{Madrigal2019} and that
has been utilizing a scraping service for many years into early 2023
itself~\cite{Newman2023}. Meta's many late, incomplete, and compromised
disclosures and its active obstruction of outside investigations are certainly
impressive. But the most striking aspect about them is that the same apparent
hostility towards transparency extends to other aspects of Meta's business too.
Notably, in addition to at least five cases lasting up to two years where it
inadvertently misreported
metrics~\cite{BruellPatel2020,Hutchinson2016,Hutchinson2016a,Hutchinson2016b,Hutchinson2017,VranicaMarshall2016},
the firm has also been manipulating ad impressions---which are the most basic
advertising metric according to the firm's own \V{SEC} filings---and thereby has
been misleading customers, investors, and regulators alike~\cite{Grimm2022a}.


\let\Oldthefootnote\thefootnote
\renewcommand*{\thefootnote}{\ding{164}}
\subsection{``The Great Question before us is: Can we Change? In Time?''%
\texorpdfstring{\>\!\protect\footnotemark}{}}
\footnotetext{%
    From the opening monologue of Tony Kushner's \emph{Angels in
    America, part~2, Perestroika}~\cite{Kushner1993}}
\let\thefootnote\Oldthefootnote

\subsubsection{Regulating Content Moderation}
\label{sec:regulating:moderation}

So what should we do about the many failures of social media content moderation?
Since I was just bemoaning Meta's apparent unwillingness to be even minimally
accountable, I'm going to continue with that thrust. The United States has been
missing in action when it comes to meaningful regulation of technology companies
in general and social media in particular. The country's inaction has allowed
internet platforms to ride roughshod over people who value privacy and civility
and to saddle especially African, Asian, and South American countries with
oppressive negative externalities. The two genocides Meta helped bring about
(see \S\ref{sec:census-limits}) are but two particularly extreme examples
amongst
many~\cite{DwoskinNewmyerea2021,ElliottChristopherea2021a,FrenkelAlba2021,Iyengar2021,MacSilverman2020,Morris2021,Newton2021,Satariano2021,Scott2021,SilvermanMacea2020,Simonite2021,WongEllisPetersen2021,WongErnst2021,WongHarding2021,ZahrzewskiDeVynckea2021}.
Not surprisingly, that is having a negative impact on the United States'
international reputation~\cite{GeltzerGhosh2018}. Relying on the Brussels
effect~\cite{Bradford2020} for solving one's homegrown problems isn't a
particularly effective political strategy. Alas, given the extreme political
polarization in Washington, it seems like that's as good as the \US\ gets.

Nonetheless, it's not that hard to draft legislation that codifies a few ground
rules, while also requiring increased transparency to help drive public policy.
Notably, such legislation should impose the following requirements:
\begin{enumerate}
    \item Localize platform, policies, help pages, etc before entering
        a new foreign market; \label{itm:law:localized}
    \item Assemble trust and safety team with native speakers for all local
        languages before entering market; \label{itm:law:team}
    \item Notify users of all content moderation actions, with exception of spam;
        % "deceptive high-volume commercial content"
    \item Track who flags what content or users for what reasons and with what
        outcomes;
    \item Report attendant summary statistics at least once a year.
    \item Provide granular logs to researchers approved by National Science
        Foundation in the \US\ or local equivalent. \label{itm:law:research}
\end{enumerate}
For each country with large platform presence, i.e., where at least 10\% of the
local population are monthly active users, the legislation should also require:
\begin{enumerate}[resume]
    \item Provide a meaningful appeals process for content moderation decisions;
        \label{itm:law:appeals}
    \item Include corresponding statistics in at least yearly transparency
        reports;
    \item Demonstrate that data collection and transparency reports have passed
        review by an independent auditor;
    \item Conduct a yearly review of systemic risks, develop and implement a
        plan for risk mitigation, and review implementation after the fact. Of
        particular interest are:
        \begin{itemize}
            \item Illegal content and activities;
            \item Fundamental rights incl.\ freedom of expression, right to
                privacy, to non-discrimination, and so on;
            \item Democratic process and election integrity;
            \item Public health and emotional well-being.
        \end{itemize} \label{itm:risk:review}
    \item Publish risk analysis, mitigation plan, and implementation review.
        \label{itm:law:publicrisk}
\end{enumerate}
Most of the above obligations are based on the Digital Services Act or
\DSA~\cite{EuropeanParliamentAndCouncil2022}. It provides a convenient and
concrete starting point, including for the actual language of the statute,
thanks to a 2011 decision by the European Commission that allows such
reuse~\cite{EuropeanCommission2011}. I included
obligations~\ref{itm:law:localized} and~\ref{itm:law:team} while also making
obligations~\ref{itm:law:appeals} through~\ref{itm:law:publicrisk} contingent on
a platform's per-country presence to counteract social media's neo-imperialist
treatment of countries outside of North America and Europe. Even if a platform
starts out with a perfunctory effort only, the hope is that having to go through
the motions of localizing a platform's texts and forming a trust and safety team
again and again sensitizes the organization towards cross-cultural differences
and encourages employees to develop cultural competency for cultures other than
their own.

Obligation~\ref{itm:law:research} is a more practical variation on the
equivalent provisions of the \DSA~\cite{Vermeulen2022}. The latter relies on
newly appointed per-country regulators, the \emph{digital services
coordinators}, for most oversight functions including the vetting of research
projects and hosting of sensitive platform data~\cite{Jaursch2022,Jaursch2022a}.
Having to ramp up all these functions before February 2024, most \EU\ members
are turning to existing regulatory agencies to also take on \DSA\ monitoring and
enforcement. But those agencies lack the necessary expertise when it comes to
data science and academic research~\cite{Jaursch2023}. Worse, much depends on
the Irish digital services coordinator, since it is responsible for the European
subsidiaries of Meta, Pinterest, TikTok, Twitter, and YouTube and hence also the
only regulator that can directly demand acccess to their
data~\cite{Albert2022,Jaursch2023}. At the same time, Irish oversight of these
same firms under the \V{GDPR} has been decidedly ho-hum, with several
high-profile complaints not moving forward over years~\cite{Burgess2022}. In
contrast, my proposal delegates to an organization that has over 70~years of
experience with vetting research projects and has the processes in place to do
so in a timely manner and at scale.

Probably the biggest weakness of the \DSA\ is its dependence on national
regulators. Another shortcoming, at least from the perspective of this
independent investigator, is the \DSA's mandate that researchers are affiliated
with a research institution. At the same time, the \DSA's treatment of privacy
concerns is refreshing: It requires that research proposals identify any privacy
risks and include appropriate mitigations. Yet it does not allow social media to
use privacy as an excuse for not providing data. Next, experience with national
regulators insufficiently enforcing \V{GDPR} did result in the European
Commission taking on \DSA\ oversight functions in addition to national
regulators. Its stated priorities are ramping up its own oversight processes,
determining supervisory fees, and preparing independent audits of very large
platforms~\cite{Bertuzzi2022,TenThije2022}. I am glad to note the third priority
because it does hold the promise of higher quality transparency disclosures.
Once the Irish digital services coordinator has figured out how to vet research
projects and convince platforms of handing over the requested data, these
provisions may just lead to more high-quality empirical research on social media
by more teams. It's a nice touch that the \DSA\ explicitly requires open access
publication of results.

Finally, we make sure that even platforms with large reserves of cash---e.g.,
Meta had \$40.74 billion of the stuff on December 31,
2022~\cite{MetaPlatforms2023}---don't treat fines as the cost of doing (shady)
business. Hence we impose fines amounting to 5--10\% of the previous year's
global revenue for each case of non-compliance. I'm pretty sure that will
convince even Mr Zuckerberg that accountability has tremendous value. And if it
doesn't, we keep doubling fines until he does.


\subsubsection{Moderating Regulation}
\label{sec:moderating:regulation}

Except I am writing this paper to bemoan the very excessiveness of content
moderation, in policy and implementation, and to push back against punitive
interventions that dehumanize in the name of platform safety. So instead of
advocating a similarly punitive approach towards internet platforms, I'd much
rather extend the same humane courtesy towards social media, including even
Meta. I readily admit that this doesn't come easy, since I consider much of that
firm's wealth ill-gotten and tained by blood. Ethics sure are troublesome.

Except that Douek's Harvard Law Review article~\cite{Douek2022} raises a good
number of substantive issues with the obligations I just outlined. The starting
point for Douek's article is equivalent to my previous point, namely that
content moderation bureaucracies are people too. And these people are
increasingly moderating based not on content but behavior, often cooperating
with governments, e.g., when it comes to \CSAM, relying on outside organizations
serving as trusted flaggers, and trying out means for devolving control to
smaller communities. The article further attests systemic short-comings due to
unrealistic expectations about perfectability, too much focus on false positives
to the (almost) exclusion of false negatives, likewise too much focus on
individual cases instead of systemic failures, including when it comes to
transparency, and procedural justice privileging the lucky few---an important
insight about equity in content moderation. In short, there's much to like in
the article. I am particularly appreciative of its focus on systemic forces. As
discussed in \S\ref{sec:transparency-theater} above, my own findings of
pervasive data quality issues support Douek's contention that transparency
disclosures have limited utility.

However, the article also suffers from two significant problems. First, the
article's basic conceit, that law makers' and academics' misconceptions about
content moderation amount to a ``standard picture,'' seems rather
contrived---especially coming from a professor at Stanford's law school. The
hierarchy of census criteria in \S\ref{sec:census} predates my reading of the
article, yet I had no difficulty integrating those of Douek's observations I had
not considered before. Likewise, the above sketch of regulatory requirements is
purposefully minimal and nonetheless can easily accommodate, for example, bulk
reviews and appeals to become more equitable. Meanwhile, the \DSA's risk
assessments are in line with Douek's recommendations, as her article
acknowledges. In other words, the chasm between what Douek calls the standard
picture on one side and manifest reality of content moderation on the other side
may not be nearly as far or deep as Douek's article makes it out to be. Second,
the article's discounting of speech as plentiful and hence ``not so special''
isn't just ``almost sacrilegious.'' It runs real danger of depriving platform
users of their voices, even though they use these platforms for that same
reason---to have a voice. That effectively makes this argument deeply cynical.
It would be a real shame if establishing the humanity of moderators resulted in
treating the moderated as more or less fungible and hence denying their
humanity. I'd much prefer to avoid such false zero sum games!


\subsubsection{Contextualizing Transparency}

As it turns out, there is a simple unifying explanation that accounts for shoddy
accountability and befuddled observers alike. It may not amount to much more
than an informed guess at this point. But it is a compelling guess nonetheless
and hence well worth exploring in future work. As I was reading and re-reading
transparency disclosures and becoming familiar with their structure and lingo,
they never ceased to make for frustrating reading. I eventually realized that
the primary cause was their deeply reductive presentation. Each metric exists in
isolation, with little motivation and hardly any helpful context, and hence
comes close to being an abstract cipher. Treating that semantic wasteland as
opportunity, marketing and public relations folk then added their unique kind of
spin, adding introductory notes that celebrate the platform's progress---towards
what exactly usually remains a mystery---and manipulating units and histogram
buckets for appearances and not insight. Hence we find Twitter throwing 0~views
into the same bin as 1—9 views when reporting reach (see
\S\ref{sec:trusting-twitter}), Meta not disclosing \NCMEC\ reports or unique
pieces when reporting \CSAM (see \S\ref{sec:census-validation}), and most every
platform not relating content sanctions to account sanctions.

That is not to say that platforms set out to spin transparency reports to their
marketing or public relations advantage. That's a secondary effect, with those
professionals doing what they are trained to be doing. At the same time, social
media bear more than a little responsibility for this reductionist view taking
hold. When I started working for Facebook during the summer of 2018 and went
through ``bootcamp,'' their orientation program for newly hired engineers,
several long-term employees made it a point of pride that a small number of
engineers were supporting a huge number of users. In fact, Andrew ``Boz''
Bosworth made just that point in the 2009 blog post announcing the creation of
bootcamp~\cite{Bosworth2009}. While that has changed somewhat in recent years,
this disparity used to extend to the overall number of employees as well. It was
markedly lower than that of technology industry peers. Notably, by the end of
2018, Facebook had about 36,000 employees, while Alphabet had 99,000, Apple
132,000, and Amazon
648,000~\cite{MacrotrendsAlphabet,MacrotrendsAmazon,MacrotrendsApple,MacrotrendsMeta}.
More generally, by trying to monopolize our attention and then bundle that
attention for sale to the highest ad bidder, social media have been reducing
their users to fungible ad impressions. That of course is dehumanizing as well
and suggests that these platforms are \emph{asocial media} first and foremost.

If we believe this explanation for the reductive presentation of transparency
disclosures, then the obvious solution is to re-establish context and re-surface
dependencies. A reasonable approach for getting us started are user story
mappings~\cite{PattonEconomy2014}, only here we track content and users through
the content moderation process. Discounting spam and behavioral triggers, a
simple first template for creating such a content moderation mapping, with
variables shown italicized between angular brackets, might read:
\begin{quote}
In \var{country}, users post \var{content} at \var{volume}. Content violating
\var{prohibition} has \var{prevalence}. It is flagged by \var{flagger} and
results in \var{sanction} for \var{content-or-user}. Users appeal \var{fraction}
of these sanctions, which are reviewed by \var{moderator} and result in
\var{outcome}.
\end{quote}
Since the template represents a process, that also suggests some kind of flow
diagram for (literally) establishing the big picture and thereby providing a
more meaningful overview over the impact of content moderation on content and
users. Since the diagram needs to accommodate eleven variables, it will
necessarily be fairly complex. But it would also help relate statistics that
currently exist only in isolation. Since the diagram needs to accommodate
categorical as well as numerical variables, neither Sankey nor alluvial diagrams
are a clear fit. A hybrid flow diagram should do nicely.

At the same time, not all context is process oriented. Notably, in
\S\ref{sec:census}, I touched on several metrics for characterizing content,
including user posts, pieces of content attached to posts, unique pieces of
content, and for \CSAM, reports to \NCMEC. Since they all capture a different
aspect of the same content, none of them is a-priori more representative and
they should \emph{all} be disclosed \emph{in relation to each other}. That
suggests a parallel sets diagram as suitable visual representation. Another
cause for significant frustration are unexplained and surprising changes in the
data reporting period over reporting period. Addressing them will require
explanations in prose, ideally right next to the data and never in a separate
document. Their preparation may very well involve additional data collection and
analysis, which suggests that at least one report author should be a data
scientist or software engineer with suitable background (e.g., PhD).


\subsubsection{Subverting the Penal Colony}

Counteracting excessively punitive interventions is hard. Doing so with laws and
regulations implies the threat of fines or prison upon non-compliance. That
makes them a non-starter. As I said in \S\ref{sec:moderating:regulation},
answering one punitive excess by starting another is more cognitive dissonance
than I'm willing to entertain in this paper. The next best alternative, trying
to convince stakeholders that current practices are excessively punitive, also
seems destined to fail. After all, the United States arbitrarily brutalized
Black, Brown, and Poor people for almost forty years before most people even
noticed, with the prison population growing by 500\% before hitting a peak in
2009. It has been declining since, albeit slowly at 0.5\%--3\% per year, but it
took a deadly pandemic to make a real 14\% difference. Alas, that was 2020 and
we are back to a small yearly decline~\cite{Nellis2023}. That makes it even more
disappointing that one of the architects of this adominable system, Joe Biden,
has not devoted all of his not insubstantial powers as \US\ president to razing
the carceral state and sowing it with
salt~\cite{Reinhart2022,StolbergHerndon2019}.

If razing the stochastic penal colony is not a viable option, then maybe a more
indirect approach can help subvert the stochastic penal colony. The best current
candidate for such an indirect approach appears to be the devolution of content
moderation to smaller communities or groups. Several of the major platforms have
at least dabbled in devolution~\cite{Goodman2021,Hern2021,Instagram2021} and
Reddit has been delegating to volunteer moderators for its subreddits or groups
for years. But the current posterchild for devolution is Mastodon's federation
of independently operated and moderated servers. Impressively, after Elon Musk's
takeover of Twitter, the so-called fediverse absorbed over two million new users
in less than two months---and lost over a million again by early January---while
remaining largely operational~\cite{Hoover2023,Peters2022}. Anecdotally, content
moderation fared worse, despite earlier success
stories~\cite{Cathcart2023,CopiaInstitute2021,Hall2023,Rozenshtein2022}.
Unfortunately, Mastodon's creator and current \V{CEO} of the benefit corporation
driving Mastodon's engineering effort seems to have little insight into the
strengths and weaknesses of the fediverse when it comes to content
moderation---or at least, he couldn't articulate them during a lengthy recent
interview~\cite{Patel2023}.

When I turn towards computer science and sociology for additional insight, an
initial review of the literature also points towards decided mixed experiences.
Notably, like other social media, the fediverse suffers from ubiquitous and
highly viral toxic content~\cite{BinZiaRamanea2022}. It also faces significant
content, user, and infrastructure pressures towards
centralization~\cite{RamanJoglekarea2019}. In contrast to research on the
fediverse, there is no shortage of papers covering
Reddit~\cite{ProferesJonesea2021}. Likewise, there seems to be no shortage of
toxic, harassing, hateful, and criminal content on
Reddit~\cite{KwonShao2021,Massanari2017,RiegerKumpelea2021}. Communities thrive
when moderators use their subjectivity for the benefit of a subreddit's members
and they, in turn, work with moderators towards the same
goal~\cite{Gibson2019,SeeringWangea2019}. That, however, is very labor-intensive
and frequently leads to burnout~\cite{SchopkeGonzalezAtrejaea2022}. Despite
these challenges, Reddit also hosts several successful communities where members
support each other on parenting, abuse, and mental health; as already mentioned
in \S\ref{sec:transparency-theater}, the use of burner accounts is pervasive but
does not detract from the
community~\cite{AmmariSchoenebeckea2019,AndalibiHaimsonea2016,ChoudhuryDe2014,Leavitt2015,PavalanathanDeChoudhury2015,SchradingOvesdotterAlmea2015}.

It would seem that devolution, by itself, does not suffice for making a platform
safer or less toxic. In a way that isn't too surprising. Humans are humans, no
matter the social media platform, and hence go for the content that promises a
more intense emotional response. Existing work does a good job identifying
design elements and affordances that encourage virality. That's far less the
case when it comes to identifying and cataloging antiviral design elements,
which would go a long way towards building new and safer social media. Finally,
devolution still is a significant win when it comes to the stochastic penal
colony because devolution, by definition, limits the worst-case impact of
content moderation decisions. A ban on Facebook implies a ban on Instagram and
Messenger too, with no way of regaining access. A ban on a server in the
fediverse implies losing direct access to the community but most of the same
content and people are also accessible from another server in the fediverse.
However, that does not mean that I am endorsing routine account bans on
Mastodon. They still are excessive!


\subsubsection{Rejecting (Necessarily False) Promises of AI Safety}

When it comes to \AI, previous work on dataset, model, and system cards already
provide a reasonable framework for practicing more holistic transparency and
covering relevant quantitative as well as qualitative characteristics of
algorithmic
interventions~\cite{GebruMorgensternea2021,MitchellWuea2019,ProcopeCheemaea2022}.
By definition, however, such cards cannot disclose what nobody has considered or
encountered yet---including due to personal or institutional blindspots or
groupthink. That's exactly the reason why hands-on probing by outsiders is so
important and, as \S\ref{sec:escape} and previous work have
demonstrated~\cite{BirhanePrabhuea2021,CarliniHayesea2023}, may just surface all
sorts of gremlins, orks, and cenobites that have been lurking in latent space.
As the lead for OpenAI's alignment team put it in a recent
interview~\cite{Heaven2023}: ``I think it's very difficult to really anticipate
what the real safety problems are going to be with these systems once you've
deployed them.'' In other words, hands-on engagement with production systems is
the only way to make them safe.

Furthermore, that's exactly the reason why we should always reject ``disclosing
the inner workings of our safety mechanism renders them ineffective'' as
justification for non-disclosure. Chances are that the people making this
argument---OpenAI's chief scientist and co-founder Ilya Sutskever only was the
most recent~\cite{Vincent2023a}---never even evaluated the safety of their
systems, haven't completed evaluation and mitigation, or have too little or too
much confidence in their work. In other words, if someone make this argument,
safety researchers should start paying particular attention to that person's
organization and \AI\ deployments. Or as Cory Doctorow succinctly put it in the
title of a blog post about the same fallacy: ``Como Is
Infosec''~\cite{Doctorow2022}. While I am not a fan of the syllabic abbreviation
``como,'' Doctorow knows his intended audience. The Trust \& Safety Professional
Association emerged out of the \V{COMO} Summit in 2018. (The people running that
association are either completely clueless or have a wicked sense of humor:
Their yearly shindig is called ``TrustCon.'')

Since there is no meaningful regulation requiring transparency about \AI-based
interventions and so many basic legal questions about training data and \AI\
outputs remain unsettled, legal uncertainty and transparency may interact in
suprising and counter-productive ways. For \DALLE~2, OpenAI published a detailed
scientific paper~\cite{RameshDhariwalea2022} and released a system card before
their April 2022 preview release~\cite{MishkinAhmad2022}. While the paper is
sufficiently detailed for others to recreate OpenAI's implementation, the firm
never released the model and its weights. It also has been consistently and
completely silent on the exact composition of its training data. By contrast,
Stability AI and its collaborators have disclosed the training data, the source
code for running the model, and the actual model parameters for Stable
Diffusion. The latter clearly is the more transparent and communitarian effort.
But it is Stability AI that is at the receiving end of highly visible copyright
lawsuits by a stock photography agency and three
artists~\cite{Butterick2023,Setty2023}, whereas OpenAI appears to have avoided
them (so far). As I already observed at the end of \S\ref{sec:dalle}, OpenAI has
stellar public relations and legal talent!


\subsubsection{Fixing the AI IP Regime}

According to a profile in New York Magazine, Emily M Bender made a rule for
herself: ``I'm not going to converse with people who won't posit my humanity as
an axiom in the conversation''~\cite{Weil2023}. I concur. And add: At the end of
the day, \AI\ certainly is artificial but it has little to do with human
intelligence. It fundamentally is software, software that runs on a computer.
Like all other software humans have created so far, \AI\ is buggy. The kinds of
bugs may have changed. And as demonstrated in \S\ref{sec:escape}, effective
attacks on this type of software may have changed. But the fundamental truth
remains: Humans are so much more than software. We rule supreme. If software
turns into a threat to even a single human, there is a very simple solution that
also scales: Switch off the damn computer and wipe its storage!

When well over a thousand industry leaders and experts call for a moratorium on
the most advanced \AI\ techniques~\cite{MetzSchmidt2023}, far more than a single
human are threatened. Though maybe not quite in the way the letter describes the
threats~\cite{KapoorNarayanan2023}. Nonetheless, switching off the damn
computers and wiping their storage remains a viable option---or destorying ``a
rogue datacenter by airstrike,'' if you are so inclined~\cite{Yudkowsky2023}.
Since that option is a bit destructive, I am instead going to sketch the
outlines of a new intellectual property regime for \AI. The primary goals for
the new \V{IP} regime are to hold those who are pushing the state-of-the-art in
\AI\ accountable for their deployments and to ensure that everyone benefits from
\AI. As before, I am particularly concerned with transparency not because it
suffices, but because it is critical for facilitating sound policy decisions in
the future. The new \AI\ \V{IP} or \AIP\ regime is inspired by (1) OpenAI's
charter, (2) \US\ copyright provisions for works created by the government, and
(3) the internationally accepted foundations of patent law.

While OpenAI's charter nominally concerns itself with \V{AGI} or artificial
\emph{general} intelligence---``highly autonomous systems that outperform humans
at most economically valuable work''---the same arguments apply to contemporary
``lesser'' \AI as well. The firm states that \AI\ should be ``used for the
benefit of all.'' Furthermore, ``our primary fiduciary duty is to humanity.'' I
couldn't agree more. But I'm not willing to take OpenAI's (or any other
organization's) word for it. \US\ copyright law points to just the right means
for enforcing that: Almost all works created by the government immediately enter
the public domain and are reusable by everyone. So \AIP\ does the same for
\emph{all} \AI\ systems including machine learning models and enters them into
the public domain by default, no matter under what circumstances they were
developed. In addition to ensuring broad access to the technology, public domain
for \AI\ also nicely accounts for recent models incorporating much material of
dubious provenance, which is plainly exploitative.

But with the public domain making it nearly impossible to keep a proprietary
\AI\ actually proprietary, most private investment into \AI\ would likely wither
away. So \AIP\ takes a cue from patent law, which grants a temporary, legal
monopoly to inventors---as long as they fully disclose the invention upfront.
Hence, \AIP\ grants people and organizations a temporary, legal monopoly on an
\AI\ if they follow certain rules---more on them in a moment. Given recent rapid
progress in \AI, setting the monopoly length to the same 20 years as for patents
seems excessive. Instead, I am proposing a duration of three to five years. To
gain this temporary protected status, an \AI\ operator files an intent notice
including a preliminary system card with the national \AIP\ office before
starting to train the model. The expectation is that intent notices are almost
always approved.

After completion of training, the operator files the finalized systems card,
which is published upon approval of the protected status on the agency's
website, as well as the actual source code and model weights, which are
embargoed until the end of the protected period. The primary rejection reason is
that an almost identical \AI\ has already been registered. By collecting the
actual artifacts, the \AIP\ website becomes a national if not global
clearinghouse for \AI\ and ensures that, with a couple of years lag, everyone
gains access to the same advanced technology. While the \AI\ has protected
status, the operator must file yearly updates to system card, code, and
models---or indicate that none are necessary. They must also provide an \V{API}
that gives \AI\ researchers direct access to the \AI\ for experimentation.
Operators are encouraged to make \V{API} access free, but must not charge more
than the marginal cost of the compute capacity they consume through the \V{API}.
Researchers must follow a responsible disclosure process, giving operators an
opportunity to remediate any flaws before they are made public. Unlike
\S\ref{sec:regulating:moderation}, the \AIP\ agency performs basic vetting of
researchers to reliably establish their identity. Meanwhile operators have no
vetting or veto rights. They should focus on having a working research endpoint
and collecting accurate data for their transparency disclosures.

In my mind, the most interesting aspect of this proposal is that, what might
appear to be a fairly radical re-imagining of intellectual property rights for
\AI, really is little more than the recombination of proven elements from
existing \V{IP} laws. Granted, OpenAI's charter makes for a more communitarian
starting point than common capitalist practice, including by OpenAI. It may just
be a remnant of Hippier days for the technology industry, when OpenAI was still
open and when Google and Microsoft still had ethical \AI\
teams~\cite{GrantBassea2021,GrantEidelson2022,Newton2023}. Since such corporate
statements about social responsibility have zero follow-through at best and are
followed by contrary actions at
worst~\cite{BebchukKastielea2022,RaghunandanRajgopal2022,Useem2020}, the charter
can only inspire, never replace regulation.

The proposed regulation, \AIP, directly and only targets \AI, making for a
precise intervention that minimizes the potential for unintended consequences.
It is based on proven elements from existing \V{IP} law. It also balances the
expansive application of public domain with a purposefully lower barrier to
protected status than for patents. As co-inventor on five patents, I can attest
to a slow moving process that critically depends on the patent attorney being
able to translate the substance of a technical innovation into an archaic blend
between technical and legal writing unique to patents. But even the best written
patent disclosure faces uncertain outcomes and most likely changes as it goes
through the review process. In contrast, applications for \AIP's protected
status do not require legal representation, ask for materials that already exist
(code and model) or should if following best practices (system card), and are
subject to much simpler review, which largely can be automated. All that makes
\AIP\ a rather conservative proposal for codifying socially responsible economic
ground rules for \AI.
