\section{Discussion}
\label{sec:discussion}

Between the depth of the analytical case studies in \S\ref{sec:dalle} and
\S\ref{sec:tweet-da-fe} as well as the breadth of the social media census in
\S\ref{sec:census}, this paper presents a comprehensive first exploration of the
stochastic penal colony. It demonstrates that mostly punitive usage of \AI\ is
far from aberation or exceptional occurrance but rather the established, inhuman
norm when it comes to content moderation. While the biggest platforms, Google,
Meta, and Twitter, are almost completely in thrall of algorithmic moderation,
smaller platforms are slowly catching up as well. They certainly have the
punitive enforcement down pat.

More fundamentally, algorithmic content moderation poses a paradox: Basic
economics serve as driving force for social media to continue expanding its
reach. The (misleading) gloss of technological neutrality and infallibility only
strengthens it. As illustrated in \S\ref{sec:dalle} and \S\ref{sec:tweet-da-fe},
that is bound to result in less transparency and diminished
fairness~\cite{GorwaBinnsea2020}. Yet algorithmic content moderation also
provides a near perfect opportunity for abandoning the stochastic penal colony.
After all, when \AI\ detects and intercepts violative content before it is
posted, there is \emph{no} human harm and hence no need for punishment. By
nonetheless insisting on punishment, social media are at best cosplaying
Prudence Pingleton absolutely, positively, permanently punishing her daughter
Penny~\cite{Waters1988} or at worst making the IngSoc Party's ruthless
prosecution of thoughtcrimes become reality~\cite{Orwell1949}.


\subsection{Moderating the Public Interest}

Remarkably, the punitive overreach described in this paper is not social media's
only overreach. Several policy provisions are so expansive they conflict with
the public interest. Notably, \DALLE's prohibition against politics and
Pinterest's prohibition against criticism place significant a-priori constraints
on public debate and hence are fundamentally anti-democratic. \DALLE's
prohibition against health-related content may interfere with both science and
public health, which are key concerns for any state, whether democratic or not.
Finally, the prohibition against violence may discriminate against religious
beliefs. Other examples include prohibitions against extremism and violence
resulting in the suppression of evidence for large-scale human rights violations
and prohibitions against hate speech and sexual content being levelled against
the very people they are intended to protect, notably \LGBT\
folk~\cite{EidelmanLeeea2021,RahmanAlJaloudAlKhatibea2019,Wille2020}.

While I focused on \DALLE\ and the crucifixion in \S\ref{sec:escape}, all social
media surveyed in \S\ref{sec:census} besides Telegram have equivalent
prohibitions against violence and, with some exceptions for Tumblr and Twitter,
also against sexual content. At the same time, many foundational religious texts
contain passages that are blatantly, even excessively violative. Consider the
story of Abraham's nephew Lot, who hosted \YHWH's messengers at his home in
Sodom, protecting them from the mob. In return, the messengers saved him, his
wife, and youngest two daughters before \YHWH\ destroyed Sodom and Gomorrah. The
story appears in the Hebrew Bible and the Quran and hence has direct scriptural
significance for Judaism, Christianity, and Islam alike.

The relevant chapter in the Hebrew Bible, Genesis~19, includes all male
inhabitants of Sodom (save Lot) clamoring to gang rape \YHWH's messengers,
\YHWH\ destroying the cities of Sodom and Gomorrah, their population, and all
surrounding fields with a rain of fire and brimstone, Lot's wife being turned
into a salt pillar for copping a glance at \YHWH\ during Their orgy of
destruction, and Lot's surviving two virgin daughters successfully conspiring to
get daddy drunk so that he knocks them up. If that wasn't disturbing enough, the
destruction of Sodom and Gomorrah was coldly premeditated. In Genesis~18, \YHWH\
hesitates at first to share Their plans for the two cities with Abraham. When
They do, Abraham expresses grave concern about innocent people dying as a result
and ends up haggling \YHWH\ down to a maximum number of~9 instead of~49.

In short, scriptural precedent easily violates many contemporary content
prohibitions and illustrates how unrealistic and contrived blanket bans against,
amongst other things, sex and violence are. I do feel it incumbent on me to
acknowledge that my summary of Genesis~18 and~19 is severely skewed to make a
point. The same two chapters also describe Abraham and Lot exhibiting generous
hospitality towards strangers. Lot even offer his own daughter in the
messengers' stead to the mob outside his home. \YHWH\ doesn't appear solely as
vengeful God, simply calling out Sarah, Abraham's wife, for lying when she
denies having laughed at \YHWH's announcement that she would have a son, even
though she long ago went through menopause. \YHWH\ also cedes substantial ground
in Their discussion about collateral damage with Abraham. Finally, Genesis~18
switches repeatedly and seamlessly between Abraham and Sarah interacting with
three men and interacting with \YHWH, making this one of the earliest passages
that can be read as an appearance of the Holy Trinity.


\subsection{Expert Disinformation}

Ironically, when social media do consider the public interest, they use it to
justify their prohibitions against disinformation. In doing so, social media are
supported by a majority of \US\ respondents in at least one survey. Remarkably,
however, political affiliation has a significant impact on people's attitudes,
with Republicans ``consistently less willing than Democrats or independents to
remove posts or penalize the accounts that posted
them''~\cite{KozyrevaHerzogea2023}. That would suggest that, to a degree,
disinformation has become a political cudgel, or at least a shortcut to avoid
the hard work of actually informing and convincing others.

Furthermore, as the pandemic illustrated over and over again, so-called experts
haven't done so well with adjusting to new facts and disseminating correct
information~\cite{SinghMcNabea2021}. Notably, the \WHO\ took years to overcome
the influence of outdated theories and acknowledge airborne transmission of
Covid-19~\cite{Chamary2021,GreenhalghOzbilginea2022,JimenezMarrea2022,Lewis2022,SirleafClark2021,SirleafClarkea2021}.
Its mask guidance wasn't much better~\cite{ChanLeungea2020} and inconsistent
with that of the \CDC~\cite{Curtis2020}. Then again, the \CDC's mask guidance
was often flawed as well, with the agency changing its position for what appear
to be political or otherwise arbitrary reasons~\cite{Netburn2021}. Furthermore,
the \CDC's recommendations on other aspects, notably testing, were also
subpar~\cite{Flam2022,Ngo2022,Scott2022}. So do we really want outsourced,
barely trained content moderators, who are toiling away under despiccable
working conditions~\cite{Newton2019b,Newton2019}, to judge what is legitimate
information and what is illegitimate disinformation in at most 30 seconds per
post?

Tragically, in all that hoopla about airborne transmission, masks, and
vaccinations, another, grave failure of medical experts turned into draconian,
inhuman policy---hospitals prohibiting visitors even for people on respirators,
who often ended up dying alone---took hold across many hospitals without
meaningful public debate or pushback. While the prohibition against visitors may
indeed eliminate potential disease carriers who otherwise might infect
vulnerable patients in general, that argument does not hold for Covid-19
patients on ventilators. They are already infected and face significant odds of
death---almost 1:2 in one study from the beginning of the
pandemic~\cite{AuldCaridiScheibleea2020}. Yet they breathe through a machine, in
a low pressure room with sophisticated air filters. Meanwhile, caregivers wear
high-quality masks and were the first to be vaccinated. In short, the infection
risk posed by patients on ventilators to visitors and visitors to personnell is
low. And even if it was higher, a person's deathbed requires more respect and
hence calls for different trade-offs. Back in manifest reality, most hospitals
imposed a draconian, inhuman policy that leads to worse outcomes, long-term
impairments, and possibly terrible deaths for patients and also significant
trauma for loved ones and caregivers
alike~\cite{AndersonShawZar2020,AnnYiAzharea2021,Capozzo2020,DCouto2022,StrangBergstromea2020,WakamMontgomeryea2020}.


\subsection{Transparency Theater}
\label{sec:transparency-theater}

As if the systemic and broad overreach of their policies wasn't enough, social
media also fail at transparent and accountable governance. The lack of
transparency disclosures, with platforms addressing at most ten out of eighteen
criteria, is somewhat disappointing. Ambiguous policy terms and inconsistent
metrics noted in \S\ref{sec:trusting-twitter} and \S\ref{sec:census-results} are
certainly confusing and get in the way of meaningful comparisons between
platforms---as the British communications regulator recently noted as
well~\cite{HarlingHenesyea2023}. But arguably the most corrosive finding of my
social media census are the many data quality issues and inconsistencies for
platforms other than Google and Reddit presented in
\S\ref{sec:census-validation}. They directly undermine any confidence in the
accuracy of platforms' transparency reports and instead raise more questions
than they answer. That is strong evidence in support of Evelyn Douek's claim in
her December 2022 Harvard Law Review article that social media platforms do not
provide meaningful transparency but rather engage in transparency
theater~\cite{Douek2022}.

That does \emph{not} mean that current transparency reports are useless. In
\S\ref{sec:census}, I gave the example of TikTok improving its algorithmic
content moderation based on the reported fraction of videos flagged by \AI.
Another interesting vignette gleaned from transparency reports concerns Reddit.
In 2021, more than half of its account sanctions were for ban evasion, i.e.,
users not accepting bans from individual subreddits or the entire site and
creating new accounts to access those same fora again. If we assume that at
least some of these users are not only driven by a desire to troll or otherwise
disrupt old haunts, then Reddit suspending their new accounts as well represents
tremendous wasted potential for the platform. After all, these are users, who
care so deeply about Reddit that they go out of their way to access the site.
But instead of finding ways of turning them into net-positive contributors
again, Reddit's punitive account moderation is pushing them away. The large
percentage of sanctions for ban evasion also is troubling given that using
throw-away accounts for posting about sensitive subjects has been an integral
and extensively studied part of culture on
Reddit~\cite{AmmariSchoenebeckea2019,AndalibiHaimsonea2016,ChoudhuryDe2014,Leavitt2015,PavalanathanDeChoudhury2015,SchradingOvesdotterAlmea2015,TreemLeonardi2013}

The problem is that, beyond such small vignettes, it is pretty difficult to gain
any systemic insight into the impact of social media, especially if we'd prefer
to do so in a scientifically acceptable, quantitative manner. Meta again leads
by significant failures. Consider its academic data sharing effort through
Harvard's Social Science One. It was supposed to facilitate unprecedented
empirical research on election integrity. In manifest reality, the first data
release was two years late, the data was marred by overly aggressive
randomization Meta does not apply to the same data shared with non-academic
partners, and the omission of about half of \US\ users was discovered only 1.5
years later, calling the results of ``dozens of papers'' into
question~\cite{Hegelich2020,HegelichMarcoea2020,Ingram2022,OHaraNelson2019,Timberg2021}.

Collecting one's own data by scraping social media websites isn't an acceptable
option either---at least not to Meta. The firm has repeatedly changed the code
of its website for the sole purpose of obstructing scraping (thereby also
breaking accessibility) and in 2021 even terminated the Facebook accounts of
academic researchers---even though they are at the same university as the firm's
chief \AI\ scientist---because they kept upstaging Meta's noticeably incomplete
database of political
ads~\cite{EdelsonMcCoy2021a,Faife2021a,MerrillTobin2019,Roose2021a}. That
certainly is a bit rich for a company that got started by Mark Zuckerberg
scraping the Harvard student directory for images~\cite{Madrigal2019} and that
has been utilizing a scraping service for many years into early 2023
itself~\cite{Newman2023}. Meta's many late, incomplete, and compromised
disclosures and its active obstruction of outside investigations are certainly
impressive. But the most striking aspect about them is that the same apparent
hostility towards transparency extends to other aspects of Meta's business too.
Notably, the firm has been manipulating ad impressions---only the most basic
advertising metric according to the firm's own \SEC\ filings---and thereby has
been misleading customers, investors, and regulators alike~\cite{grimm2022a}.


\let\Oldthefootnote\thefootnote
\renewcommand*{\thefootnote}{\ding{168}}
\subsection{``The Great Question before us is: Can we Change? In Time?''%
\texorpdfstring{\>\!\protect\footnotemark}{}}
\footnotetext{%
    From the opening monologue of Tony Kushner's \emph{Angels in
    America, part~2, Perestroika}~\cite{Kushner1993}}
\let\thefootnote\Oldthefootnote

So what should we do about the many failures of social media content moderation?
Since I was just bemoaning Meta's apparent unwillingness to be even minimally
accountable, I'm going to continue with that thrust. The United States has been
missing in action when it comes to meaningful regulation of social media. Its
inaction keeps saddling mostly African, Asian, and South American countries with
the negative externalities of its neo-imperialist technology industry. Yet
writing appropriate legislation isn't that hard. It should require platforms to:
\begin{enumerate}
    \item Be localized \emph{before} entering a foreign market;
        \label{itm:law:localized}
    \item Likewise, have a trust and safety team comprising native speakers for
        \emph{all} major local languages at the ready; \label{itm:law:team}
    \item Notify users electronically of all content moderation actions with
        exception of ``deceptive high-volume commercial
        content''~\cite{EuropeanParliamentAndCouncil2022}, i.e., spam;
    \item Track who flags what content or users for what reasons and with what
        outcomes;
    \item Report attendant summary statistics at least once a year.
    \item Make granular event logs available to research projects approved by a
        jurisdiction's equivalent to the National Science Foundation in the \US.
        That institution also vets projects for global log access.
        \label{itm:law:research}
\end{enumerate}
For each country with high platform presence, i.e., where at least 10\% of the
local population are monthly active users, it should require them to:
\begin{enumerate}[resume]
    \item Provide a meaningful appeals process for content moderation decisions;
        \label{itm:law:appeals}
    \item Include corresponding statistics in at least yearly transparency
        reports;
    \item Demonstrate that data collection and transparency reports have passed
        review by an independent auditor;
    \item Conduct a yearly review of systemic risks, develop and implement a
        plan for risk mitigation, and review implementation after the fact. Of
        particular interest are:
        \begin{itemize}
            \item Illegal content and activities;
            \item Fundamental rights including freedom of expression, right to
                privacy, right to non-discrimination, right of the child, and
                consumer protection;
            \item Democratic process and election integrity;
            \item Public health and emotional well-being.
        \end{itemize}
    \item Publish risk analysis, mitigation plan, and implementation review.
        \label{itm:law:publicrisk}
\end{enumerate}
Many of these requirements are directly based on the \DSA. We could even copy
relevant clauses verbatim. Thanks to a 2011 decision by the European
Commission~\cite{EuropeanCommission2011}, such reuse is explicitly encouraged. I
included requirements~\ref{itm:law:localized} and~\ref{itm:law:team} while also
making requirements~\ref{itm:law:appeals} through~\ref{itm:law:publicrisk}
contingent on a platform's per-country presence to counteract the current
neo-imperialist treatment of countries outside of North America and Europe. The
intent is to ensure that internet platforms finally become attuned to the
countries they have a significant presence in.

Requirement~\ref{itm:law:research} is a much simplified alternative to
equivalent provisions of the \DSA\ that errs on the side of research access. In
my opinion, that is justified given the importance of basing decisions on
empirical evidence. It also doesn't represent an undue burder on platforms,
which already collect this data and then some. The one notable difference from
current practice would be that platforms have to treat log formats like a public
\API\ and hence carefully manage changes. By contrast, the \DSA\ introduces a
formal process for research access, whose outcomes are uncertain and that
critically relies on national regulators as gatekeepers. Disappointingly for
this independent investigator, it is explicitly limited to institutional
researchers, and the principal investigator must be European as well.

Finally, we make sure that even platforms with large reserves of cash---notably,
Meta had \$40.74 billion of the stuff on December 31,
2022~\cite{MetaPlatforms2023}---don't treat fines as the cost of doing (shady)
business. Hence we impose fines amounting to 5--10\% of the previous year's
global revenue for each case of non-compliance. I'm pretty sure that will
convince even Mr Zuckerberg that accountability has tremendous value. And if it
does not, we can always double fines.


\subsubsection{Non-Punitive Moderation of Content Moderation}

Except I am writing this paper to bemoan the very excessiveness of content
moderation, in policy and implementation, and to push back against punitive
interventions that dehumanize in the name of platform safety. So instead of
advocating a similarly punitive approach towards digital platforms, I'd much
rather extend the same humane courtesy towards social media including Meta. I
readily admit that this doesn't come easy, since I consider much of that firm's
wealth ill-gotten and tained by blood. Ethics sure are troublesome.

Except that Douek's Harvard Law Review article raises a good number of
substantive issues with the obligations I just outlined. The starting point for
Douek's article is equivalent to my previous point, namely that content
moderation bureaucracies are people too. And these people are increasingly
moderating based not on content but behavior, often cooperating with
governments, e.g., when it comes to \CSAM, relying on outside organizations
serving as trusted flaggers, and trying out means for devolving control to
smaller communities. The article further attests systemic short-comings due to
unrealistic expectations about perfectability, too much focus on false positives
to the (almost) exclusion of false negatives, likewise too much focus on
individual cases instead of systemic failures, notably when it comes to
transparency, and procedural justice privileging the lucky few---an important
insight about equity in content moderation.

Overall, I am deeply appreciative of Douek's focus on systemic forces and concur
with many of her observations. As discussed in \S\ref{sec:transparency-theater}
above, my own findings of pervasive data quality issues support Douek's
contention that transparency disclosures are not very meaningful. However, the
article's claim that law makers' and academics' misconceptions about content
moderation amount to a ``standard picture'' seems contrived and too much of a
straw man---especially coming from a professor at Stanford's law school.
Furthermore, after developing the hierarchy of census criteria presented in
\S\ref{sec:census}, I had no difficulty integrating Douek's observations. In
fact, I had independently introduced some of them already. Likewise, the above
regulatory requirements account for many of these criticisms already. The
article acknowledges the \DSA's risk reviews as suitably systemic. In short, the
chasm between what Douek calls the standard picture and manifest reality in
content moderation may not be nearly as far or deep as Douek's article makes it
out to be. Finally, her discounting of speech as plentiful and hence ``not so
special'' isn't just ``almost sacrilegious.'' It runs real danger of depriving
platform users of their voices, even though many people use these platforms for
that same reason---to have a voice.

\hidden{
the speech is plentiful and can
thus be treated as disposable


That makes the plentiful speech argument deeply cynical, with potentially
dehumanizing impact. It would be a real shame if the humanity of moderators vs
moderated became viewed as yet another zero sum game in late stage capitalism.


, who utilize these platforms to have  their voice and thereby dehumanizing them.


rather ironic given that people use
these platforms to have a voice. That, of course, also is dehumanizing. It would
be a real shame if the humanity of moderators vs moderated became yet another
zero sum game of late stage capitalism.
}

There is a simple unifying explanation that accounts for shoddy accountability
and befuddled observers alike. Right now, it doesn't amount to much more than an
informed guess. But it is a compelling guess and hence well worth exploring in
future work. As I was reading and re-reading transparency disclosures and hence
becoming familiar with their structure and lingo, they never ceased to be
frustrating. It took me a while, but I eventually realized that the primary
cause was their deeply reductive presentation. Each metric exists in isolation,
with little motivation, has been stripped of all helpful context, and hence
comes close to be nothing more than an abstract cipher. Treating that semantic
wasteland as opportunity, marketing and public relations folk then added their
unique kind of spin, manipulating units, histogram buckets, and some such for
appearances and not insight. Hence we have Twitter not separating 0~views from
1—9 views when reporting reach (see \S\ref{sec:trusting-twitter}), Meta not
disclosing \NCMEC\ reports or unique pieces when reporting \CSAM (see
\S\ref{sec:census-validation}), and most every platform not relating content
sanctions to account sanctions.

That is not to say that platforms set out to spin transparency reports to their
marketing or public relations advantage. That's a secondary effect, with
professionals doing what they are trained to be doing. Alas, social media
certainly contributed to the dominance of the reductionist view.



an after-effect of
professionals doing what they are paid to be doing. The root cause is that the
humans responsible for producing these reports do not know who their audience
might be and how their audience might engage with the data. That is a
demotivating proposition par excellence. Similarly, the few academics and
journalists who might chance a glance at these reports don't know what to do
with them either. The only reason that I found a story in them is that I went
looking beyond individual reports and started correlating \CSAM\ disclosures.
While that did yield an important story in the end, it also was an oppressive
endeavor that I pieced together in spurts spread out over months. I couldn't
bear homo infanti lupus otherwise. Though even that look into one of the worst
abysses of human depravity does not come close to the despair I feel when seeing
our collective inaction on the one threat that matters more than anything,
climate change. But I stop here. Otherwise, I might have to...


\subsubsection{A New Kind of AI + IP = AIP}


\hidden{

Punitive policies, a structural bias against the public interest, dodgy
enforcement practices, and little meaningful accountability are not sustainable
in the long term. I defer to Douek's Law Review article for
Policy overreach and dodgy enforcement are
More generally, however, it is clear that current content moderation practices
are too aggressive and are alienating non-negligible numbers of people with
their heavy-handed and also politically biased enforcement. Alas, making less
moderation palatable to users is going to be difficult and will require
offsetting functionality that lets individual users enforce their own policies,
but without requiring manual review. A first step in that direction may just be
content warnings: When a user creates content, they are responsible for applying
appropriate warnings, too. They have been available on Mastodon for years, allow
for arbitrary label text, and are used for a wide range of content, including
for example spoiler warnings, strong emotions, sensitive subjects ``like
addition, drugs, war, news, politics,'' and \AI-generated
content~\cite{Maloki2022,Sheehan2022,Voit2022}. Tumblr introduced their own
version in September 2022, though the choice of labels is far more conventional
and limited to mature, drug and alcohol addiction, violence, and sexual
themes~\cite{Tumblr2022}. More generally, platforms are experimenting with how
to give users more
controls~\cite{AngelidouSmith2021,Goodman2021,Hern2021,Instagram2021}


Giving smaller online communities more autonomy in shaping their discourse
seems reasonable. But


Devolution of content moderation does have the potential to give groups
of like-minded
}


Since the very potential of \AI\ also turns it into an incredibly disruptive
technology, we'd be well advised not to wait with regulating its use until
negative externalities are piling up as they have been for social media. My
proposal for doing so is primarily concerned with the goals of ensuring that not
just a few privileged individuals and corporations but all of society reap \AI's
benefits and that researchers, reporters, and other interested parties can
readily explore even commercial \AI\ systems.

\hidden{
readily
get access to \AI.

Given that the very potential of \AI\ also turns it into an incredibly
disruptive force, we'd be well advised not to wait with regulating the use of
\AI\ as we did for online platforms.


The very potential of \AI\ also turns it into an incredibly disruptive force.
So the question becomes how should we r
}


\subsection{Safety by Obscurity}

Finally, in the course of researching this paper, I kept encountering assertions
that disclosure is detrimental to safety. Let's be clear on this: That assertion
is false. Safety by obscurity is a fallacy in similar ways to security by
obscurity is a fallacy.

Without full disclosure, there is no way to ascertain
the actual safety provided.

Worse, it runs counter how we structure governance in democratic societies. But
with algorithmic content moderation enforcing policies that run counter the
public interest and \AI\ threatening to drive inequality to levels that are even
more destabilizing for society, safety by obscurity should be understood as a
direct threat to democratic societies.

When
combined with my finding, discussed above, of content mod policies that already run counter
the public interest, safety by obscurity probably turns into a real threat to
democracies.




Safety by obscurity is utter nonsense in the same way that security by
obscurity is utter nonsense. It  You are bound to overlook some critical flaw.


! Just like security by obscurity is utter nonsense. The only difference is that
the \AI\ community hasn't yet internalized the dictum, whereas security
professionals have. I am not the first to observe this. In a slightly more
limited scope, Cory Doctorow put it as ``como is infosec''~\cite{Doctorow2022}.
I personally find the use of \emph{como} instead of \emph{content moderation}
grating. But this clearly is a case of Doctorow using the language best suited
to hist intended audience: The so-called \COMO\ Summit of 2018 directly lead to
the formation of the Trust \& Safety Professional Association---which either is
utterly clueless or has a wicked sense of humor, seeing that its yearly
conference, now in its second year, is called \emph{TrustCon}!

Yet neither social media nor companies pushing \AI\ such as OpenAI seem to tire
of trying to get away with such assertions. Sure enough, in a March 2023
interview with The Verge, OpenAI's chief scientist and co-founder Ilya Sutskever
was working just that angle again~\cite{Vincent2023a}. To his credit he demurred
for the most part when confronted. Yet they keep trying. Worse, all too often
credulous journalists fail to push back, even though the underlying reason isn't
hard to discern. OpenAI has shared a wealth of material about \DALLE, including
in a scientific paper~\cite{RameshDhariwalea2022} and in the system card for the
April 2022 preview release~\cite{MishkinAhmad2022}. Yet the firm has also been
consistently and completely silent on the composition of its training data. That
reticence has clearly paid off. The far more open and transparent Stable
Diffusion is the target of highly visible copyright lawsuits by a stock
photography agency and three artists~\cite{Butterick2023,Setty2023}, whereas
OpenAI has avoided them---so far. As I already observed at the end of
\S\ref{sec:dalle}, OpenAI has access to stellar public relations and legal
talent. But their spin doesn't make safety by obscurity any better. It remains a
recipe for disaster, since small, insular communities tend towards groupthink.
As illustrated in \S\ref{sec:escape}, that already makes OpenAI's systems
vulnerable.


\hidden{

back towards smaller scopes of moderation \cite{HasinoffSchneider2022}

Mastodon is losing users again \cite{Hoover2023}
maybe we need the equivalent of user stories for transparency \cite{}
no quoting on Mastodon \cite{Rochko2018}  vs dunking is delicious \cite{Schwedel2017}
a longer interview \cite{Tilley2018}

THE book on user story mapping \cite{PattonEconomy2014}

Popularity on Reddit is still a long tail  \cite{ThukralMeisheriea2018}
Reddit is becoming self-sufficient, not that many links to the outside  \cite{SingerFlockea2014}
Toxic Reddit \cite{Massanari2017,RiegerKumpelea2021}

Mastodon \cite{ZulliLiuea2020}
Toxic Mastodon \cite{Cathcart2023,Rozenshtein2022}    Good Mastodon \cite{Hall2023}
Software used by right wing websites, but site cut off from federatin \cit{CopiaInstitute2021}

Don't grow too big: EU DSA \cite{KomaitisdeFranssu2022}

Moderators nurturing communities \cite{SeeringWangea2019}
But users may not take it up \cite{Squirrell2019}
and moderators may be toxic  \cite{AlmerekhiJansenea2020}

Moderators burning out \cite{SchopkeGonzalezAtrejaea2022}  \cite{Hoover2023}: users
are leaving again but moderators are relieved

Large scale deplatformization \cite{VanDijckdeWinkelea2021}

crowdsourcing for moderation \cite{HettiachchiGoncalves2020} bad idea!


disproportionate content removal: conservatives, trans, Black  \cite{HaimsonDelmonacoea2021}

}
