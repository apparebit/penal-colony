This paper serves as pointed critique that far too many algorithmic interventions outside the criminal injustice system are excessively punitive, often resulting in the figurative death of users through permanent account suspension. First, based on my own experiences and grounded in procedural justice, this paper starts by exploring the many ways policy and automated enforcement turn punitive on the example of OpenAI's DALL·E 2. Second, it illustrates how even best-practices policy turns punitive performance on the example of pre-Musk Twitter. Third, a comprehensive survey of non-Chinese social media demonstrates the pervasiveness of excessively punitive content moderation. It also tests the limits of their accountability, notably by projecting the likely impact of the European Union's Digital Services Act and by correlating data released by Facebook, Google, and the National Center for Missing and Exploited Children. Fourth, to counter any hope that doing better is a viable strategy for addressing the basic inhumanity of these interventions, this paper next illustrates the impossibility of ridding ourselves even of punitive algorithmic snake oil on the example of the October 2020 bar exam in California. Fifth, it illustrates the limits of algorithmic content moderation through a successful strategy for subverting DALL·E 2's aggressive automated censor, which inadvertently also unleashed grotesquely racist imagery. This paper concludes by pointing towards harm reduction as a strategy for, possibly maybe, making life in the penal colony at least somewhat bearable --- because, I fear, we are stuck in just that penal colony.
