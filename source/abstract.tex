This paper serves as pointed critique of algorithmic practice outside the
criminal injustice system. Far too many interventions including social media's
content moderation are excessively punitive, often resulting in the figurative
death of users through permanent account suspension. First, based on my own
experiences and grounded in procedural justice, this paper starts by exploring
the many ways policy and automated enforcement turn punitive on the example of
OpenAI's \DALLE\ 2. Second, it illustrates how even best-practices policy turns
punitive performance on the example of pre-Musk Twitter. Third, a comprehensive
survey of non-Chinese social media demonstrates the pervasiveness of excessively
punitive content moderation. It also tests the limits of their accountability,
notably by projecting the likely impact of the European Union's Digital Services
Act and by correlating data released by Facebook, Google, and the National
Center for Missing and Exploited Children. Fourth, to illustrate the limits of
algorithmic content moderation, it presents a successful strategy for subverting
\DALLE's aggressive automated censor, which inadvertently also unleashed
grotesquely racist imagery. Fifth, the paper identifies several more obvious
remedies to the many systemic concerns raised previously, though clearly there
is a pressing need for more research and more options. This paper concludes by
pointing towards harm reduction as a mindset for, possibly maybe, making life in
this digital penal colony at least somewhat bearable --- because, I fear, we are
stuck in it.
