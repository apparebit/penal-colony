This paper serves as pointed critique of algorithmic practice outside the
criminal injustice system. Far too many interventions including social media's
content moderation are excessively punitive, often resulting in the figurative
death of users through permanent account suspension. First, based on my own
experiences and grounded in procedural justice, this paper starts by exploring
the many ways policy and automated enforcement turn punitive on the example of
OpenAI's \DALLE\ 2. Second, it illustrates how even best-practices policy turns
punitive performance on the example of pre-Musk Twitter. Third, a comprehensive
survey of non-Chinese social media demonstrates the pervasiveness of excessively
punitive content moderation. It also tests the limits of their accountability,
notably by projecting the likely impact of the European Union's Digital Services
Act and by correlating data released by Facebook, Google, and the National
Center for Missing and Exploited Children. Fourth, to illustrate the limits of
algorithmic content moderation, this paper presents a successful strategy for
subverting \DALLE's aggressive automated censor, which inadvertently also
unleashed grotesquely racist imagery. Fifth, this paper proposes a new
intellectual property regime specifically for \V{AI}. It re-combines proven
elements from copyright and patent law, resulting in a framework that balances
the interests of those who invest in state-of-the-art \V{AI} and everyone else.
Finally, this paper concludes by pointing towards harm reduction as a mindset
for, possibly maybe, making life in this digital penal colony at least somewhat
bearable---because, I fear, we are stuck in it.
