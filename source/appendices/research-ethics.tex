% !TEX root = ../main.tex

\newpage
\section{Research Methodology and Ethics}
\label{adx:research-ethics}

This paper is motivated by the observation that prevailing practice for
structuring algorithmic interventions is deeply unethical because the results
are primarily punitive and, worse, excessively so. In support of this claim, the
paper includes three analytical case studies, a survey of social media
platforms, and a study exploring an algorithmic censor's limits. The latter is
comparable to studies that probe the security of internet-facing systems, with
one important difference: Whereas probing a system's security invariably ends up
exploring a path infrequently travelled and hence has a non-zero risk of causing
disruption, my probling of \DALLE's censor was well within the intended, common
use of the system and hence did not pose any risk for disruption. Furthermore,
unlike for other text-to-image systems, generated images are private by default,
i.e., only visible to myself and OpenAI. That allows for considered curation
before sharing potentially upsetting images --- which is just what I did.

The choice of topics for the case studies in this paper was entirely myopic,
based on personal involvement and interest. Over the years, I had made a habit
out of documenting interesting or just odd content and interactions online. So
when I decided to write about the punitive overreach of OpenAI's and Twitter's
algorithmic content moderation, I had all the materials I originally needed at
hand. Those seem like conditions that lend themselves to just the
auto-ethnographic approach I took for the first two case studies. But admittedly,
I didn't even reflect about the methodological choice at the start.

When I did, I noticed that I was also trying to counteract my biases by focusing
on the more objective aspects, i.e., the content policies and their automated
enforcement, while also de-emphasizing my own contributions. In a way, my
initial auto-ethnographic approach let me have my cake --- a fairly seamless
transition from personal interest to academic research --- and eat it too ---
not worrying too much about being unduly subjective. To put it differently, my
apparent willingness to test limits had the substantial benefit of me more fully
scoping OpenAI's and Twitter's content moderation and thereby providing
motivation and initial source material for this paper.

That hints at a tension that permeats this paper, namely the tension between me
as individual and also outsider versus me as academic researcher working within
an institutional tradition. Being personally subjected to Twitter's punishment
ritual certainly made me hyperaware of its punitive condenscension. Yet I turn
to scholarship for demonstrating that this weird anecdote actually has broader
significance. That same scholarship prefers that we maintain an emotional and
intellectual distance --- but grants us authority in return. Yet this weird
anecdote started because I was acting as a daily active shithead --- mind you,
that's an actual industry metric~\cite{}. Though I doubt that anecdote or metric
will grow my academic cachet. Then again, institutional authority is brittle and
prone to double standards, blindspots, and other failures --- as the paper
demonstrates on the example of OpenAI and \DALLE{}.

Consistent with that qualified skepticism of authority, this paper explicitly
favors the perspectives and experiences of individuals over those of
organizations, even if the individual has violated an organization's rules or
country's laws. That is a deliberate reaction to the dehumanizing impact of
organizational policies and algorithmic enforcement described in this paper. For
those same reasons, I appear as myself in this paper and don't hide behind a
pluralis majestatis (uhm, right, see previous paragraph) or similar linguistic
device. They only project but never provide authority. Furthermore, this isn't
an academic affectation. I conducted the research and wrote this paper on my
own, without institutional support and, in particular, without access to an
institutional library.

Thankfully, regular search engines do a passable job at surfacing academic
literature. Furthermore, preprint archives and open access publications make
many publications of the last decade or so readily available. However, that
still leaves a large number of publications behind the paywalls of academic
publishers. I observed per-article rates from \$15 by professional societies to
around \$50 by for-profit publishers, which strike me as excessive. While
publishers often offer better rates for bulk access, they still are entirely
unreasonable given that publishers paid nothing for the hard work of writing,
reviewing, and editing these articles. Instead, I relied on sci-hub as well as
my browser's developer tools to help procure publications, with the latter
letting me discover the publicly accessible URLs of PDF files some publishers
already displayed to me in the browser. Since my prior publications did not
benefit from open access options, I encourage others to also rely on sci-hub for
accessing them.


\subsection{Potential Conflicts of Interest}

I worked at Meta n\'ee Facebook as a software engineer from mid 2018 to mid
2019. During that time, I discovered compelling evidence that Meta is cooking
the books when it comes to ad impressions, i.e., the most fundamental
advertising metric~\anon[{[elided]}]{\cite{grimm2022a}}. I also served as paid
consultant to litigation against Meta during the second half of 2022. At the
same time, my work for both roles was unrelated to this research and I do not
have a financial interest in Meta --- or any of the other corporations mentioned
in this paper --- beyond possible indirect interest through index funds.
